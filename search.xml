<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AlexNet</title>
    <url>/2020/03/12/AlexNet/</url>
    <content><![CDATA[<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><blockquote>
<p>该篇介绍了python的基本语句</p>
</blockquote>
<a id="more"></a>
<h4 id="AlexNet小技巧"><a href="#AlexNet小技巧" class="headerlink" title="AlexNet小技巧"></a>AlexNet小技巧</h4><ul>
<li>使用Relu作为激活函数，验证其效果更佳，且解决了sigmoid在网络较深时的梯度弥散问题。</li>
<li>dropout随机忽略一些神经元，以避免模型过拟合。</li>
<li>采用最大池化，替代了平均池化，避免平均池化的模糊化效果。</li>
<li>lrn层，实际没用</li>
<li>数据增强</li>
</ul>
<h4 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h4><ul>
<li>加深网络结构来提升性能</li>
<li>多个核的使用是的对特征学习能力更强</li>
<li>先训练级别A的简单网络，再服用A的网络权重来初始化后面的复杂模型，收敛速度提高</li>
<li>用multi-scale来数据增强</li>
<li></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>csp准备</title>
    <url>/2020/03/02/csp%E5%87%86%E5%A4%87/</url>
    <content><![CDATA[<h1 id="csp准备"><a href="#csp准备" class="headerlink" title="csp准备"></a>csp准备</h1><blockquote>
<p>重拾C语言，准备csp的考试</p>
</blockquote>
<a id="more"></a>
<h4 id="奇技淫巧（小注意）"><a href="#奇技淫巧（小注意）" class="headerlink" title="奇技淫巧（小注意）"></a>奇技淫巧（小注意）</h4><ol>
<li>i++ 和++i 老生常谈</li>
<li>比较大的数组声明在main函数外，函数内申请内存有限制</li>
<li>fgets(buf,maxn,fin)将读取完整的数量不超过maxn的一行放入buf钟，而gets(s)没有数量限制</li>
<li>循环中不要对循环变量操作，千万不要！！！！！！！！！！！！！</li>
<li>不知道为啥我总忘掉typedef  typedef struct {double x,y;}Point;</li>
<li>对复杂表达式化简，减少运算并且要防止输出</li>
<li>建议把为此</li>
<li>把数组大小设置为 #define n 20，可以免去很多修改</li>
<li>建议把谓词（判断事物是否具有某种特性的函数）命名为is_xx 的形式，返回int值，1代表有</li>
<li>形参和实参，形实结合</li>
<li>数组名实际是指针，所以传参时应该传给指针，未避免指针悬挂，应给数组的长度</li>
<li>递归判断可用？：代替        return n==0? 1:n*f(n-1);</li>
<li>未避免浮点误差 建议+0.5，这样库用四舍五入。</li>
<li>从0~n-1到1~n的数学转化p=(p+n-1)%n+1;</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>xs</title>
    <url>/2020/02/24/xs/</url>
    <content><![CDATA[<h1 id="基本语句"><a href="#基本语句" class="headerlink" title="基本语句"></a>基本语句</h1><blockquote>
<p>该篇介绍了python的基本语句</p>
</blockquote>
<a id="more"></a>
<p><img src="/2020/02/24/xs/avatar.jpg" alt></p>
]]></content>
  </entry>
  <entry>
    <title>矩阵求导</title>
    <url>/2020/02/24/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/</url>
    <content><![CDATA[<h1 id="矩阵求导"><a href="#矩阵求导" class="headerlink" title="矩阵求导"></a>矩阵求导</h1><blockquote>
<p>该篇介绍了矩阵求导的基本方法</p>
</blockquote>
<a id="more"></a>
<blockquote>
<p>前言：对于机器学习来说，矩阵求导是很重要的一部分，同时对向量求导是其中的主要组成部分，这里 对于矩阵求导这里不展开。</p>
</blockquote>
<p>求导和梯度概念很相似，注意比较</p>
<script type="math/tex; mode=display">
前置公式\quad x^T=[x_1\quad x_2 \quad \cdots \quad x_n]</script><p>基础公式：</p>
<script type="math/tex; mode=display">
f(x)是关于x的单值函数:
\\所以 \quad
\frac {\partial f(x)} {\partial x^T} = 
\left [ 
\frac {\partial f(x)} {\partial x_1} \quad \frac {\partial f(x)} {\partial x_2} \quad \cdots \quad \frac {\partial f(x)} {\partial x_n}

\right]  
\\
\quad f(x)^T=\left [f_1(x)\quad f_2(x) \quad \cdots \quad f_n(x)\right ]
\\
所以 \quad   \frac{\partial f(x)}{\partial x^T}=
\left( \begin{matrix} 
   \frac{\partial f_1(x)}{\partial x_1} &\frac{\partial f_1(x)}{\partial x_2} &\cdots &\frac{\partial f_1(x)}{\partial x_n}

\\ \frac{\partial f_2(x)}{\partial x_1} &\frac{\partial f_2(x)}{\partial x_2} &\cdots &\frac{\partial f_2(x)}{\partial x_n}

\\ \vdots &\vdots &\ddots &\vdots

\\ \frac{\partial f_n(x)}{\partial x_1} &\frac{\partial f_n(x)}{\partial x_2} &\cdots &\frac{\partial f_n(x)}{\partial x_n}
\end{matrix} \right)
\\

\frac{\partial f(x)^T}{\partial x}是行向量向列向量求导，和上面列向量对行向量求导类似</script><p>常用公式：</p>
<script type="math/tex; mode=display">
\frac {\partial x^T}{\partial x} = I</script><p>规则：</p>
<script type="math/tex; mode=display">
当f(x)是单值函数时，线性法则，乘积法则和商法则同微积分\\
链式法则，若y(x)时关于x的向量值函数(列向量)时。则\\
 \frac {\partial f(y(x))}{\partial x} = \frac {\partial y^T(x)} {\partial x}
 \frac{\partial f(y)}{\partial y}</script><p>常用公式：</p>
<script type="math/tex; mode=display">
前提公式：x^Ty=<x,y>=y^Tx</script><script type="math/tex; mode=display">
\frac{\partial a^Tx}{\partial x} =
\frac{\partial x^Ta}{\partial x} =a
\\下面不展示转置后的项以及利用转置的步骤\\
\frac{\partial a^Ty(x)}{\partial x} =\frac{\partial y^T(x)}{\partial x} a
\\
\frac{\partial x^TAy}{\partial x} =Ay
\\
\frac{\partial x^TAx}{\partial x} =(A+A^T)x  \quad如果A对称，则为2Ax
\\
\frac{\partial y(x)^TAy(x)}{\partial x} =\frac {\partial y^T(x)}{\partial x}(A+A^T)y(x)
\\
\frac{\partial y(x)^TAz(x)}{\partial x} =\frac {\partial y^T(x)}{\partial x}Az(x)+ \frac {\partial z^T(x)}{\partial x}A^Ty(x)</script><p>单值函数的梯度矩阵(其余公式不证)</p>
<script type="math/tex; mode=display">
 \quad   \frac{\partial f(x)}{\partial A}=
\left( \begin{matrix} 
   \frac{\partial f(x)}{\partial A_{11}} &\frac{\partial f(x)}{\partial A_{12}} &\cdots &\frac{\partial f(x)}{\partial A_{1n}}

\\ \frac{\partial f(x)}{\partial A_{21}} &\frac{\partial f(x)}{\partial A_{22}} &\cdots &\frac{\partial f(x)}{\partial A_{2n}}

\\ \vdots &\vdots &\ddots &\vdots

\\ \frac{\partial f(x)}{\partial A_{n1}} &\frac{\partial f(x)}{\partial A_{n2}} &\cdots &\frac{\partial f(x)}{\partial A_{nn}}
\end{matrix} \right)
\\</script>]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>基础类</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数</title>
    <url>/2020/02/21/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</url>
    <content><![CDATA[<h1 id="基本语句"><a href="#基本语句" class="headerlink" title="基本语句"></a>基本语句</h1><blockquote>
<p>该篇介绍了python的基本语句</p>
</blockquote>
<a id="more"></a>
]]></content>
  </entry>
  <entry>
    <title>markdown数学公式</title>
    <url>/2020/02/21/markdown%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</url>
    <content><![CDATA[<h1 id="基本语句"><a href="#基本语句" class="headerlink" title="基本语句"></a>基本语句</h1><blockquote>
<p>该篇介绍了python的基本语句</p>
</blockquote>
<a id="more"></a>
<blockquote>
<p>如果是typora，记得在文件的偏好设置中的markdown语法的内联公式开启</p>
</blockquote>
<h4 id="行内与独行"><a href="#行内与独行" class="headerlink" title="行内与独行"></a>行内与独行</h4><ul>
<li><p>行内公式 ： 2个美元符包围  $ ss $</p>
</li>
<li><p>独行公式：4个美元符包围 </p>
<script type="math/tex; mode=display">ss</script></li>
</ul>
<h4 id="上标下标组合"><a href="#上标下标组合" class="headerlink" title="上标下标组合"></a>上标下标组合</h4><ul>
<li>上标  ^符 $ x^4$</li>
<li>下标 _ 符$x_1$</li>
<li>组合符号 {}符   ${16}_{8x}O{2+}_{2}$</li>
</ul>
<h4 id="汉字、字体格式"><a href="#汉字、字体格式" class="headerlink" title="汉字、字体格式"></a>汉字、字体格式</h4><ul>
<li>汉字形式，符号：<code>\mbox{}</code>，如：$V_{\mbox {初始}}$</li>
<li>字体控制，符号：<code>\displaystyle</code>，如：$\displaystyle\frac{x+y}{y+z}$</li>
<li>下划线符号，符号：<code>\underline</code>，如：$\underline{x+y}$</li>
<li>标签，符号<code>\tag{数字}</code>，如：$\tag{11}$</li>
<li>上大括号，符号：<code>\overbrace{算式}</code>，如：$\overbrace{a+b+c+d}^{2.0}$</li>
<li>下大括号，符号：<code>\underbrace{算式}</code>，如：$a+\underbrace{b+c}_{1.0}+d$</li>
<li>上位符号，符号：<code>\stackrel{上位符号}{基位符号}</code>，如：$\vec{x}\stackrel{def}{=}x_1,\dots,x_n$</li>
</ul>
<h4 id="占位符"><a href="#占位符" class="headerlink" title="占位符"></a>占位符</h4><ul>
<li>两个quad空格，符号：<code>\qquad</code>，如：$x \qquad y$</li>
<li>quad空格，符号：<code>\quad</code>，如：$x \quad y$</li>
<li>大空格，符号<code>\</code>，如：$x \: y$ </li>
<li>紧贴，符号<code>\!</code>，如：$x!y$</li>
</ul>
<h4 id="定界符与组合"><a href="#定界符与组合" class="headerlink" title="定界符与组合"></a>定界符与组合</h4><ul>
<li>括号，符号：<code>（）\big(\big) \Big(\Big) \bigg(\bigg) \Bigg(\Bigg)</code>，如：$（）\big(\big)\quad \Big(\Big) \bigg(\bigg) \Bigg(\Bigg)$</li>
<li>大括号，符号：<code>\{ \}</code>，如：$\{x+y\}$  因为</li>
<li>自适应括号，符号：<code>\left \right</code>，如：$\left(x\right)$，$\left(x{yz}\right)$</li>
<li>组合公式，符号：<code>{上位公式 \choose 下位公式}</code>，如：${n+1 \choose k}={n \choose k}+{n \choose k-1}$</li>
<li>组合公式，符号：<code>{上位公式 \atop 下位公式}</code>，如：$\sum_{k_0,k_1,\ldots&gt;0 \atop k_0+k_1+\cdots=n}A_{k_0}A_{k_1}\cdots$</li>
</ul>
<h4 id="四则运算"><a href="#四则运算" class="headerlink" title="四则运算"></a>四则运算</h4><ul>
<li>加法运算，符号：<code>+</code>，如：$x+y=z$</li>
<li>减法运算，符号：<code>-</code>，如：$x-y=z$</li>
<li>加减运算，符号：<code>\pm</code>，如：$x \pm y=z$</li>
<li>减甲运算，符号：<code>\mp</code>，如：$x \mp y=z$</li>
<li>乘法运算，符号：<code>\times</code>，如：$x \times y=z$</li>
<li>点乘运算，符号：<code>\cdot</code>，如：$x \cdot y=z$</li>
<li>星乘运算，符号：<code>\ast</code>，如：$x \ast y=z$</li>
<li>除法运算，符号：<code>\div</code>，如：$x \div y=z$</li>
<li>斜法运算，符号：<code>/</code>，如：$x/y=z$</li>
<li>分式表示，符号：<code>\frac{分子}{分母}</code>，如：$\displaystyle \frac{x+y}{y+z}$</li>
<li>分式表示，符号：<code>{分子} \voer {分母}</code>，如：${x+y} \over {y+z}$</li>
<li>绝对值表示，符号：<code>||</code>，如：$|x+y|$</li>
</ul>
<h4 id="高级运算"><a href="#高级运算" class="headerlink" title="高级运算"></a>高级运算</h4><ul>
<li><p>平均数运算，符号：<code>\overline{算式}</code>，如：$\overline{xyz}$</p>
</li>
<li><p>开二次方运算，符号：<code>\sqrt</code>，如：$\sqrt x$</p>
</li>
<li><p>开方运算，符号：<code>\sqrt[开方数]{被开方数}</code>，如：$\sqrt[3]{x+y}$</p>
</li>
<li><p>对数运算，符号：<code>\log</code>，如：$\log(x)$</p>
</li>
<li><p>极限运算，符号：<code>\lim</code>，如：$\lim^{x \to \infty}_{y \to 0}{\displaystyle\frac{x}{y}}$</p>
</li>
<li><p>极限运算，符号：<code>\displaystyle \lim</code>，如：$\displaystyle \lim^{x \to \infty}_{y \to 0}{\frac{x}{y}}$</p>
</li>
<li><p>求和运算，符号：<code>\sum</code>，如：$\sum^{x \to \infty}_{y \to 0}{\frac{x}{y}}$</p>
</li>
<li><p>求和运算，符号：<code>\displaystyle \sum</code>，如：$\displaystyle \sum^{x \to \infty}_{y \to 0}{\frac{x}{y}}$</p>
</li>
<li><p>积分运算，符号：<code>\int</code>，如：$\int^{\infty}_{0}{xdx}$</p>
</li>
<li><p>积分运算，符号：<code>\displaystyle \int</code>，如：$\displaystyle \int^{\infty}_{0}{xdx}$</p>
</li>
<li><p>微分运算，符号：<code>\partial</code>，如：$\frac{\partial x}{\partial y}$</p>
</li>
<li><p>矩阵表示，符号：<code>\begin{matrix} \end{matrix}</code>，如：</p>
<script type="math/tex; mode=display">
\qquad \left( \begin{matrix} 
1 &2 &\cdots &4 

\\ 5 &6 &\cdots & 8 \\ 
\vdots &\vdots &\ddots &\vdots 

\\13 &14 &\cdots &16

\end{matrix} \right)</script></li>
</ul>
<h4 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a>逻辑运算</h4><ul>
<li><p>等于运算，符号：<code>=</code>，如：$x+y=z$</p>
</li>
<li><p>大于运算，符号：<code>&gt;</code>，如：$x+y&gt;z$</p>
</li>
<li><p>小于运算，符号：<code>&lt;</code>，如：$x+y&lt;z$</p>
</li>
<li><p>大于等于运算，符号：<code>\geq</code>，如：$x+y \geq z$</p>
</li>
<li><p>小于等于运算，符号：<code>\leq</code>，如：$x+y \leq z$</p>
</li>
<li><p>不等于运算，符号：<code>\neq</code>，如：$x+y \neq z$</p>
</li>
<li><p>不大于等于运算，符号：<code>\ngeq</code>，如：$x+y \ngeq z$</p>
</li>
<li><p>不大于等于运算，符号：<code>\not\geq</code>，如：$x+y \not\geq z$</p>
</li>
<li><p>不小于等于运算，符号：<code>\nleq</code>，如：$x+y \nleq z$</p>
</li>
<li><p>不小于等于运算，符号：<code>\not\leq</code>，如：$x+y \not\leq z$</p>
</li>
<li><p>约等于运算，符号：<code>\approx</code>，如：$x+y \approx z$</p>
</li>
<li><p>恒定等于运算，符号：<code>\equiv</code>，如：$x+y \equiv z$</p>
</li>
</ul>
<h4 id="集合运算"><a href="#集合运算" class="headerlink" title="集合运算"></a>集合运算</h4><ul>
<li><p>属于运算，符号：<code>\in</code>，如：$x \in y$</p>
<p>不属于运算，符号：<code>\notin</code>，如：$x \notin y$</p>
<p>不属于运算，符号：<code>\not\in</code>，如：$x \not\in y$</p>
<p>子集运算，符号：<code>\subset</code>，如：$x \subset y$</p>
<p>子集运算，符号：<code>\supset</code>，如：$x \supset y$</p>
<p>真子集运算，符号：<code>\subseteq</code>，如：$x \subseteq y$</p>
<p>非真子集运算，符号：<code>\subsetneq</code>，如：$x \subsetneq y$</p>
<p>真子集运算，符号：<code>\supseteq</code>，如：$x \supseteq y$</p>
<p>非真子集运算，符号：<code>\supsetneq</code>，如：$x \supsetneq y$</p>
<p>非子集运算，符号：<code>\not\subset</code>，如：$x \not\subset y$</p>
<p>非子集运算，符号：<code>\not\supset</code>，如：$x \not\supset y$</p>
<p>并集运算，符号：<code>\cup</code>，如：$x \cup y$</p>
<p>交集运算，符号：<code>\cap</code>，如：$x \cap y$</p>
<p>差集运算，符号：<code>\setminus</code>，如：$x \setminus y$</p>
<p>同或运算，符号：<code>\bigodot</code>，如：$x \bigodot y$</p>
<p>同与运算，符号：<code>\bigotimes</code>，如：$x \bigotimes y$</p>
<p>实数集合，符号：<code>\mathbb{R}</code>，如：$\mathbb{R}$</p>
<p>自然数集合，符号：<code>\mathbb{Z}</code>，如:$\mathbb{Z}$</p>
<p>空集，符号：<code>\emptyset</code>，如：$\emptyset$</p>
</li>
</ul>
<h4 id="数学符号"><a href="#数学符号" class="headerlink" title="数学符号"></a>数学符号</h4><p>无穷，符号：<code>\infty</code>，如：$\infty$</p>
<p>虚数，符号：<code>\imath</code>，如：$\imath$</p>
<p>虚数，符号：<code>\jmath</code>，如：$\jmath$</p>
<p>数学符号，符号<code>\hat{a}</code>，如：$\hat{a}$</p>
<p>数学符号，符号<code>\check{a}</code>，如：$\check{a}$</p>
<p>数学符号，符号<code>\breve{a}</code>，如：$\breve{a}$</p>
<p>数学符号，符号<code>\tilde{a}</code>，如：$\tilde{a}$</p>
<p>数学符号，符号<code>\bar{a}</code>，如：$\bar{a}$</p>
<p>矢量符号，符号<code>\vec{a}</code>，如：$\vec{a}$</p>
<p>数学符号，符号<code>\acute{a}</code>，如：$\acute{a}$</p>
<p>数学符号，符号<code>\grave{a}</code>，如：$\grave{a}$</p>
<p>数学符号，符号<code>\mathring{a}</code>，如：$\mathring{a}$</p>
<p>一阶导数符号，符号<code>\dot{a}</code>，如：$\dot{a}$</p>
<p>二阶导数符号，符号<code>\ddot{a}</code>，如：$\ddot{a}$</p>
<p>上箭头，符号：<code>\uparrow</code>，如：$\uparrow$</p>
<p>上箭头，符号：<code>\Uparrow</code>，如：$\Uparrow$</p>
<p>下箭头，符号：<code>\downarrow</code>，如：$\downarrow$</p>
<p>下箭头，符号：<code>\Downarrow</code>，如：$\Downarrow$</p>
<p>左箭头，符号：<code>\leftarrow</code>，如：$\leftarrow$</p>
<p>左箭头，符号：<code>\Leftarrow</code>，如：$\Leftarrow$</p>
<p>右箭头，符号：<code>\rightarrow</code>，如：$\rightarrow$</p>
<p>右箭头，符号：<code>\Rightarrow</code>，如：$\Rightarrow$</p>
<p>底端对齐的省略号，符号：<code>\ldots</code>，如：$1,2,\ldots,n$</p>
<p>中线对齐的省略号，符号：<code>\cdots</code>，如：$x_1^2 + x_2^2 + \cdots + x_n^2$</p>
<p>竖直对齐的省略号，符号：<code>\vdots</code>，如：$\vdots$</p>
<p>斜对齐的省略号，符号：<code>\ddots</code>，如：$\ddots$</p>
<h4 id="希腊字符"><a href="#希腊字符" class="headerlink" title="希腊字符"></a>希腊字符</h4><p><img src="/2020/02/21/markdown%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/Users\31311\Desktop\github_web\source\images\字符.png" alt></p>
<p><img src="/2020/02/21/markdown%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/Users\31311\Desktop\github_web\source\images\字符2.png" alt></p>
]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>-基础类</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习导论</title>
    <url>/2020/02/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/</url>
    <content><![CDATA[<h1 id="机器学习导论"><a href="#机器学习导论" class="headerlink" title="机器学习导论"></a>机器学习导论</h1><blockquote>
<p><strong><em>ML = Matrix （信息）+ Optimation（优化） + Algorithm（使优化更有效） +Statistics（建模）</em></strong></p>
</blockquote>
<a id="more"></a>
<p>one-hot表示方法，单纯表示方法</p>
<p>监督学习问题：分类问题和回归问题</p>
<p>分类属于特殊的回归问题</p>
<p>最小二乘估计‘</p>
<p>矩阵求导相关？？</p>
<p>岭回归（最小二乘估计加上惩罚）</p>
<p>数据分为三组：Training data， Validation data，Test data</p>
<p>极大似然估计（MLE</p>
<p>无监督问题：聚类</p>
<p>半监督学习</p>
<p>迁移学习：将测试数据也放进去</p>
<p>参数（模型参数固定，不需要训练，超参数）和非参数（需要训练，提前未给定）</p>
<p>判别模型和生成模型</p>
]]></content>
  </entry>
  <entry>
    <title>大数据机器学习</title>
    <url>/2020/02/17/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h1 id="大数据机器学习"><a href="#大数据机器学习" class="headerlink" title="大数据机器学习"></a>大数据机器学习</h1><blockquote>
<p>该篇介绍了python的基本语句</p>
</blockquote>
<a id="more"></a>
<blockquote>
<p> 机器学习是一门多领域交叉学课，涉及概率论，统计学，逼近论，图分析，算法复杂度理论，是自动“学习的算法”，与统计推断学密切相关，也被称为统计学习理论</p>
</blockquote>
<p><em>辉煌历史不提，比如深蓝，文字转换，3dvr，生物信息学</em></p>
<p>机器学习是人工智能的计算方法，表示学习（浅层自编码器），深度学习（多层感知机）</p>
<h3 id="共性和差异"><a href="#共性和差异" class="headerlink" title="共性和差异"></a>共性和差异</h3><ul>
<li><p>基于规则的系统：不可学习，专家系统</p>
</li>
<li><p>基于表示的系统</p>
<ul>
<li>学习获得，经过特征映射得到</li>
<li>深度学习，通过层的深入得到递进的特征，从而特征映射获得结果</li>
</ul>
</li>
</ul>
<p>计算机视觉是机器学习最重要的领域</p>
<p>机器学</p>
]]></content>
  </entry>
  <entry>
    <title>加性高斯模糊自编码器</title>
    <url>/2020/02/17/%E5%8A%A0%E6%80%A7%E9%AB%98%E6%96%AF%E6%A8%A1%E7%B3%8A%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</url>
    <content><![CDATA[<h1 id="加性高斯噪声自动编码器"><a href="#加性高斯噪声自动编码器" class="headerlink" title="加性高斯噪声自动编码器"></a>加性高斯噪声自动编码器</h1><blockquote>
<p>该篇介绍了加性高斯噪声自动编码器的构造方法，用类实现方便集成</p>
</blockquote>
<a id="more"></a>
<h3 id="加性高斯噪声自动编码器-1"><a href="#加性高斯噪声自动编码器-1" class="headerlink" title="加性高斯噪声自动编码器"></a>加性高斯噪声自动编码器</h3><p><em>目的：给数字加上模糊后再还原它</em>，以及用sklearn库的尝试</p>
<h4 id="xavier-init"><a href="#xavier-init" class="headerlink" title="xavier_init"></a>xavier_init</h4><blockquote>
<p>它为了保证前向传播和反向传播时每一层的方差一致，根据每层的输入个数和输出个数来决定参数随机初始化的分布范围，是一个通过该层的输入和输出参数个数得到的分布范围内的均匀分布。 </p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn.preprocessing <span class="keyword">as</span> prep</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment">### xaver_init方法保证初始化的方差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xavier_init</span><span class="params">(fan_in, fan_out, constant = <span class="number">1</span>)</span>:</span></span><br><span class="line">    low = -constant * np.sqrt(<span class="number">6.0</span> / (fan_in + fan_out))</span><br><span class="line">    high = constant * np.sqrt(<span class="number">6.0</span> / (fan_in + fan_out))</span><br><span class="line">    <span class="keyword">return</span> tf.random_uniform((fan_in, fan_out),</span><br><span class="line">                             minval = low, maxval = high,</span><br><span class="line">                             dtype = tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 建立自编码器类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdditiveGaussianNoiseAutoencoder</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_input, n_hidden, transfer_function = tf.nn.softplus, optimizer = tf.train.AdamOptimizer<span class="params">()</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 scale = <span class="number">0.1</span>)</span>:</span></span><br><span class="line">        self.n_input = n_input</span><br><span class="line">        self.n_hidden = n_hidden</span><br><span class="line">        self.transfer = transfer_function</span><br><span class="line">        self.training_scale = scale</span><br><span class="line">        </span><br><span class="line">        network_weights = self._initialize_weights()</span><br><span class="line">        self.weights = network_weights</span><br><span class="line"></span><br><span class="line">        <span class="comment"># model</span></span><br><span class="line">        <span class="comment">### 建立占位符</span></span><br><span class="line">        self.scale = tf.placeholder(tf.float32)  <span class="comment">##高斯模糊的大小</span></span><br><span class="line">        self.x = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_input])</span><br><span class="line">        <span class="comment">### 加高斯模糊的隐藏层</span></span><br><span class="line">        self.hidden = self.transfer(tf.add(tf.matmul(self.x + scale * tf.random_normal((n_input,)),</span><br><span class="line">                self.weights[<span class="string">'w1'</span>]),</span><br><span class="line">                self.weights[<span class="string">'b1'</span>]))</span><br><span class="line">        <span class="comment">###重建输出</span></span><br><span class="line">        self.reconstruction = tf.add(tf.matmul(self.hidden, self.weights[<span class="string">'w2'</span>]), self.weights[<span class="string">'b2'</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cost</span></span><br><span class="line">        self.cost = <span class="number">0.5</span> * tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), <span class="number">2.0</span>))</span><br><span class="line">        self.optimizer = optimizer.minimize(self.cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### 初始化</span></span><br><span class="line">        init = tf.global_variables_initializer()</span><br><span class="line">        self.sess = tf.Session()</span><br><span class="line">        self.sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### 建立字典参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_initialize_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        all_weights = dict()</span><br><span class="line">        all_weights[<span class="string">'w1'</span>] = tf.Variable(xavier_init(self.n_input, self.n_hidden))</span><br><span class="line">        all_weights[<span class="string">'b1'</span>] = tf.Variable(tf.zeros([self.n_hidden], dtype = tf.float32))</span><br><span class="line">        all_weights[<span class="string">'w2'</span>] = tf.Variable(tf.zeros([self.n_hidden, self.n_input], dtype = tf.float32))</span><br><span class="line">        all_weights[<span class="string">'b2'</span>] = tf.Variable(tf.zeros([self.n_input], dtype = tf.float32))</span><br><span class="line">        <span class="keyword">return</span> all_weights</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 训练</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">partial_fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict = &#123;self.x: X,</span><br><span class="line">                                                                            self.scale: self.training_scale</span><br><span class="line">                                                                            &#125;)</span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="comment">### 计算cost</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_total_cost</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.sess.run(self.cost, feed_dict = &#123;self.x: X,</span><br><span class="line">                                                     self.scale: self.training_scale</span><br><span class="line">                                                     &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.sess.run(self.hidden, feed_dict = &#123;self.x: X,</span><br><span class="line">                                                       self.scale: self.training_scale</span><br><span class="line">                                                       &#125;)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    无效代码</span></span><br><span class="line"><span class="string">    def generate(self, hidden = None):</span></span><br><span class="line"><span class="string">        if hidden is None:</span></span><br><span class="line"><span class="string">            hidden = np.random.normal(size = self.weights["b1"])</span></span><br><span class="line"><span class="string">        return self.sess.run(self.reconstruction, feed_dict = &#123;self.hidden: hidden&#125;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def reconstruct(self, X):</span></span><br><span class="line"><span class="string">        return self.sess.run(self.reconstruction, feed_dict = &#123;self.x: X,</span></span><br><span class="line"><span class="string">                                                               self.scale: self.training_scale</span></span><br><span class="line"><span class="string">                                                               &#125;)</span></span><br><span class="line"><span class="string">    def getWeights(self):</span></span><br><span class="line"><span class="string">        return self.sess.run(self.weights['w1'])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def getBiases(self):</span></span><br><span class="line"><span class="string">        return self.sess.run(self.weights['b1'])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    '''</span>    </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">preprocessing这个模块还提供了一个实用类StandarScaler，它可以在训练数据集上做了标准转换操作之后，把相同的转换应用到测试训练集中。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">这是相当好的一个功能。可以对训练数据，测试数据应用相同的转换，以后有新的数据进来也可以直接调用，不用再重新把数据放在一起再计算一次了。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standard_scale</span><span class="params">(X_train, X_test)</span>:</span></span><br><span class="line">    preprocessor = prep.StandardScaler().fit(X_train)</span><br><span class="line">    X_train = preprocessor.transform(X_train)</span><br><span class="line">    X_test = preprocessor.transform(X_test)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_test</span><br><span class="line"></span><br><span class="line"><span class="comment">### 随机返回batch_size的数据块</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_random_block_from_data</span><span class="params">(data, batch_size)</span>:</span></span><br><span class="line">    start_index = np.random.randint(<span class="number">0</span>, len(data) - batch_size)</span><br><span class="line">    <span class="keyword">return</span> data[start_index:(start_index + batch_size)]</span><br><span class="line"></span><br><span class="line">X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)</span><br><span class="line"></span><br><span class="line">n_samples = int(mnist.train.num_examples)</span><br><span class="line">training_epochs = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 生成对象</span></span><br><span class="line">autoencoder = AdditiveGaussianNoiseAutoencoder(n_input = <span class="number">784</span>,</span><br><span class="line">                                               n_hidden = <span class="number">200</span>,</span><br><span class="line">                                               transfer_function = tf.nn.softplus,</span><br><span class="line">                                               optimizer = tf.train.AdamOptimizer(learning_rate = <span class="number">0.001</span>),</span><br><span class="line">                                               scale = <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">    avg_cost = <span class="number">0.</span></span><br><span class="line">    total_batch = int(n_samples / batch_size)</span><br><span class="line">    <span class="comment"># Loop over all batches</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">        batch_xs = get_random_block_from_data(X_train, batch_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Fit training using batch data</span></span><br><span class="line">        cost = autoencoder.partial_fit(batch_xs)</span><br><span class="line">        <span class="comment"># Compute average loss</span></span><br><span class="line">        avg_cost += cost / n_samples * batch_size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display logs per epoch step</span></span><br><span class="line">    <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Total cost: "</span> + str(autoencoder.calc_total_cost(X_test)))</span><br></pre></td></tr></table></figure>
<pre><code>Epoch: 0001 cost= 18419.810043182
Epoch: 0002 cost= 12060.265021591
Epoch: 0003 cost= 10484.025294886
Epoch: 0004 cost= 10011.340872727
Epoch: 0005 cost= 10345.419741477
Epoch: 0006 cost= 8558.077478977
Epoch: 0007 cost= 9358.221301705
Epoch: 0008 cost= 8207.935371023
Epoch: 0009 cost= 9071.398990341
Epoch: 0010 cost= 7567.627819318
Epoch: 0011 cost= 8493.544392614
Epoch: 0012 cost= 9373.162595455
Epoch: 0013 cost= 8513.334756818
Epoch: 0014 cost= 8793.045787500
Epoch: 0015 cost= 8341.089327841
Epoch: 0016 cost= 7673.378882386
Epoch: 0017 cost= 8378.051731818
Epoch: 0018 cost= 7996.387832386
Epoch: 0019 cost= 8001.048122727
Epoch: 0020 cost= 8242.288252273
Total cost: 661986.6
</code></pre>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积神经网络</title>
    <url>/2020/02/17/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><blockquote>
<p>该篇介绍了卷积神经网络的基本构造方法以及相关函数</p>
</blockquote>
<a id="more"></a>
<h3 id="卷积神经网络-1"><a href="#卷积神经网络-1" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><h3 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h3><ol>
<li><p>正则化，惩罚参数，避免过拟合</p>
<pre><code>  + tf.nn.l1_loss ,l1范数，使得参数稀疏，很多0
  + tf.nn.l2_loss ,l2范数，使得参数变小
</code></pre></li>
<li><p>广播机制（broadcasting）</p>
<script type="math/tex; mode=display">
一个300*10的矩阵和一个1*10的向量相加。和直觉相悖。\\
其实是在tensorflow中允许矩阵和向量相加。\\
C=A+b\\
即C[ij]=A[ij]+b[j]。也就是给矩阵A的每一行都加上向量b。\\
那么这至少要求矩阵的列数和向量的元素个数对齐。\\
这种隐式的复制向量b到很多位置的办法，叫做broadcasting。\\</script></li>
<li><p>tf.constant(value,dtype=None,shape=None,name=’Const’,verify_shape=False)</p>
</li>
<li></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="comment">###tf.constant(value,dtype=None,shape=None,name='Const',verify_shape=False)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"> </span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tf.nn.conv2d(</span></span><br><span class="line"><span class="string">    input,  </span></span><br><span class="line"><span class="string">    ##需要做卷积的输入图像（tensor），具有[batch,in_height,in_width,in_channels]这样的4维</span></span><br><span class="line"><span class="string">    filter,</span></span><br><span class="line"><span class="string">    ##相当于CNN中的卷积核，它是一个tensor，</span></span><br><span class="line"><span class="string">    ##shape是[filter_height,filter_width,in_channels,out_channels]</span></span><br><span class="line"><span class="string">    strides,</span></span><br><span class="line"><span class="string">    ## 卷积在每一维的步长，一般为一个一维向量，长度为4，一般为[1,stride,stride,1]。</span></span><br><span class="line"><span class="string">    padding,</span></span><br><span class="line"><span class="string">    ## 定义元素边框和元素内容之间的空间，只能是‘SAME’（边缘填充）或者‘VALID’（边缘不填充）</span></span><br><span class="line"><span class="string">    use_cudnn_on_gpu=True,</span></span><br><span class="line"><span class="string">    data_format='NHWC',</span></span><br><span class="line"><span class="string">    dilations=[1, 1, 1, 1],</span></span><br><span class="line"><span class="string">    name=None</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span>    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tf.nn.max_pool(value, ksize, strides, padding, name=None)</span></span><br><span class="line"><span class="string">参数是四个，和卷积很类似：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">第一个参数value：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">第二个参数ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">第三个参数strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">第四个参数padding：和卷积类似，可以取'VALID' 或者'SAME'</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                        strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)  </span><br><span class="line">                        </span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br><span class="line">                        </span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line"></span><br><span class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br><span class="line"></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">        x:batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">        print(<span class="string">"step %d, training accuracy %g"</span>%(i, train_accuracy),end=<span class="string">' '</span>)</span><br><span class="line">    train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">"test accuracy %g"</span>%accuracy.eval(feed_dict=&#123;</span><br><span class="line">    x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<pre><code>test accuracy 0.9489
</code></pre><h3 id="cifar卷积神经网络-未成功"><a href="#cifar卷积神经网络-未成功" class="headerlink" title="cifar卷积神经网络(未成功)"></a>cifar卷积神经网络(未成功)</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cifar10,cifar10_input</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">max_steps = <span class="number">3000</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">data_dir = <span class="string">'/cifar'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_with_weight_loss</span><span class="params">(shape, stddev, wl)</span>:</span></span><br><span class="line">    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))</span><br><span class="line">    <span class="keyword">if</span> wl <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        weight_loss = tf.multiply(tf.nn.l2_loss(var), wl, name=<span class="string">'weight_loss'</span>)</span><br><span class="line">        <span class="comment">##将参数统一放入losses中</span></span><br><span class="line">        tf.add_to_collection(<span class="string">'losses'</span>, weight_loss)</span><br><span class="line">    <span class="keyword">return</span> var</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(logits, labels)</span>:</span></span><br><span class="line">    </span><br><span class="line">    labels = tf.cast(labels, tf.int64)</span><br><span class="line">    </span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,labels)</span></span><br><span class="line"><span class="string">    相当于戳logits进行softmax后再计算与labels的交叉熵，不同的是labels不是one-hot型</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        logits=logits, labels=labels, name=<span class="string">'cross_entropy_per_example'</span>)</span><br><span class="line">    cross_entropy_mean = tf.reduce_mean(cross_entropy, name=<span class="string">'cross_entropy'</span>)</span><br><span class="line">    </span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>, cross_entropy_mean)</span><br><span class="line">    </span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">     tf.get_collection(key,scope=None)该函数可以用来获取key集合中的所有元素，返回一个列表。列表的顺序依变量放入集合中的先后而定。</span></span><br><span class="line"><span class="string">      tf.add_n 将列表里东西相加</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> tf.add_n(tf.get_collection(<span class="string">'losses'</span>), name=<span class="string">'total_loss'</span>)</span><br><span class="line">  </span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#cifar10.maybe_download_and_extract()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">images_train, labels_train = cifar10_input.distorted_inputs(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">images_test, labels_test = cifar10_input.inputs(eval_data=<span class="literal">True</span>,</span><br><span class="line">                                                batch_size=batch_size)                                                  </span><br><span class="line"><span class="comment">#images_train, labels_train = cifar10.distorted_inputs()</span></span><br><span class="line"><span class="comment">#images_test, labels_test = cifar10.inputs(eval_data=True)</span></span><br><span class="line"></span><br><span class="line">image_holder = tf.placeholder(tf.float32, [batch_size, <span class="number">24</span>, <span class="number">24</span>, <span class="number">3</span>])</span><br><span class="line">label_holder = tf.placeholder(tf.int32, [batch_size])</span><br><span class="line"></span><br><span class="line"><span class="comment">#logits = inference(image_holder)</span></span><br><span class="line"></span><br><span class="line">weight1 = variable_with_weight_loss(shape=[<span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">64</span>], stddev=<span class="number">5e-2</span>, wl=<span class="number">0.0</span>)</span><br><span class="line">kernel1 = tf.nn.conv2d(image_holder, weight1, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">bias1 = tf.Variable(tf.constant(<span class="number">0.0</span>, shape=[<span class="number">64</span>]))</span><br><span class="line">conv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))</span><br><span class="line">pool1 = tf.nn.max_pool(conv1, ksize=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                       padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="comment">### 基本废弃了</span></span><br><span class="line">norm1 = tf.nn.lrn(pool1, <span class="number">4</span>, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">weight2 = variable_with_weight_loss(shape=[<span class="number">5</span>, <span class="number">5</span>, <span class="number">64</span>, <span class="number">64</span>], stddev=<span class="number">5e-2</span>, wl=<span class="number">0.0</span>)</span><br><span class="line">kernel2 = tf.nn.conv2d(norm1, weight2, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">bias2 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">64</span>]))</span><br><span class="line">conv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))</span><br><span class="line">norm2 = tf.nn.lrn(conv2, <span class="number">4</span>, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>)</span><br><span class="line">pool2 = tf.nn.max_pool(norm2, ksize=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                       padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line">reshape = tf.reshape(pool2, [batch_size, <span class="number">-1</span>])</span><br><span class="line">dim = reshape.get_shape()[<span class="number">1</span>].value</span><br><span class="line"></span><br><span class="line">weight3 = variable_with_weight_loss(shape=[dim, <span class="number">384</span>], stddev=<span class="number">0.04</span>, wl=<span class="number">0.004</span>)</span><br><span class="line">bias3 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">384</span>]))</span><br><span class="line">local3 = tf.nn.relu(tf.matmul(reshape, weight3) + bias3)</span><br><span class="line"></span><br><span class="line">weight4 = variable_with_weight_loss(shape=[<span class="number">384</span>, <span class="number">192</span>], stddev=<span class="number">0.04</span>, wl=<span class="number">0.004</span>)</span><br><span class="line">bias4 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">192</span>]))                                      </span><br><span class="line">local4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)</span><br><span class="line"></span><br><span class="line">weight5 = variable_with_weight_loss(shape=[<span class="number">192</span>, <span class="number">10</span>], stddev=<span class="number">1</span>/<span class="number">192.0</span>, wl=<span class="number">0.0</span>)</span><br><span class="line">bias5 = tf.Variable(tf.constant(<span class="number">0.0</span>, shape=[<span class="number">10</span>]))</span><br><span class="line">logits = tf.add(tf.matmul(local4, weight5), bias5)</span><br><span class="line"></span><br><span class="line">loss = loss(logits, label_holder)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_op = tf.train.AdamOptimizer(<span class="number">1e-3</span>).minimize(loss) <span class="comment">#0.72</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tf.nn.in_top_k(predictions, targets, k)</span></span><br><span class="line"><span class="string">返回predictions第1-k的值的索引，看是否包含targets</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">top_k_op = tf.nn.in_top_k(logits, label_holder, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"></span><br><span class="line">tf.train.start_queue_runners()</span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(max_steps):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    image_batch,label_batch = sess.run([images_train,labels_train])</span><br><span class="line">    _, loss_value = sess.run([train_op, loss],feed_dict=&#123;image_holder: image_batch, </span><br><span class="line">                                                         label_holder:label_batch&#125;)</span><br><span class="line">    duration = time.time() - start_time</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        examples_per_sec = batch_size / duration</span><br><span class="line">        sec_per_batch = float(duration)</span><br><span class="line">    </span><br><span class="line">        format_str = (<span class="string">'step %d, loss = %.2f (%.1f examples/sec; %.3f sec/batch)'</span>)</span><br><span class="line">        print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))</span><br><span class="line">    </span><br><span class="line"><span class="comment">###</span></span><br><span class="line">num_examples = <span class="number">10000</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">num_iter = int(math.ceil(num_examples / batch_size))</span><br><span class="line">true_count = <span class="number">0</span>  </span><br><span class="line">total_sample_count = num_iter * batch_size</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> step &lt; num_iter:</span><br><span class="line">    image_batch,label_batch = sess.run([images_test,labels_test])</span><br><span class="line">    predictions = sess.run([top_k_op],feed_dict=&#123;image_holder: image_batch,</span><br><span class="line">                                                 label_holder:label_batch&#125;)</span><br><span class="line">    true_count += np.sum(predictions)</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">precision = true_count / total_sample_count</span><br><span class="line">print(<span class="string">'precision @ 1 = %.3f'</span> % precision)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>全连接神经网络</title>
    <url>/2020/02/17/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h1 id="全连接神经网络"><a href="#全连接神经网络" class="headerlink" title="全连接神经网络"></a>全连接神经网络</h1><blockquote>
<p>该篇介绍了全连接神经网络，做了mnist的数据集</p>
</blockquote>
<a id="more"></a>
<h3 id="全连接神经网络-1"><a href="#全连接神经网络-1" class="headerlink" title="全连接神经网络"></a>全连接神经网络</h3><p><strong>简单介绍：就相当于线性函数，用随机梯度优化算法进行优化</strong></p>
<blockquote>
<ul>
<li>生成随机数：tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) </li>
<li>reduce_mean(input_tensor, axis=None) 返回均值，axis是多维数组每个维度的坐标。还拿2维来说，数字3的坐标是[0, 1]，那么第一个数字0的axis是0，第二个数字1的axis是1</li>
<li>tf.cast（var，type）投射到指定类型</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">##会话的展开可以使用with tf.Session() as sess:</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 占位符，None可以批量处理数据</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## 实际证明 zeros有用，比random好</span></span><br><span class="line"><span class="comment">#tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) </span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 注意这里是matmul(矩阵乘)，而不是multiply或是*【这两者等价，皆为元素乘】</span></span><br><span class="line"><span class="comment">###  softmax是激活函数，就这个好用</span></span><br><span class="line">y_ = tf.nn.softmax(tf.matmul(x,W)+b)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 香农熵 y * tf.log(y_)</span></span><br><span class="line"><span class="comment">### reduce_mean(input_tensor, axis=None) 返回均值，sum返回和</span></span><br><span class="line"><span class="comment">## axis是多维数组每个维度的坐标。还拿2维来说，数字3的坐标是[0, 1]，那么第一个数字0的axis是0，第二个数字1的axis是1</span></span><br><span class="line"><span class="comment">### 因此axis = 0 为按第一维度操作</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 先sum每个样本的误差，然后求均值</span></span><br><span class="line">loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_), axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    xs,ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    sess.run(train_step,feed_dict=&#123;x:xs,y:ys&#125;)</span><br><span class="line">    <span class="keyword">if</span> (i%<span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">        print(sess.run(loss,feed_dict=&#123;x:xs,y:ys&#125;))</span><br><span class="line"></span><br><span class="line">        <span class="comment">### tf.argmax(input,axis)根据axis取值的不同返回每行或者每列最大值的索引</span></span><br><span class="line"><span class="comment">###equal，相等的意思。顾名思义，就是判断，x, y 是不是相等，它的判断方法不是整体判断，</span></span><br><span class="line"><span class="comment">###而是逐个元素进行判断，如果相等就是True，不相等，就是False。</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">### tf.cast投射到浮点数</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line">print(sess.run(accuracy,feed_dict=&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>
<pre><code>结果：
1.7234213
0.28298083
0.34306362
0.22262634
0.18778156
0.32757837
0.22582108
0.2445744
0.33943504
0.23404454

准确率：0.9178
</code></pre><h3 id="多层神经网络（MLP）"><a href="#多层神经网络（MLP）" class="headerlink" title="多层神经网络（MLP）"></a>多层神经网络（MLP）</h3><p>  <em>背景介绍（只加了层数和dropout）</em></p>
<ol>
<li><strong>Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如图1所示。</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape,stddev=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(tf.random_normal(shape=shape,stddev=stddev))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(tf.zeros(shape))</span><br><span class="line"></span><br><span class="line">in_units = <span class="number">784</span></span><br><span class="line">h1_units = <span class="number">300</span></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, in_units])</span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">W1 = weight_variable([in_units,h1_units])</span><br><span class="line">b1 = bias_variable([h1_units])</span><br><span class="line"></span><br><span class="line">hidden1 = tf.nn.relu(tf.matmul(x, W1) + b1)</span><br><span class="line">hidden1_drop = tf.nn.dropout(hidden1, keep_prob)</span><br><span class="line"></span><br><span class="line">W2 = weight_variable([h1_units,<span class="number">10</span>])</span><br><span class="line">b2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">y = tf.nn.softmax(tf.matmul(hidden1_drop, W2) + b2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define loss and optimizer</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), axis=<span class="number">1</span>))</span><br><span class="line">train_step = tf.train.AdagradOptimizer(<span class="number">0.3</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3000</span>):</span><br><span class="line">  batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">  train_step.run(&#123;x: batch_xs, y_: batch_ys, keep_prob: <span class="number">0.75</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test trained model</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">print(accuracy.eval(&#123;x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<pre><code>0.9791
</code></pre>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>时间模块</title>
    <url>/2020/02/17/%E6%97%B6%E9%97%B4%E6%A8%A1%E5%9D%97/</url>
    <content><![CDATA[<h1 id="时间"><a href="#时间" class="headerlink" title="时间"></a>时间</h1><blockquote>
<p>time 和calendar 的简单应用</p>
</blockquote>
<a id="more"></a>
<h3 id="时间模块"><a href="#时间模块" class="headerlink" title="时间模块"></a>时间模块</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment">#time.time()返回到1970年的s数，为浮点数</span></span><br><span class="line"><span class="comment">#localtime 返回时间元组</span></span><br><span class="line">localTime = time.localtime(time.time()) </span><br><span class="line">print(localTime)</span><br><span class="line"><span class="comment">#asctime返回一个可读的时间</span></span><br><span class="line">print(time.asctime(localTime))</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    %y 两位数的年份表示（00-99）</span></span><br><span class="line"><span class="string">    %Y 四位数的年份表示（000-9999）</span></span><br><span class="line"><span class="string">    %m 月份（01-12）</span></span><br><span class="line"><span class="string">    %d 月内中的一天（0-31）</span></span><br><span class="line"><span class="string">    %H 24小时制小时数（0-23）</span></span><br><span class="line"><span class="string">    %I 12小时制小时数（01-12）</span></span><br><span class="line"><span class="string">    %M 分钟数（00=59）</span></span><br><span class="line"><span class="string">    %S 秒（00-59）</span></span><br><span class="line"><span class="string">    %a 本地简化星期名称</span></span><br><span class="line"><span class="string">    %A 本地完整星期名称</span></span><br><span class="line"><span class="string">    %b 本地简化的月份名称</span></span><br><span class="line"><span class="string">    %B 本地完整的月份名称</span></span><br><span class="line"><span class="string">    %c 本地相应的日期表示和时间表示</span></span><br><span class="line"><span class="string">    %j 年内的一天（001-366）</span></span><br><span class="line"><span class="string">    %p 本地A.M.或P.M.的等价符</span></span><br><span class="line"><span class="string">    %U 一年中的星期数（00-53）星期天为星期的开始</span></span><br><span class="line"><span class="string">    %w 星期（0-6），星期天为星期的开始</span></span><br><span class="line"><span class="string">    %W 一年中的星期数（00-53）星期一为星期的开始</span></span><br><span class="line"><span class="string">    %x 本地相应的日期表示</span></span><br><span class="line"><span class="string">    %X 本地相应的时间表示</span></span><br><span class="line"><span class="string">    %Z 当前时区的名称</span></span><br><span class="line"><span class="string">    %% %号本身</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (time.strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>, time.localtime()))</span><br></pre></td></tr></table></figure>
<pre><code>time.struct_time(tm_year=2020, tm_mon=2, tm_mday=15, tm_hour=19, tm_min=37, tm_sec=33, tm_wday=5, tm_yday=46, tm_isdst=0)
Sat Feb 15 19:37:33 2020
2020-02-15 19:37:33
</code></pre><h3 id="日历"><a href="#日历" class="headerlink" title="日历"></a>日历</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> calendar</span><br><span class="line"></span><br><span class="line">cal = calendar.month(<span class="number">2020</span>,<span class="number">2</span>)</span><br><span class="line">print(cal)</span><br></pre></td></tr></table></figure>
<pre><code>   February 2020
Mo Tu We Th Fr Sa Su
                1  2
 3  4  5  6  7  8  9
10 11 12 13 14 15 16
17 18 19 20 21 22 23
24 25 26 27 28 29
</code></pre><p>​    </p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>-基础类 -语言</tag>
      </tags>
  </entry>
  <entry>
    <title>函数模块</title>
    <url>/2020/02/15/%E5%87%BD%E6%95%B0%E6%A8%A1%E5%9D%97/</url>
    <content><![CDATA[<h1 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h1><blockquote>
<p>该篇介绍了python的自定义函数</p>
</blockquote>
<a id="more"></a>
<h3 id="类型"><a href="#类型" class="headerlink" title="类型"></a>类型</h3><ol>
<li>可变类型和不可变对象<ul>
<li>strings, tuples, 和 numbers 是不可更改的对象，而 list,dict 等则是可以修改的对象。</li>
<li>不可变类型 形参结合时，形参是深拷贝，  可变类型则是浅拷贝，因此会影响原来元素</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#不可变类型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">changeInt</span><span class="params">(a)</span>:</span></span><br><span class="line">    a = <span class="number">10</span></span><br><span class="line">b = <span class="number">2</span></span><br><span class="line">changeInt(b)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">changeList</span><span class="params">(a)</span>:</span></span><br><span class="line">    a.extend([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> b</span><br><span class="line">b =[<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">changeList(b)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<pre><code>2
[2, 3, 1, 2]
</code></pre><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><ul>
<li>必备参数</li>
<li>关键字参数 ,可以不必要符合顺序</li>
<li>默认参数 ，事先设置参数</li>
<li>不定长参数,示例</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可写函数说明</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printinfo</span><span class="params">( arg1, *vartuple )</span>:</span></span><br><span class="line">   <span class="string">"打印任何传入的参数"</span></span><br><span class="line">   print( <span class="string">"输出: "</span>)</span><br><span class="line">   <span class="keyword">print</span> (arg1)</span><br><span class="line">   <span class="keyword">for</span> var <span class="keyword">in</span> vartuple:</span><br><span class="line">      <span class="keyword">print</span> (var)</span><br><span class="line">   <span class="keyword">return</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 调用printinfo 函数</span></span><br><span class="line">printinfo( <span class="number">10</span> )</span><br><span class="line">printinfo( <span class="number">70</span>, <span class="number">60</span>, <span class="number">50</span> )</span><br></pre></td></tr></table></figure>
<pre><code>输出: 
10
输出: 
70
60
50
</code></pre><h3 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h3><blockquote>
<p>同C语言的define方法，看示例</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sum = <span class="keyword">lambda</span> arg1, arg2: arg1 + arg2</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 调用sum函数</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"相加后的值为 : "</span>, sum( <span class="number">10</span>, <span class="number">20</span> ))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"相加后的值为 : "</span>, sum( <span class="number">20</span>, <span class="number">20</span> ))</span><br></pre></td></tr></table></figure>
<pre><code>相加后的值为 :  30
相加后的值为 :  40
</code></pre><h3 id="变量作用域"><a href="#变量作用域" class="headerlink" title="变量作用域"></a>变量作用域</h3><blockquote>
<p>同C语言</p>
</blockquote>
<h3 id="import-相关"><a href="#import-相关" class="headerlink" title="import 相关"></a>import 相关</h3><blockquote>
<p>略</p>
</blockquote>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>基础类</tag>
        <tag>语言</tag>
      </tags>
  </entry>
  <entry>
    <title>结构详细</title>
    <url>/2020/02/15/%E7%BB%93%E6%9E%84%E8%AF%A6%E7%BB%86/</url>
    <content><![CDATA[<h1 id="结构详细"><a href="#结构详细" class="headerlink" title="结构详细"></a>结构详细</h1><blockquote>
<p>该篇介绍了python的详细结构</p>
</blockquote>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="comment">#dir(math)</span></span><br></pre></td></tr></table></figure>
<h3 id="random-随机函数"><a href="#random-随机函数" class="headerlink" title="random 随机函数"></a>random 随机函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> *</span><br><span class="line">print(choice(range(<span class="number">10</span>))) <span class="comment">#简单</span></span><br><span class="line">print(randrange(<span class="number">1</span>,<span class="number">10</span>,<span class="number">2</span>))  <span class="comment">#x,y,z,为start，end，step</span></span><br><span class="line">print(random())</span><br><span class="line">print(uniform(<span class="number">1</span>,<span class="number">2</span>))  <span class="comment">#x,y为start和end</span></span><br></pre></td></tr></table></figure>
<pre><code>3
1
0.17893877145507286
1.054488927162555
</code></pre><h3 id="字符串格式化"><a href="#字符串格式化" class="headerlink" title="字符串格式化"></a>字符串格式化</h3><blockquote>
<p>同C语言</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"my nameis %s,%d years"</span>%(<span class="string">'yb'</span>,<span class="number">20</span>))</span><br></pre></td></tr></table></figure>
<pre><code>my nameis yb,20 years
</code></pre><h3 id="列表详细函数"><a href="#列表详细函数" class="headerlink" title="列表详细函数"></a>列表详细函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list =[]</span><br><span class="line">list.append(<span class="string">"fool"</span>)   <span class="comment">#尾部添加单个元素</span></span><br><span class="line">list.extend([<span class="string">"genius"</span>]) <span class="comment">#尾部添加列表</span></span><br><span class="line"></span><br><span class="line">print(list)</span><br><span class="line"><span class="keyword">del</span> list[<span class="number">0</span>]</span><br><span class="line">print(list)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">list.reverse()</span></span><br><span class="line"><span class="string">反向列表中元素</span></span><br><span class="line"><span class="string">list.sort(cmp=None, key=None, reverse=False)</span></span><br><span class="line"><span class="string">对原列表进行排序</span></span><br><span class="line"><span class="string">list.remove(obj)</span></span><br><span class="line"><span class="string">移除列表中某个值的第一个匹配项</span></span><br><span class="line"><span class="string">list.insert(index, obj)</span></span><br><span class="line"><span class="string">将对象插入列表</span></span><br><span class="line"><span class="string">list.count(obj) </span></span><br><span class="line"><span class="string">统计某个元素在列表中出现的次数</span></span><br><span class="line"><span class="string">list.index(obj)</span></span><br><span class="line"><span class="string">从列表中找出某个值第一个匹配项的索引位置</span></span><br><span class="line"><span class="string">cmp(list1, list2)</span></span><br><span class="line"><span class="string">比较两个列表的元素</span></span><br><span class="line"><span class="string">len(list)</span></span><br><span class="line"><span class="string">列表元素个数</span></span><br><span class="line"><span class="string">max(list)</span></span><br><span class="line"><span class="string">返回列表元素最大值</span></span><br><span class="line"><span class="string">list(seq)</span></span><br><span class="line"><span class="string">将元组转换为列表</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<pre><code>[&#39;fool&#39;, &#39;genius&#39;]
[&#39;genius&#39;]
</code></pre><h3 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h3><ul>
<li>注意一个元素的元组为 tup1 = (50,)</li>
<li>注意不允许删改，必须新建</li>
<li>方法同列表</li>
</ul>
<h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><ul>
<li>键不可变，值可以</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict = &#123;<span class="string">'Name'</span>: <span class="string">'Zara'</span>, <span class="string">'Age'</span>: <span class="number">7</span>, <span class="string">'Class'</span>: <span class="string">'First'</span>&#125;</span><br><span class="line">dict[<span class="string">'Name'</span>]=<span class="string">'yb'</span></span><br><span class="line">dict[<span class="string">'x'</span>]=<span class="number">4</span></span><br><span class="line">print(dict)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dict.keys():   <span class="comment">#keys返回键值</span></span><br><span class="line">    print(dict[i],end=<span class="string">' '</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dict.values():  <span class="comment">#values返回值</span></span><br><span class="line">    print(i,end=<span class="string">' '</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dict.items():   <span class="comment"># items 返回键和值的二元组</span></span><br><span class="line">    print(i[<span class="number">0</span>],end=<span class="string">' '</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">dict.clear()</span></span><br><span class="line"><span class="string">删除字典内所有元素</span></span><br><span class="line"><span class="string">dict.copy()</span></span><br><span class="line"><span class="string">返回一个字典的浅复制</span></span><br><span class="line"><span class="string">dict.deepcopy()</span></span><br><span class="line"><span class="string">返回一个字典的深复制</span></span><br><span class="line"><span class="string">dict.fromkeys(seq[, val])</span></span><br><span class="line"><span class="string">创建一个新字典，以序列 seq 中元素做字典的键，val 为字典所有键对应的初始值</span></span><br><span class="line"><span class="string">dict.update(dict2)</span></span><br><span class="line"><span class="string">把字典dict2的键/值对更新到dict里</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<pre><code>{&#39;Name&#39;: &#39;yb&#39;, &#39;Age&#39;: 7, &#39;Class&#39;: &#39;First&#39;, &#39;x&#39;: 4}
yb 7 First 4 yb 7 First 4 Name
Age
Class
x
</code></pre>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>基础类</tag>
        <tag>语言</tag>
      </tags>
  </entry>
  <entry>
    <title>基本语句</title>
    <url>/2020/02/15/%E5%9F%BA%E6%9C%AC%E8%AF%AD%E5%8F%A5/</url>
    <content><![CDATA[<h1 id="基本语句"><a href="#基本语句" class="headerlink" title="基本语句"></a>基本语句</h1><blockquote>
<p>该篇介绍了python的基本语句</p>
</blockquote>
<a id="more"></a>
<h3 id="条件语句"><a href="#条件语句" class="headerlink" title="条件语句"></a>条件语句</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if 判断条件：</span><br><span class="line">    执行语句……</span><br><span class="line">elif 判断条件：</span><br><span class="line">    执行语句</span><br><span class="line">else：</span><br><span class="line">    执行语句……</span><br></pre></td></tr></table></figure>
<h3 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h3><ol>
<li>while循环</li>
<li>for循环</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> (count &lt; <span class="number">9</span>):</span><br><span class="line">   <span class="keyword">print</span> (<span class="string">'The count is:'</span>, count)</span><br><span class="line">   count = count + <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"循环结束"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Good bye!"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>The count is: 0
The count is: 1
The count is: 2
The count is: 3
The count is: 4
The count is: 5
The count is: 6
The count is: 7
The count is: 8
循环结束
Good bye!
</code></pre><h4 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> letter <span class="keyword">in</span> <span class="string">'Python'</span>:     <span class="comment"># 第一个实例</span></span><br><span class="line">   <span class="keyword">print</span> (<span class="string">'当前字母 :'</span>, letter)</span><br><span class="line"> </span><br><span class="line">fruits = [<span class="string">'banana'</span>, <span class="string">'apple'</span>,  <span class="string">'mango'</span>]</span><br><span class="line"><span class="keyword">for</span> fruit <span class="keyword">in</span> fruits:        <span class="comment"># 第二个实例</span></span><br><span class="line">   <span class="keyword">print</span> (<span class="string">'当前水果 :'</span>, fruit)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(fruits)):</span><br><span class="line">    print(fruits[i])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"x"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Good bye!"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>当前字母 : P
当前字母 : y
当前字母 : t
当前字母 : h
当前字母 : o
当前字母 : n
当前水果 : banana
当前水果 : apple
当前水果 : mango
banana
apple
mango
x
Good bye!
</code></pre><h4 id="实践-1"><a href="#实践-1" class="headerlink" title="实践"></a>实践</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">20</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>,i):</span><br><span class="line">        <span class="keyword">if</span>(i%j==<span class="number">0</span>):</span><br><span class="line">            print(i,end=<span class="string">' '</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br></pre></td></tr></table></figure>
<pre><code>4 6 8 9 10 12 14 15 16 18 
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">i = <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span>(i&lt;<span class="number">20</span>):</span><br><span class="line">    j = <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span>(j&lt;i):</span><br><span class="line">        <span class="keyword">if</span>(i%j == <span class="number">0</span>): <span class="keyword">break</span></span><br><span class="line">        j=j+<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span>(j&gt;=i): print(i,<span class="string">"是素数"</span>,end=<span class="string">' '</span>)</span><br><span class="line">    i=i+<span class="number">1</span></span><br></pre></td></tr></table></figure>
<pre><code>2 是素数 3 是素数 5 是素数 7 是素数 11 是素数 13 是素数 17 是素数 19 是素数 
</code></pre>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>基础类</tag>
        <tag>语言</tag>
      </tags>
  </entry>
  <entry>
    <title>变量类型及数据结构</title>
    <url>/2020/02/15/%E5%8F%98%E9%87%8F%E7%B1%BB%E5%9E%8B%E5%8F%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<h1 id="变量类型及数据结构"><a href="#变量类型及数据结构" class="headerlink" title="变量类型及数据结构"></a>变量类型及数据结构</h1><blockquote>
<p>该篇介绍了python的变量类型，字符串以及简单的列表，元组和字典</p>
</blockquote>
<a id="more"></a>
<h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><ol>
<li><p>s=”a1a2···an”(n&gt;=0) </p>
<ul>
<li>从左到右索引<strong>默认0开始</strong>的，最大范围是字符串长度少1</li>
<li>从右到左索引默认-1开始的，最大范围是字符串开头</li>
</ul>
</li>
<li><p>加号（+）是字符串连接运算符，星号（*）是重复操作</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">str = <span class="string">"hello,world"</span></span><br><span class="line">print(str)      <span class="comment"># 输出完成字符串</span></span><br><span class="line">print(str[<span class="number">2</span>])   <span class="comment"># 输出字符串的地三个字符</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## [x,y,z] 输出x到y间的字符，包括x，不包括y，步长为z</span></span><br><span class="line">print(str[<span class="number">2</span>:<span class="number">5</span>:<span class="number">2</span>]) <span class="comment"># 输出第三道第六个字符，不包括第六个</span></span><br><span class="line">print(str[<span class="number">2</span>:])  <span class="comment"># 输出第3个开头</span></span><br><span class="line">print(str*<span class="number">2</span>)</span><br><span class="line">print(str+<span class="string">"s"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>hello,world
l
lo
llo,world
hello,worldhello,world
hello,worlds
</code></pre><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><blockquote>
<p>操作同字符串</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list = [ <span class="string">'runoob'</span>, <span class="number">786</span> , <span class="number">2.23</span>, <span class="string">'john'</span>, <span class="number">70.2</span> ]</span><br><span class="line">tinylist = [<span class="number">123</span>, <span class="string">'john'</span>]</span><br><span class="line"> </span><br><span class="line"><span class="keyword">print</span> (list)               <span class="comment"># 输出完整列表</span></span><br><span class="line"><span class="keyword">print</span> (list[<span class="number">0</span>])            <span class="comment"># 输出列表的第一个元素</span></span><br><span class="line"><span class="keyword">print</span> (list[<span class="number">1</span>:<span class="number">3</span>] )         <span class="comment"># 输出第二个至第三个元素 </span></span><br><span class="line"><span class="keyword">print</span> (list[<span class="number">2</span>:] )          <span class="comment"># 输出从第三个开始至列表末尾的所有元素</span></span><br><span class="line"><span class="keyword">print</span> (tinylist * <span class="number">2</span>)       <span class="comment"># 输出列表两次</span></span><br><span class="line"><span class="keyword">print</span> (list + tinylist)    <span class="comment"># 打印组合的列表</span></span><br></pre></td></tr></table></figure>
<pre><code>[&#39;runoob&#39;, 786, 2.23, &#39;john&#39;, 70.2]
runoob
[786, 2.23]
[2.23, &#39;john&#39;, 70.2]
[123, &#39;john&#39;, 123, &#39;john&#39;]
[&#39;runoob&#39;, 786, 2.23, &#39;john&#39;, 70.2, 123, &#39;john&#39;]
</code></pre><h3 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h3><blockquote>
<p>相当于只读列表</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list = (<span class="string">'runoob'</span>, <span class="number">786</span> , <span class="number">2.23</span>, <span class="string">'john'</span>, <span class="number">70.2</span> )</span><br><span class="line">tinylist = (<span class="number">123</span>, <span class="string">'john'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">print</span> (list)               <span class="comment"># 输出完整列表</span></span><br><span class="line"><span class="keyword">print</span> (list[<span class="number">0</span>])            <span class="comment"># 输出列表的第一个元素</span></span><br><span class="line"><span class="keyword">print</span> (list[<span class="number">1</span>:<span class="number">3</span>] )         <span class="comment"># 输出第二个至第三个元素 </span></span><br><span class="line"><span class="keyword">print</span> (list[<span class="number">2</span>:] )          <span class="comment"># 输出从第三个开始至列表末尾的所有元素</span></span><br><span class="line"><span class="keyword">print</span> (tinylist * <span class="number">2</span>)       <span class="comment"># 输出列表两次</span></span><br><span class="line"><span class="keyword">print</span> (list + tinylist)    <span class="comment"># 打印组合的列表</span></span><br></pre></td></tr></table></figure>
<pre><code>(&#39;runoob&#39;, 786, 2.23, &#39;john&#39;, 70.2)
runoob
(786, 2.23)
(2.23, &#39;john&#39;, 70.2)
(123, &#39;john&#39;, 123, &#39;john&#39;)
(&#39;runoob&#39;, 786, 2.23, &#39;john&#39;, 70.2, 123, &#39;john&#39;)
</code></pre><h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><blockquote>
<p>通过键来存取，而非偏移</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict = &#123;&#125;</span><br><span class="line">dict[<span class="string">'one'</span>] = <span class="string">"This is one"</span></span><br><span class="line">dict[<span class="number">2</span>] = <span class="string">"This is two"</span></span><br><span class="line">tinydict = &#123;<span class="string">'name'</span>: <span class="string">'john'</span>,<span class="string">'code'</span>:<span class="number">6734</span>, <span class="string">'dept'</span>: <span class="string">'sales'</span>&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">print</span> (dict[<span class="string">'one'</span>])          <span class="comment"># 输出键为'one' 的值</span></span><br><span class="line"><span class="keyword">print</span> (dict[<span class="number">2</span>])              <span class="comment"># 输出键为 2 的值</span></span><br><span class="line"><span class="keyword">print</span> (tinydict)             <span class="comment"># 输出完整的字典</span></span><br><span class="line"><span class="keyword">print</span> (tinydict.keys())      <span class="comment"># 输出所有键,结果为列表</span></span><br><span class="line"><span class="keyword">print</span> (tinydict.values())    <span class="comment"># 输出所有值，结果为列表</span></span><br></pre></td></tr></table></figure>
<pre><code>This is one
This is two
{&#39;name&#39;: &#39;john&#39;, &#39;code&#39;: 6734, &#39;dept&#39;: &#39;sales&#39;}
dict_keys([&#39;name&#39;, &#39;code&#39;, &#39;dept&#39;])
dict_values([&#39;john&#39;, 6734, &#39;sales&#39;])
</code></pre><h3 id="数据转换类型"><a href="#数据转换类型" class="headerlink" title="数据转换类型"></a>数据转换类型</h3><p>列举较为重要的</p>
<ul>
<li>tuple(s),list(s)</li>
<li>set(s) 转换为可变集合</li>
<li>frozenset(s) 不可变集合</li>
<li>dict(s) 创建一个字典。d 必须是一个序列 (key,value)元组。</li>
</ul>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>基础类</tag>
        <tag>语言</tag>
      </tags>
  </entry>
  <entry>
    <title>markdown语法</title>
    <url>/2020/02/15/markdown%E8%AF%AD%E6%B3%95/</url>
    <content><![CDATA[<h1 id="markdown基本语法"><a href="#markdown基本语法" class="headerlink" title="markdown基本语法"></a>markdown基本语法</h1><blockquote>
<p>该篇介绍了markdown 的基本语法及typora的快捷键</p>
</blockquote>
<a id="more"></a>
<h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><ol>
<li>n个#代表了n级标题</li>
<li>typora 快捷键 ctrl+ n</li>
</ol>
<h3 id="粗体-斜体、删除线和下划线"><a href="#粗体-斜体、删除线和下划线" class="headerlink" title="粗体 斜体、删除线和下划线"></a>粗体 斜体、删除线和下划线</h3><ol>
<li><em>斜体</em>  （**包围）快捷键ctrl+i  （italic）</li>
<li><strong>粗体</strong>（四个*包围）快捷键 ctrl+b （bold）</li>
<li><strong><em>加粗斜体</em></strong>  （6分*包围） 无快捷键</li>
<li><del>删除线</del> （4个~包围）无快捷键</li>
<li>下划线 快捷键 ctrl+u （underline） <u>快乐</u></li>
</ol>
<h3 id="引用块"><a href="#引用块" class="headerlink" title="引用块"></a>引用块</h3><blockquote>
<p>文字 用&gt;开头</p>
</blockquote>
<h3 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h3><p><code>行内代码</code>   //用两个`包围，必须时英文字符</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">多行代码 &#x2F;&#x2F;用6个&#96;包围 ，必须英文</span><br></pre></td></tr></table></figure>
<h3 id="公式块"><a href="#公式块" class="headerlink" title="公式块"></a>公式块</h3><script type="math/tex; mode=display">
数学公式，用$$$$包围</script><h3 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h3><ol>
<li><hr>
<p>用3个-或+或*都可</p>
</li>
</ol>
<h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><ul>
<li>+,-,*代表无须列表</li>
<li>1.代表有序列表项</li>
</ul>
<h3 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h3><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>直接记忆快捷项 ctrl+t（table)</p>
<h3 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h3><p><a href="www.baidu.com">ssss</a></p>
<www.baidu.com>

<p>1.用【l链接文字】（链接地址）//用英文 快捷键ctrl+k </p>
<p>2.用《链接地址》 //用英文</p>
<h3 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![图片文字](图片地址 &quot;图片描述&quot;)</span><br></pre></td></tr></table></figure>
<p>快捷键ctrl+shift+i (image)</p>
</www.baidu.com>]]></content>
      <categories>
        <category>makedown</category>
      </categories>
      <tags>
        <tag>基础类</tag>
      </tags>
  </entry>
  <entry>
    <title>python_start</title>
    <url>/2020/02/15/python-start/</url>
    <content><![CDATA[<h1 id="python基础"><a href="#python基础" class="headerlink" title="python基础"></a>python基础</h1><blockquote>
<p>该篇介绍了python 的基础</p>
</blockquote>
<a id="more"></a>
<h3 id="标识符"><a href="#标识符" class="headerlink" title="标识符"></a>标识符</h3><ul>
<li>在 Python 里，标识符由字母、数字、下划线组成。</li>
<li>在 Python 中，所有标识符可以包括英文、数字以及下划线(_)，但不能以数字开头。</li>
<li>Python 中的标识符是区分大小写的。</li>
<li>以下划线开头的标识符是有特殊意义的。以单下划线开头 _foo 的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用 from xxx import * 而导入。</li>
<li>以双下划线开头的 <strong>foo 代表类的私有成员，以双下划线开头和结尾的 </strong>foo<strong> 代表 Python 里特殊方法专用的标识，如</strong> <strong>init__</strong>() 代表类的构造函数。</li>
</ul>
<hr>
<h3 id="保留字"><a href="#保留字" class="headerlink" title="保留字"></a>保留字</h3><blockquote>
<p>略</p>
</blockquote>
<h3 id="行和缩进"><a href="#行和缩进" class="headerlink" title="行和缩进"></a>行和缩进</h3><blockquote>
<p>4个空格</p>
</blockquote>
<hr>
<h3 id="多行语句"><a href="#多行语句" class="headerlink" title="多行语句"></a>多行语句</h3><blockquote>
<p>\</p>
</blockquote>
<hr>
<h3 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h3><ul>
<li><h1 id="单行"><a href="#单行" class="headerlink" title="单行"></a>单行</h1></li>
<li>“”” “””  多行注释</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注释</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">这是多行注释，使用双引号</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print(<span class="string">"按下enter键退出\n"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>按下enter键退出
</code></pre><h3 id="print"><a href="#print" class="headerlink" title="print"></a>print</h3><blockquote>
<p><del>不换行加逗号</del>,实测没用，用第一种方法吧</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = <span class="string">"a"</span></span><br><span class="line">y = <span class="string">"b"</span></span><br><span class="line">print(x,y)</span><br><span class="line">print(x)</span><br><span class="line">print(<span class="string">"-----"</span>)</span><br><span class="line">print(x,)</span><br><span class="line">print(y,)</span><br></pre></td></tr></table></figure>
<pre><code>a b
a
-----
a
b
</code></pre><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p><strong>多变量赋值(特别是内存的思考)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">a=b=c a,b,c分在同一个内存区</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">a=b=c=<span class="number">1</span> </span><br><span class="line">print(a)</span><br><span class="line">b=<span class="number">2</span></span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">q,w=1,1 在多个位置</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">q,w =<span class="number">1</span>,<span class="number">1</span></span><br><span class="line">print(q)</span><br><span class="line">w=<span class="number">2</span></span><br><span class="line">print(w)</span><br><span class="line"></span><br><span class="line"><span class="comment">## del 删除了引用，但不是删除空间，所以b可以用</span></span><br><span class="line"><span class="keyword">del</span> a  </span><br><span class="line"><span class="comment">#print(a) 报错</span></span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<pre><code>1
2
1
2
2
</code></pre><h3 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h3><p>挑有用的讲</p>
<ul>
<li>a**b ，返回a的b次幂</li>
<li>a//b ，返回a整除b，向下取整</li>
<li>逻辑符为 and or not</li>
<li>成员运算符 in 和not in  如果在指定的序列中找到值返回 True，否则返回 False。</li>
<li>身份运算符 is 和is not    is 是判断两个标识符是不是引用自一个对象</li>
</ul>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>基础类</tag>
        <tag>语言</tag>
      </tags>
  </entry>
</search>
