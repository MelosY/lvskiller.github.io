<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>PeLeeNet代码解读</title>
    <url>/2021/01/07/PeLeeNet%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<h1 id="PeLeeNet代码解读"><a href="#PeLeeNet代码解读" class="headerlink" title="PeLeeNet代码解读"></a>PeLeeNet代码解读</h1><blockquote>
<p>该论文解读，也将成为母版。</p>
<p>PeleeNet是一种基于Densenet的轻量化SSD网络变体,所以参数量更小，可以在mobile端跑。</p>
<p>解读代码：<a href="https://github.com/yxlijun/Pelee.Pytorch" target="_blank" rel="noopener">https://github.com/yxlijun/Pelee.Pytorch</a></p>
</blockquote>
<a id="more"></a>
<p>本文将以train.py为索引解读其中用到的其他模块，以点带面介绍完该代码。</p>
<p>​        首先是<code>if __name == &#39;__main__&#39;:</code>这句话有什么作用，简单来说就是使得其他模块写的在外面的东西不能执行，只执行本模块的东西，因为当前运行模块就会被自动指定为main，其他则是自己的模块名字。</p>
<p>   enumerate的使用，大部分地方都会使用这个东西,看一下就知道了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;seasons = [<span class="string">'Spring'</span>, <span class="string">'Summer'</span>, <span class="string">'Fall'</span>, <span class="string">'Winter'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(enumerate(seasons))</span><br><span class="line">[(<span class="number">0</span>, <span class="string">'Spring'</span>), (<span class="number">1</span>, <span class="string">'Summer'</span>), (<span class="number">2</span>, <span class="string">'Fall'</span>), (<span class="number">3</span>, <span class="string">'Winter'</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(enumerate(seasons, start=<span class="number">1</span>))       <span class="comment"># 下标从 1 开始</span></span><br><span class="line">[(<span class="number">1</span>, <span class="string">'Spring'</span>), (<span class="number">2</span>, <span class="string">'Summer'</span>), (<span class="number">3</span>, <span class="string">'Fall'</span>), (<span class="number">4</span>, <span class="string">'Winter'</span>)]</span><br></pre></td></tr></table></figure>
<h3 id="Parser的使用"><a href="#Parser的使用" class="headerlink" title="Parser的使用"></a>Parser的使用</h3><blockquote>
<p>文章一开头就是Parser，这是常用的设置参数的方法，方便程序调试输出，有它的存在就可以直接利用 后缀 </p>
<p>-h 来输出程序所需要的参数。（当然也有人喜欢用一个Config来代表，甚至混着用，这里就看个人喜好了）。推荐还是使用Parser，还是蛮香的。</p>
</blockquote>
<p>先介绍一下parser的用法：</p>
<ol>
<li>调包，import argparse</li>
<li>创建 ,  <code>parser = argparse.ArgumentParser(description=&quot;your script description&quot;)</code>          # description参数可以用于插入描述脚本用途的信息，可以为空</li>
<li>添加,  <code>parser.add_argument(&#39;--verbose&#39;, &#39;-v&#39;, default=1, type=int,help=&#39;verbose mode&#39;)</code>  # 添加—verbose标签，标签别名可以为-v(别名可以不设置)，default和type不必多说，help则是你-h会输出的信息，建议设置，相当于参数的注释。</li>
<li>放进字典 <code>args = parser.parse_args()</code> ，将变量以标签-值的字典形式存入args字典，这样就可以将参数按照字典方式调取。</li>
</ol>
<p>一般这些就够了，如果有高端操作，以后可以补充。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'Pelee Training'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-c'</span>, <span class="string">'--config'</span>, default=<span class="string">'configs/Pelee_VOC.py'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-d'</span>, <span class="string">'--dataset'</span>, default=<span class="string">'VOC'</span>,</span><br><span class="line">                    help=<span class="string">'VOC or COCO dataset'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--ngpu'</span>, default=<span class="number">1</span>, type=int, help=<span class="string">'gpus'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--resume_net'</span>, default=<span class="literal">None</span>,</span><br><span class="line">                    help=<span class="string">'resume net for retraining'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--resume_epoch'</span>, default=<span class="number">0</span>, type=int,</span><br><span class="line">                    help=<span class="string">'resume iter for retraining'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-t'</span>, <span class="string">'--tensorboard'</span>, type=bool,</span><br><span class="line">                    default=<span class="literal">False</span>, help=<span class="string">'Use tensorborad to show the Loss Graph'</span>)</span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure>
<p>代码有上述一段，就完成了很简单的多参数的设计。</p>
<h3 id="输出设置"><a href="#输出设置" class="headerlink" title="输出设置"></a>输出设置</h3><blockquote>
<p>这里要介绍一个让输出好看的库 termcolor</p>
</blockquote>
<p>简单介绍一下用法</p>
<ol>
<li>调库，from termcolor import colored, cprint  </li>
<li>输出，有两种方法<ul>
<li>text = colored(‘Hello, World!’, ‘red’, attrs=[‘reverse’, ‘blink’])<br>print(text)  </li>
<li>cprint(‘Hello, World!’, ‘green’, ‘on_red’)  </li>
</ul>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print_info(&#39;----------------------------------------------------------------------\n&#39;</span><br><span class="line">           &#39;|                       Pelee Training Program                       |\n&#39;</span><br><span class="line">           &#39;----------------------------------------------------------------------&#39;, [&#39;yellow&#39;, &#39;bold&#39;])</span><br><span class="line">logger &#x3D; set_logger(args.tensorboard)</span><br></pre></td></tr></table></figure>
<p>这段代码的print_info 就是用util.core 的cprint来实现的黑色粗体输出。</p>
<p>这里的logger就用了一个logger库的东西，暂时跳过，不影响。</p>
<h3 id="Config"><a href="#Config" class="headerlink" title="Config"></a>Config</h3><blockquote>
<p>他用了双重的设置</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">global cfg #Python中定义函数时，若想在函数内部对函数外的变量进行操作，就需要在函数内部声明其为global。</span><br><span class="line">cfg &#x3D; Config.fromfile(args.config)</span><br></pre></td></tr></table></figure>
<p>这里他用了一个global，应该是全局通用的意思，设置了cfg，这里就涉及了它config的设计了，他设计的比较复杂，我也没兴趣了解，但是它用dict来存储的思想还是不错的，这样就能分的比较开了。这里简单看一下。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">loss &#x3D; dict(overlap_thresh&#x3D;0.5,</span><br><span class="line">            prior_for_matching&#x3D;True,</span><br><span class="line">            bkg_label&#x3D;0,</span><br><span class="line">            neg_mining&#x3D;True,</span><br><span class="line">            neg_pos&#x3D;3,</span><br><span class="line">            neg_overlap&#x3D;0.5,</span><br><span class="line">            encode_target&#x3D;False)</span><br></pre></td></tr></table></figure>
<p>每个模块分得比较分明。</p>
<h3 id="网络设计"><a href="#网络设计" class="headerlink" title="网络设计"></a>网络设计</h3><p>主函数一句 net = build_net(‘train’, cfg.model.input_size, cfg.model)就完成了搭建网络，我们去看一下具体网络设计。</p>
<p>主流写法就是将小网络搭起来构成大网络，小的重复的块写成类，重复调用，减少代码重复。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">conv_bn_relu</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment">#构建一个conv 后bn再relu的块</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, activation=True, **kwargs)</span>:</span></span><br><span class="line">        super(conv_bn_relu, self).__init__()<span class="comment">#调用上级进行初始化</span></span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, bias=<span class="literal">False</span>, **kwargs)</span><br><span class="line">        self.norm = nn.BatchNorm2d(out_channels)</span><br><span class="line">        self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.norm(self.conv(x))</span><br><span class="line">        <span class="keyword">if</span> self.activation:</span><br><span class="line">            out = F.relu(out, inplace=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>尽管是小网络，但是<code>__init__</code>和<code>forward</code>都不能省略，同理还有<code>ResBlock</code>和<code>conv_relu</code>这些都比较简单，不做赘述。</p>
<p>接下来是核心模块了，这里我们看一下论文内容中搭建的模块，</p>
<h4 id="StemBlock模块"><a href="#StemBlock模块" class="headerlink" title="StemBlock模块"></a>StemBlock模块</h4><p>直接上代码，其中相关函数就在上面，可以理解一下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_StemBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_input_channels, num_init_features)</span>:</span></span><br><span class="line">        super(_StemBlock, self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#二阶段分支2的中间channel</span></span><br><span class="line">        num_stem_features = int(num_init_features / <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#一阶段</span></span><br><span class="line">        self.stem1 = conv_bn_relu(</span><br><span class="line">            num_input_channels, num_init_features, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#二阶段分支2</span></span><br><span class="line">        self.stem2a = conv_bn_relu(</span><br><span class="line">            num_init_features, num_stem_features, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.stem2b = conv_bn_relu(</span><br><span class="line">            num_stem_features, num_init_features, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#三阶段</span></span><br><span class="line">        self.stem3 = conv_bn_relu(</span><br><span class="line">            <span class="number">2</span> * num_init_features, num_init_features, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#二阶段分支1</span></span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.stem1(x)</span><br><span class="line"></span><br><span class="line">        branch2 = self.stem2a(out)</span><br><span class="line">        branch2 = self.stem2b(branch2)</span><br><span class="line">        branch1 = self.pool(out)</span><br><span class="line"></span><br><span class="line">        out = torch.cat([branch1, branch2], dim=<span class="number">1</span>)</span><br><span class="line">        out = self.stem3(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>有点晕是吧，对应下图食用，注释已经处理完毕了。</p>
<p><img src="/2021/01/07/PeLeeNet%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Users/31311/Desktop/实习报告/PeLeeNet代码解读/StemBlock.png" alt></p>
<p>并且在不增加较多计算量的前提下，该模块能够确保较强的特征表达能力。</p>
<h4 id="DenseBlock"><a href="#DenseBlock" class="headerlink" title="DenseBlock"></a>DenseBlock</h4><p>这次就先放图了，避免突然死亡.右边的才是本次实现的</p>
<p>受Inception结构的启发，由两路分别捕捉不同尺度感受野信息的网络分支构成。第一路经过一层1x1卷积完成bottleneck之后，再经过一层3x3卷积；第二路则在bottleneck之后，再经过两层3x3卷积。</p>
<p>首先明确一点，这里的k是growth_rate,也即增长的东西，简单来说，就是为原有数据增加一些提取的信息。</p>
<p><img src="/2021/01/07/PeLeeNet%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Users/31311/Desktop/实习报告/PeLeeNet代码解读/DenseLayer.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_DenseLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""docstring for _DenseLayer"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_input_features, growth_rate, bottleneck_width, drop_rate)</span>:</span></span><br><span class="line">        super(_DenseLayer, self).__init__()</span><br><span class="line">        growth_rate = growth_rate // <span class="number">2</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">## 这里和论文不一致，可能这就是它优于原模型的原因</span></span><br><span class="line">        <span class="comment">## 原本1x1卷积后出来的是2k，这里就变成了一个可变参量，这个是在cfg中有设定</span></span><br><span class="line">        <span class="comment">## block_config就是这个参量 [3, 4, 8, 6]</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Dynamic Number of Channels in Bottleneck Layer：瓶颈层（1x1卷积层）的输出通道数随输入形        状而变化，而并非DenseNet中growth-rate的4倍（growth-rate表示每经过一个dense block，所增加的        通道数），从而确保瓶颈层的计算量不会显著增加,别的地方也有设计。不算优化。</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        inter_channel = int(growth_rate * bottleneck_width / <span class="number">4</span>) * <span class="number">4</span></span><br><span class="line">        <span class="keyword">if</span> inter_channel &gt; num_input_features / <span class="number">2</span>:</span><br><span class="line">            inter_channel = int(num_input_features / <span class="number">8</span>) * <span class="number">4</span></span><br><span class="line">            print(<span class="string">'adjust inter_channel to '</span>, inter_channel)</span><br><span class="line"></span><br><span class="line">        self.branch1a = conv_bn_relu(</span><br><span class="line">            num_input_features, inter_channel, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch1b = conv_bn_relu(</span><br><span class="line">            inter_channel, growth_rate, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.branch2a = conv_bn_relu(</span><br><span class="line">            num_input_features, inter_channel, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch2b = conv_bn_relu(</span><br><span class="line">            inter_channel, growth_rate, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch2c = conv_bn_relu(</span><br><span class="line">            growth_rate, growth_rate, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out1 = self.branch1a(x)</span><br><span class="line">        out1 = self.branch1b(out1)</span><br><span class="line"></span><br><span class="line">        out2 = self.branch2a(x)</span><br><span class="line">        out2 = self.branch2b(out2)</span><br><span class="line">        out3 = self.branch2c(out2)</span><br><span class="line"></span><br><span class="line">        out = torch.cat([x, out1, out2], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>代码和StemBlock过于类似，这里不做解读，不过这里的输出是三个堆叠，和StemBlock不一致。</p>
<p>对了，由于这个模块会被反复使用，所有他还特意写了一个DenseBlock，来重复使用。这里使用的是nn.Sequential。那我们来简单介绍一下吧</p>
<blockquote>
<p>简单来说，就是一个容器，那么有三种方式去填充容器。</p>
<ol>
<li><p>```<br>network1 = nn.Sequential(</p>
<pre><code>nn.Flatten(start_dim = 1),
nn.Linear(in_features, out_features),
</code></pre><h1 id="nn-BatchNorm2d-6-添加BN的示例"><a href="#nn-BatchNorm2d-6-添加BN的示例" class="headerlink" title="nn.BatchNorm2d(6) 添加BN的示例"></a>nn.BatchNorm2d(6) 添加BN的示例</h1><pre><code>nn.Linear(out_features, out_classes)
</code></pre><p>)</p>
<h2 id="这样处理，每一层就只有一个id作为名字"><a href="#这样处理，每一层就只有一个id作为名字" class="headerlink" title="这样处理，每一层就只有一个id作为名字"></a>这样处理，每一层就只有一个id作为名字</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">2. </span><br></pre></td></tr></table></figure>
<p>layers = OrderedDict([</p>
<pre><code>(&#39;flat&#39;, nn.Flatten(start_dim = 1)),
(&#39;hidden&#39;,nn.Linear(in_features, out_features)),
(&#39;output&#39;, nn.Linear(out_features, out_classes))
</code></pre><p>])</p>
<p>network2 = nn.Sequential(layers)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">3. </span><br></pre></td></tr></table></figure>
<p>network3 = nn.Sequential()<br>network3.add_module(‘flat’, nn.Flatten(start_dim = 1))<br>network3.add_module(‘hidden’, nn.Linear(in_features, out_features))<br>network3.add_module(‘output’, nn.Linear(out_features, out_classes))</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">   在大型网络中，我推荐2和3的混用，同时这个nn.Sequential是可以嵌套调用的。</span><br><span class="line"></span><br><span class="line">我们再看一下它具体实现细节，</span><br><span class="line"></span><br><span class="line">![](C:&#x2F;Users&#x2F;31311&#x2F;Desktop&#x2F;实习报告&#x2F;PeLeeNet代码解读&#x2F;Sequential.png)</span><br><span class="line"></span><br><span class="line">那么我继承这个类干点事情，重写&#96;__init__&#96;就可以完成了。然后利用第三种方式来add_module，而forword是自己它应该实现好了，所以直接不用了。</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br><span class="line">class _DenseBlock(nn.Sequential):</span><br><span class="line"></span><br><span class="line">   def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):</span><br><span class="line">       super(_DenseBlock, self).__init__()</span><br><span class="line">       for i in range(num_layers):</span><br><span class="line">           layer &#x3D; _DenseLayer(num_input_features + i *</span><br><span class="line">                               growth_rate, growth_rate, bn_size, drop_rate)</span><br><span class="line">           self.add_module(&#39;denselayer%d&#39; % (i + 1), layer) </span><br></pre></td></tr></table></figure>
</li>
</ol>
</blockquote>
<h4 id="Transition-Layer"><a href="#Transition-Layer" class="headerlink" title="Transition Layer"></a>Transition Layer</h4><p>过渡层（transition layer）的输入输出通道数保持一致，即为dense group中最后一个dense block的输出通道数（in_ch+n*growth_rate。利用AvgPool2d进行缩小feature map 的尺寸，很不错。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.features.add_module(<span class="string">'transition%d'</span> % (i + <span class="number">1</span>), conv_bn_relu(</span><br><span class="line">    num_features, num_features, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> i != len(cfg.block_config) - <span class="number">1</span>:</span><br><span class="line">    self.features.add_module(<span class="string">'transition%d_pool'</span> % (</span><br><span class="line">        i + <span class="number">1</span>), nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>))</span><br><span class="line">    num_features = num_features</span><br></pre></td></tr></table></figure>
<h4 id="Composite-Function"><a href="#Composite-Function" class="headerlink" title="Composite Function"></a><strong>Composite Function</strong></h4><p>  采用post-activation结构，替换DenseNet中的pre-activation结构。因而在inference阶段，BN层和卷积层可以融合在一起，以提升推理速度,这个仁者见仁吧，但是普遍这样了。也即conv_bn_relu代替了conv_relu_bn。</p>
<h4 id="总的模块"><a href="#总的模块" class="headerlink" title="总的模块"></a>总的模块</h4><p>直接上图，这个就是搭起来的积木</p>
<p><img src="/2021/01/07/PeLeeNet%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Users/31311/Desktop/实习报告/PeLeeNet代码解读/all.png" alt></p>
<p>前面四个阶段，用下面代码可以表示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">###阶段0</span></span><br><span class="line">self.features = nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">'stemblock'</span>, _StemBlock(<span class="number">3</span>, cfg.num_init_features)),</span><br><span class="line">]))</span><br><span class="line"></span><br><span class="line"><span class="comment">###设置参数</span></span><br><span class="line"><span class="keyword">if</span> type(cfg.growth_rate) <span class="keyword">is</span> list:</span><br><span class="line">    growth_rates = cfg.growth_rate</span><br><span class="line">    <span class="keyword">assert</span> len(</span><br><span class="line">        growth_rates) == <span class="number">4</span>, <span class="string">'The growth rate must be the list and the size must be 4'</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    growth_rates = [cfg.growth_rate] * <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> type(cfg.bottleneck_width) <span class="keyword">is</span> list:</span><br><span class="line">    bottleneck_widths = cfg.bottleneck_width</span><br><span class="line">    <span class="keyword">assert</span> len(</span><br><span class="line">        bottleneck_widths) == <span class="number">4</span>, <span class="string">'The bottleneck width must be the list and the size must be 4'</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    bottleneck_widths = [cfg.bottleneck_width] * <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###阶段1，2，3，4</span></span><br><span class="line"><span class="comment"># Each denseblock</span></span><br><span class="line">num_features = cfg.num_init_features</span><br><span class="line"><span class="comment">###cfg有具体参数</span></span><br><span class="line"><span class="keyword">for</span> i, num_layers <span class="keyword">in</span> enumerate(cfg.block_config):</span><br><span class="line">    <span class="comment">##增加模块DenseBlock</span></span><br><span class="line">    block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,</span><br><span class="line">                        bn_size=bottleneck_widths[i], growth_rate=growth_rates[i], drop_rate=cfg.drop_rate)</span><br><span class="line">    self.features.add_module(<span class="string">'denseblock%d'</span> % (i + <span class="number">1</span>), block)</span><br><span class="line">    num_features = num_features + num_layers * growth_rates[i]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">###增加模块Transition Layer</span></span><br><span class="line">    self.features.add_module(<span class="string">'transition%d'</span> % (i + <span class="number">1</span>), conv_bn_relu(</span><br><span class="line">        num_features, num_features, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">###最后一阶段没有这个</span></span><br><span class="line">    <span class="keyword">if</span> i != len(cfg.block_config) - <span class="number">1</span>:</span><br><span class="line">        self.features.add_module(<span class="string">'transition%d_pool'</span> % (</span><br><span class="line">            i + <span class="number">1</span>), nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>))</span><br><span class="line">        num_features = num_features</span><br></pre></td></tr></table></figure>
<p>这就是大致总的模块</p>
<h4 id="extra-模块"><a href="#extra-模块" class="headerlink" title="extra 模块"></a>extra 模块</h4><p>这里就是用了一个函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">###简单来说就是几个卷积的事情，就是最后几个卷积，返回一个layer，然后放进ModuleList里</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_extras</span><span class="params">(i, batch_norm=False)</span>:</span></span><br><span class="line">    layers = []</span><br><span class="line">    in_channels = i</span><br><span class="line">    channels = [<span class="number">128</span>, <span class="number">256</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">128</span>, <span class="number">256</span>]</span><br><span class="line">    stride = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">    padding = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> enumerate(channels):</span><br><span class="line">        <span class="keyword">if</span> k % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> batch_norm:</span><br><span class="line">                layers += [conv_bn_relu(in_channels, v,</span><br><span class="line">                                        kernel_size=<span class="number">1</span>, padding=padding[k])]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                layers += [conv_relu(in_channels, v,</span><br><span class="line">                                     kernel_size=<span class="number">1</span>, padding=padding[k])]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> batch_norm:</span><br><span class="line">                layers += [conv_bn_relu(in_channels, v,</span><br><span class="line">                                        kernel_size=<span class="number">3</span>, stride=stride[k], padding=padding[k])]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                layers += [conv_relu(in_channels, v,</span><br><span class="line">                                     kernel_size=<span class="number">3</span>, stride=stride[k], padding=padding[k])]</span><br><span class="line">        in_channels = v</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> layers</span><br></pre></td></tr></table></figure>
<p>最后 <code>extras = add_extras(704, batch_norm=True),self.extras = nn.ModuleList(extras)</code> 就可以完成了。</p>
<blockquote>
<p>稍微介绍一下nn.Modulelist()。</p>
<p>这里就类似一个list，但是为什么专门需要呢，因为list不会被加入网络，尽管可以正向传播，但是反向求导就会出错。而且，该模块相对于nn.Sequential,它的顺序就可以颠倒，容易像list一样处理，除了直接上述方法，还可以用append方法来拓展。</p>
<p>但是这个是没有自动的forward，所以就需要用for来执行里面的东西。只能说有好有坏吧，好的就是有些合在一起的东西可以分开执行。</p>
</blockquote>
<h4 id="多尺度检测"><a href="#多尺度检测" class="headerlink" title="多尺度检测"></a>多尺度检测</h4><ol>
<li><p><strong>Feature Map Selection：</strong>为了节省计算量，只将5个尺度的Feature Map连接至检测分支，即 (19 x 19, 10 x 10, 5 x 5, 3x 3, and 1 x 1)</p>
</li>
<li><p><strong>Residual Prediction Block：</strong>在每个检测分支的检测层之前，加入轻量化的残差块，以确保特征表达能力：</p>
<p><img src="/2021/01/07/PeLeeNet%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Users/31311/Desktop/实习报告/PeLeeNet代码解读/MD.png" alt></p>
</li>
</ol>
<h4 id="残差块的处理"><a href="#残差块的处理" class="headerlink" title="残差块的处理"></a>残差块的处理</h4><p>简单来说就是把几个合在一起，方便管理，所以这里必须是ModuleList</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">       nchannels = [<span class="number">512</span>, <span class="number">704</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>]</span><br><span class="line"></span><br><span class="line">       resblock = add_resblock(nchannels)</span><br><span class="line">       self.resblock = nn.ModuleList(resblock)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_resblock</span><span class="params">(nchannels)</span>:</span></span><br><span class="line">   layers = []</span><br><span class="line">   <span class="keyword">for</span> k, v <span class="keyword">in</span> enumerate(nchannels):</span><br><span class="line">       layers += [ResBlock(v)]</span><br><span class="line">   <span class="keyword">return</span> layers</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>吴恩达机器学习</title>
    <url>/2021/01/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h1 id="吴恩达机器学习笔记"><a href="#吴恩达机器学习笔记" class="headerlink" title="吴恩达机器学习笔记"></a>吴恩达机器学习笔记</h1><blockquote>
<p>学习地址：<a href="https://www.bilibili.com/video/BV164411b7dx" target="_blank" rel="noopener">B站</a></p>
<p>学习时间：2020.12.20 ~2021.1.5（15天）</p>
<p>学习内容：基本的机器学习和一些技巧</p>
<p>注解：由于本文和cs231n有些重合，所以相关内容可能大幅省略，详见cs231n</p>
</blockquote>
<p>关键词：<strong>传统机器学习（ML）</strong>  <strong>深度机器学习（DNN）</strong>  <strong>小技巧</strong></p>
<a id="more"></a>
<h2 id="梗概"><a href="#梗概" class="headerlink" title="梗概"></a>梗概</h2><p>机器学习是一种强大的技术，它广泛地应用于NLP（自然语言处理）、CV（计算机视觉）、self-customizing program（自适应编程）、Database Mining（数据挖掘）。</p>
<p>定义：a well-posed learning problem <strong>A</strong> 、a program is said to learn from experience <strong>E</strong>、a task <strong>T</strong> 、a performance <strong>P</strong> .Then machine learning is the performance on <strong>T</strong> ,as measured by <strong>P</strong>, which can improves with  experience <strong>E</strong>.（机器学习是在任务T上通过P测出来的表现能通过经验E改善。）</p>
<p>那么核心就是：</p>
<ul>
<li><p>怎么去定义任务T，怎么预处理T，这些都可以在<strong><em>数据预处理</em></strong>中看到。（<em>简</em>）</p>
</li>
<li><p>怎么去测试表现，这个P怎么去操作，不同的P是怎么样测试不同表现，这些都在<strong><em>loss（cost）</em></strong></p>
</li>
<li>怎么去通过经验改善表现，这些可以在<strong><em>梯度下降</em></strong>看到。</li>
</ul>
<p>最后在这些问题中，有两种常用的架构：<strong>传统机器学习</strong>和<strong>深度神经网络</strong></p>
<p><em>本文首先分开介绍上述三个核心，以及这里所包含的一些技巧，再介绍传统机器学习和深度神经问题，最后介绍一些应用</em></p>
<h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><p>两个重要概念：overfitting（过拟合）、underfitting(欠拟合)</p>
<p>过拟合：模型过于复杂，挑选过多复杂的特征等等经过训练后，这个模型适用于训练的数据，但是不适用于测试的数据。 </p>
<p>欠拟合：模型过于简单等问题经过训练后，这个模型连训练数据都无法拟合。见下图</p>
<p><img src="/2021/01/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Users/31311/Desktop/实习报告/吴恩达/fitting.png" alt></p>
<p>下面是一些概念，这里不做详细介绍</p>
<p><strong>supervised learning(监督学习)</strong>：给定label</p>
<p><strong>unsupervised learning（非监督学习）</strong>：不给定label</p>
<p><strong>classification（分类）</strong> <strong>regression(回归)</strong>    <strong>clustering（聚类）</strong> </p>
<p><strong>hypothesis（假设 用h表示）</strong> </p>
<p>下面是一些必要知识：</p>
<p><strong>线性代数</strong>：主要是一些向量的操作，还有一些乘的操作</p>
<p><strong>矩阵求导</strong>：这个在用数学公式直接算得最优解中有涉及。</p>
<p><strong>微积分</strong>：这里主要是求导。</p>
<p><strong>一些简单的数学知识</strong></p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><ul>
<li>Feature Scaling（特征缩放）：to make $-1\leq x \leq 1$,otherwise your gradients may end upo taking a long time.</li>
<li>Mean normlization:Replace x with $(x-\mu)/\sigma$ ，简单来说就是方差归一化。</li>
</ul>
<h2 id="loss（损失、误差）"><a href="#loss（损失、误差）" class="headerlink" title="loss（损失、误差）"></a>loss（损失、误差）</h2><blockquote>
<p> 预备：$h_\theta$是你预测数来的函数，x是对应的<strong>feature</strong>（特征，在计算机中将输入等称为特征。所以预测出来的为$h_\theta(x^{(i)})$ ，如果是分类任务，那么预测的是个向量（每个类别的得分，如果经过sgmoid（<em>这个以后会介绍,更多应该是softmax</em>）那就是类似于每个类别的概率）。如果是回归任务，预测的直接是一个实数（得分）。</p>
</blockquote>
<h3 id="MSE-loss"><a href="#MSE-loss" class="headerlink" title="MSE loss"></a>MSE loss</h3><p>首先是是回归问题（容易讨论），介绍最简单的loss，也即得分的偏差和，均方误差MSE（L2_loss）：$J(θ)=\frac1{2m}∑[h_\theta(x^{(i)})-y^{(i)}]^2$，为啥用平方，而非绝对值呢，这里是由于绝对值在0点出不可导，因此梯度下降算法可能不能应用，所以就采用平方损失。见下图,那么loss你也可以轻松计算得到。</p>
<p><img src="/2021/01/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Users/31311/Desktop/实习报告/吴恩达机器学习/MSE_loss.png" alt></p>
<h3 id="Cross-Entropy-Error-Function"><a href="#Cross-Entropy-Error-Function" class="headerlink" title="Cross Entropy Error Function"></a>Cross Entropy Error Function</h3><p>然后就是分类任务，此时应该介绍一个函数sigmoid函数（现已被淘汰，不过作为第一个提出的激活函数，还是值得给面子的）,公式为$\sigma(z)=\frac1{1+e^{-z}}$，它大概函数长成</p>
<p><img src="/2021/01/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Users/31311/Desktop/实习报告/吴恩达机器学习/sigmoid.png" alt></p>
<p>因此你可以理解为原本预测值处理过后就是每个类别的概率，我们的目的就是使得这个正确类的概率尽可能大，错误的概率尽可能小。</p>
<blockquote>
<p>假设有猫狗两个类别，编码为0 和1。那么对于一个x，$h’_\theta(x^{(i)})$预测的结果为</p>
<p>$h’{_\theta}(x^{(i)})[0]=5,h’_\theta(x^{(i)})[1]=0.1$ 。这是原始预测，经过了sigmoid后，$h’{_\theta}(x^{(i)})[0]=0.987,h’_\theta(x^{(i)})[1]=0.01 ，$这才是最后的预测结果。</p>
</blockquote>
<p>此时不能用MSE_loss ,因为sigmoid的存在，使得Loss非凸（凸函数等可以参见微积分），因此要欢用其他loss，而log会使得它凸，一般使用Cross Entropy Error Function（交叉熵损失函数）公式如下：</p>
<script type="math/tex; mode=display">
loss=\left\{\begin{aligned}-log(h_\theta(x))\qquad if \quad y=1 \\-log(1-h_\theta(x))\qquad if \quad y=0 \end{aligned}\right.</script><p>由一点小数学技巧，将loss变幻为：</p>
<script type="math/tex; mode=display">
loss = -ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))</script><p>显然，我们可以发现上述loss好像只适用于二分类，不适用于多分类，但是这里题外话一句，多分类就是训练多个二分类器（将自己视为一类，将其他视为第二类）。</p>
<h3 id="F1-score"><a href="#F1-score" class="headerlink" title="F1 score"></a>F1 score</h3><p>第三个衡量指标，这多出现于样本类别不平均的情况下，假如一个数据集它的负样本只有1%，那么你的模型直接全部预测正，loss也只有1%，这是较为可观了。因此上面的loss都不靠谱了。</p>
<p>几个基本概念：</p>
<ol>
<li>True Positives,TP：预测为正样本，实际也为正样本的特征数</li>
<li>False Positives,FP：预测为正样本，实际为负样本的特征数（错预测为正样本了，所以叫False）</li>
<li>True Negatives,TN：预测为负样本，实际也为负样本的特征数</li>
<li>False Negatives,FN：预测为负样本，实际为正样本的特征数（错预测为负样本了，所以叫False）</li>
<li>Precision=TP/(TP+FP)，准确率，预测结果为有多少正样本是预测正确了的</li>
<li>Recall=TP/(TP+FN)，召回率很有意思，相对于Precision只不过参考样本从预测总正样本数结果变成了实际总正样本数。</li>
</ol>
<p>这里就要引入F1 score了 $F1=2\frac{PR}{P+R}$,  P是precision，R是Recall。这个指标很强，PR曲线之类的是题外话了。</p>
<h3 id="权重惩罚"><a href="#权重惩罚" class="headerlink" title="权重惩罚"></a>权重惩罚</h3><p>最后一个实际和这里没有太多关联，叫做正则损失，也就是Regularation,也即权重惩罚，惩罚过于复杂的模型，用于防止过拟合的。假设参数用$\theta$表示。形式一般为$\frac1n\Sigma ^n _1 {\theta_i}^2$  。</p>
<p>引申：在现在的机器学习中，还有很多优秀的loss，大家可以自行查找。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>写在最前面，你可以利用矩阵求导得到最优解结构，这里不介绍。</p>
<p>首先这里需要一点数学思想。</p>
<blockquote>
<p>​       在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是(∂f/∂x, ∂f/∂y)T,简称grad f(x,y)或者▽f(x,y)。对于在点(x0,y0)的具体梯度向量就是(∂f/∂x0, ∂f/∂y0)T.或者▽f(x0,y0)，如果是3个参数的向量梯度，就是(∂f/∂x, ∂f/∂y，∂f/∂z)T,以此类推。</p>
<p>​       那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点(x0,y0)，沿着梯度向量的方向就是(∂f/∂x0, ∂f/∂y0)T的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 -(∂f/∂x0, ∂f/∂y0)T的方向，梯度减少最快，也就是更加容易找到函数的最小值。</p>
</blockquote>
<h3 id="直观解释"><a href="#直观解释" class="headerlink" title="直观解释"></a>直观解释</h3><p>首先来看看梯度下降的一个直观的解释。比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。</p>
<p>　   从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。</p>
<p><img src="/2021/01/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Users/31311/Desktop/实习报告/吴恩达机器学习/gradient.png" alt></p>
<p>当然这些就足够你进行训练了，如果你想进一步探究梯度如何求导，这里推荐看我另外矩阵求导一文。</p>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>我们所做的一切都是为了minimize $J(\theta)$</p>
<p>算法也即:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">repeat until convergence&#123;</span><br><span class="line">     　θi &#x3D; θi − α*∂&#x2F;θi J(θ0,θ1...,θn)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面简单的偏导就是求导过程，不必深究。</p>
<p>这里有一个$\alpha$ ,在梯度中叫做步长，我们经常称为学习率（learning rate），是一个超参，需要人为设计</p>
<ul>
<li><p>学习率设置过小，gradient decent can be slow.</p>
<p><img src="/2021/01/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Users/31311/Desktop/实习报告/吴恩达机器学习/s_lr.png" alt></p>
</li>
<li><p>学习率设置过大，it may fail to convege.</p>
<p><img src="/2021/01/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Users/31311/Desktop/实习报告/吴恩达机器学习/l_sr.png" alt>*</p>
</li>
</ul>
<h3 id="怎么判断收敛呢"><a href="#怎么判断收敛呢" class="headerlink" title="怎么判断收敛呢"></a>怎么判断收敛呢</h3><p>做一个test，去观测loss的变化，如果出现了问题，检查模型或者换学习率。</p>
<h3 id="不同的梯度下降方法"><a href="#不同的梯度下降方法" class="headerlink" title="不同的梯度下降方法"></a>不同的梯度下降方法</h3><blockquote>
<p>真正的方法有很多，这里只介绍几个大类。最常用的是Mini-batch gradient descent</p>
</blockquote>
<ol>
<li><p>批量梯度下降法（Batch Gradient Descent）</p>
<p>批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新，这 个方法对应于前面的线性回归的梯度下降算法（如果看不懂就直接理解为梯度就好了），也就是说3.3.1的梯度下降算法就是批量梯度下降法。　　</p>
<p>　　　　$θ_i=θ_i−α\sum^m_{j=1}(h_\theta(x_0^{(j)},x_1^{(j)},……x_n^{(j)})-y^{(j)})x_i^{(j)}$</p>
<p> 由于我们有m个样本，这里求梯度的时候就用了所有m个样本的梯度数据。</p>
</li>
<li><p>随机梯度下降法（Stochastic Gradient Descent)</p>
</li>
</ol>
<p>​       随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅  仅选取一个样本j来求梯度。对应的更新公式是：</p>
<p>​    　　   　$θ_i=θ_i−α(h_\theta(x_0^{(j)},x_1^{(j)},……x_n^{(j)})-y^{(j)})x_i^{(j)}$</p>
<ol>
<li>小批量梯度下降法（Mini-batch Gradient Descent）</li>
</ol>
<p>​       小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭           代，1&lt;x&lt;m。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。对应的更新公式是：</p>
<p>​                  $θ_i=θ_i−α\sum^{t+x+1}_{j=t}(h_\theta(x_0^{(j)},x_1^{(j)},……x_n^{(j)})-y^{(j)})x_i^{(j)}$</p>
<h2 id="传统神经网络"><a href="#传统神经网络" class="headerlink" title="传统神经网络"></a>传统神经网络</h2><h2 id="深度神经网络"><a href="#深度神经网络" class="headerlink" title="深度神经网络"></a>深度神经网络</h2><h2 id="一些应用"><a href="#一些应用" class="headerlink" title="一些应用"></a>一些应用</h2>]]></content>
  </entry>
  <entry>
    <title>crnn</title>
    <url>/2020/05/15/crnn/</url>
    <content><![CDATA[<h1 id="CRNN"><a href="#CRNN" class="headerlink" title="CRNN"></a>CRNN</h1><blockquote>
<p>该篇介绍了CRNN+tensforflow的模块</p>
</blockquote>
<a id="more"></a>
<h3 id="config"><a href="#config" class="headerlink" title="config"></a>config</h3><blockquote>
<p>：当遇到一个陌生的python第三方库时，可以去pypi这个主页查看描述以迅速入门！<br> 或者 import time   dir(time)</p>
</blockquote>
<p>这里使用了一个easydict ，而easydict：<strong>可以使得以属性的方式去访问字典的值！</strong></p>
<p>简单例子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; from easydict import EasyDict as edict</span><br><span class="line">&gt;&gt;&gt; d &#x3D; edict(&#123;&#39;foo&#39;:3, &#39;bar&#39;:&#123;&#39;x&#39;:1, &#39;y&#39;:2&#125;&#125;)</span><br><span class="line">&gt;&gt;&gt; d.foo</span><br><span class="line">3</span><br><span class="line">&gt;&gt;&gt; d.bar.x</span><br><span class="line">1</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; d &#x3D; edict(foo&#x3D;3)</span><br><span class="line">&gt;&gt;&gt; d.foo</span><br><span class="line">3</span><br></pre></td></tr></table></figure>
<p>json文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; from easydict import EasyDict as edict</span><br><span class="line">&gt;&gt;&gt; from simplejson import loads</span><br><span class="line">&gt;&gt;&gt; j &#x3D; &quot;&quot;&quot;&#123;</span><br><span class="line">&quot;Buffer&quot;: 12,</span><br><span class="line">&quot;List1&quot;: [</span><br><span class="line">    &#123;&quot;type&quot; : &quot;point&quot;, &quot;coordinates&quot; : [100.1,54.9] &#125;,</span><br><span class="line">    &#123;&quot;type&quot; : &quot;point&quot;, &quot;coordinates&quot; : [109.4,65.1] &#125;,</span><br><span class="line">    &#123;&quot;type&quot; : &quot;point&quot;, &quot;coordinates&quot; : [115.2,80.2] &#125;,</span><br><span class="line">    &#123;&quot;type&quot; : &quot;point&quot;, &quot;coordinates&quot; : [150.9,97.8] &#125;</span><br><span class="line">]</span><br><span class="line">&#125;&quot;&quot;&quot;</span><br><span class="line">&gt;&gt;&gt; d &#x3D; edict(loads(j))</span><br><span class="line">&gt;&gt;&gt; d.Buffer</span><br><span class="line">12</span><br><span class="line">&gt;&gt;&gt; d.List1[0].coordinates[1]</span><br><span class="line">54.9</span><br></pre></td></tr></table></figure>
<p>本例用的方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; d &#x3D; EasyDict()</span><br><span class="line">&gt;&gt;&gt; d.foo &#x3D; 3</span><br><span class="line">&gt;&gt;&gt; d.foo</span><br></pre></td></tr></table></figure>
<h3 id="CNN模块"><a href="#CNN模块" class="headerlink" title="CNN模块"></a>CNN模块</h3><blockquote>
<p>pass: 占位语句，避免空函数报错</p>
<p>  @staticmethod #静态方法 类或实例均可调用，改静态方法函数里不传入self 或 cls</p>
</blockquote>
]]></content>
      <categories>
        <category>文字识别</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>安装ubuntu</title>
    <url>/2020/05/13/%E5%AE%89%E8%A3%85ubuntu/</url>
    <content><![CDATA[<h1 id="安装ubuntu"><a href="#安装ubuntu" class="headerlink" title="安装ubuntu"></a>安装ubuntu</h1><blockquote>
<p>该篇介绍了python的基本语句</p>
</blockquote>
<a id="more"></a>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ol>
<li><p>镜像文件</p>
</li>
<li><p>破解的vmware </p>
</li>
<li><p>典型安装</p>
</li>
</ol>
<h3 id><a href="#" class="headerlink" title=" "></a> </h3>]]></content>
  </entry>
  <entry>
    <title>重构</title>
    <url>/2020/03/31/%E9%87%8D%E6%9E%84/</url>
    <content><![CDATA[<h1 id="基本语句"><a href="#基本语句" class="headerlink" title="基本语句"></a>基本语句</h1><blockquote>
<p>该篇介绍了python的基本语句</p>
</blockquote>
<a id="more"></a>
<blockquote>
<p>背景：</p>
<ol>
<li>过度设计。设计模式固然美妙，但是一个具有高弹性，灵活的模式需要较长设计来构建，这不符合敏捷开发的道理。而且需求的变化需要我们未卜先知，所以有时候可能会做无用功。</li>
<li>设计不足：没有时间重构，知识不足，需求变化</li>
</ol>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>金融</title>
    <url>/2020/03/30/%E9%87%91%E8%9E%8D/</url>
    <content><![CDATA[<h1 id="基本语句"><a href="#基本语句" class="headerlink" title="基本语句"></a>基本语句</h1><blockquote>
<p>该篇介绍了python的基本语句</p>
</blockquote>
<a id="more"></a>
<h4 id="思想观念"><a href="#思想观念" class="headerlink" title="思想观念"></a>思想观念</h4><blockquote>
<p>一个人只有经济上独立了，才能在生活上获得心理上的安宁。</p>
</blockquote>
<ol>
<li>注重积累、善于理财，小钱也能成就大财富。（给自己定下铁律：从每月薪水固定地投资）</li>
<li>从自己“第一笔收入”开始</li>
<li>人生不同阶段有不同的追求与需求，所以应根据轻重缓急逐个击破。</li>
<li>甩掉思想包袱，投资不是储蓄，单纯储蓄没用</li>
<li>建立正确的投资思路（投资是一种<strong>智力</strong>游戏）<ul>
<li>巧妙分配（对个人和家庭资源进行管理）</li>
<li>理财无须大资本，理财只是一种生活战略，用来保障生活</li>
<li>对理财手法认识不够，用投入门槛低，进出方便的产品投资</li>
<li>正确看待专家的观念，既不可以盲目信任，也不可断然不信，当作参考而已。</li>
<li>驾驭好工具，要与时俱进。</li>
</ul>
</li>
<li>走出理财误区<ul>
<li>理财并不全是赚钱，也可以只是一种保障手段</li>
<li>理财并不复杂，操作简单</li>
<li>要行动起来，不要只心动，不行动</li>
</ul>
</li>
</ol>
<h3 id="个人理财"><a href="#个人理财" class="headerlink" title="个人理财"></a>个人理财</h3><ol>
<li><p>分析自己的财产状况</p>
<ul>
<li>了解自己的负债情况以及债务的还款计划</li>
<li>计算自己可支配财产</li>
<li>预估未来收入情况</li>
</ul>
</li>
<li><p>制定理财目标</p>
<ul>
<li>需要指定短期财务目标，即目标要量化</li>
</ul>
</li>
<li><p>减少理财风险</p>
<ul>
<li>考虑理财产品在收入的比重</li>
<li>综合分析自己的理财能力</li>
<li>按照自己性格来承担风险</li>
</ul>
</li>
<li><p>合理管理财产，需要谨慎，离职，不可盲目跟风投资，也不可草率投资</p>
</li>
</ol>
<h3 id="家庭理财"><a href="#家庭理财" class="headerlink" title="家庭理财"></a>家庭理财</h3><ol>
<li>统计各种财产</li>
<li>做好收支记录，由于我的身份，我建议个人理财也可以做</li>
<li>重视财产安全</li>
<li>充分利用收入，增加支出</li>
</ol>
<hr>
<h3 id="投资方式"><a href="#投资方式" class="headerlink" title="投资方式"></a>投资方式</h3><blockquote>
<p> 安全保守型: 7:2:1 ,储蓄<strong>保险</strong>投资70%，债卷20%，其他10%</p>
<p> 稳中求进型：4：2：2：1 ，储蓄保险40%，债卷20%，基金股票20%，其他20%</p>
<p>冒险激进型：2：3：5，暂且不说</p>
</blockquote>
<p><strong>步骤</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>1.从现在开始</th>
</tr>
</thead>
<tbody>
<tr>
<td>2. 制定目标</td>
</tr>
<tr>
<td>3.合理投资（保持稳健，适当高风险）</td>
</tr>
<tr>
<td>4.每月固定投资，即使调整投资方向</td>
</tr>
<tr>
<td>5.买了股票要长期持有，吃分红</td>
</tr>
<tr>
<td>6. 让税务局成为你的伙伴，即使了解税务信息</td>
</tr>
<tr>
<td>7. 减少不必要的消费/</td>
</tr>
</tbody>
</table>
</div>
<h3 id="思想原理"><a href="#思想原理" class="headerlink" title="思想原理"></a>思想原理</h3><ul>
<li><p>墨菲定理，墨菲大法好，所以不要轻信市场当前情况</p>
</li>
<li><p>不要将所有鸡蛋放到一个篮子里，所以资产投资五等份</p>
</li>
<li><p>80原则，高风险资产比例不超过（80-20）%的</p>
</li>
<li><p>4321规律，收入，40%用来供房和投资，30%消费生活开支，20%存款，10%购买保险</p>
</li>
<li><p>双十定律，保险额度是家庭收入的10倍，保险支出不超过收入的10%</p>
</li>
<li><p>72定理，复利，本金翻倍的时间为72年除以年收入率</p>
</li>
<li><p>跑赢通货膨胀</p>
</li>
<li><p>不追逐热点，就是求稳，不以物喜，不以物悲</p>
</li>
</ul>
<h3 id="不同阶段的投资方式"><a href="#不同阶段的投资方式" class="headerlink" title="不同阶段的投资方式"></a>不同阶段的投资方式</h3><p>家庭形成期，成长期，成熟期，衰退期，不表</p>
<h3 id="tip"><a href="#tip" class="headerlink" title="tip"></a>tip</h3><ol>
<li>做出日常收支表，观察自己干了啥</li>
<li>做出负债表，看看自己该干啥</li>
<li>设定管理目标，实现财产的保值和升值，也是个人的升值方式<ul>
<li>合理规划生活，既有品质也节约</li>
<li>了解风险敞口</li>
<li>充分评估风险成熟能力</li>
<li>合理设定目标（目标可衡量，目标可达成，目标有期限）</li>
</ul>
</li>
</ol>
<h3 id="资产配置"><a href="#资产配置" class="headerlink" title="资产配置"></a>资产配置</h3><ol>
<li><p>基本层：日常生活。一般要留出2-6月的日常开销的钱，一般占收入的10%</p>
</li>
<li><p>基石层：财产保险，健康保险，意外保险</p>
</li>
<li><p>保值层：储蓄，定期存款，购买风险小的理财产品</p>
</li>
<li><p>增值层：股票，外汇和艺术品</p>
</li>
</ol>
<h3 id="现金管理"><a href="#现金管理" class="headerlink" title="现金管理"></a>现金管理</h3><ol>
<li><p>每天做好消费记录，记账本身不是为了记账，只是为了控制不必要的花费，和学习的时间一样</p>
</li>
<li><p>准备好家庭预留金，预留版半年到一年的生活费</p>
</li>
<li><p>理性消费</p>
</li>
<li><p>指定现金规划表，并严格执行。制定学习规划表，并严格执行</p>
</li>
<li><p>储蓄方式</p>
<ul>
<li><p>计划储蓄，根据收入和支出情况，指定储蓄方法</p>
</li>
<li><p>目标储蓄，设定目标</p>
</li>
<li><p>节约储蓄，节约的钱用来储蓄</p>
</li>
<li><p>缓买储蓄，不要冲动消费，等一会再买</p>
</li>
<li><p>降档储蓄，对于贵重物品降档，将余下的钱存下来</p>
</li>
</ul>
</li>
</ol>
<h4 id="制表"><a href="#制表" class="headerlink" title="制表"></a>制表</h4><blockquote>
<p>收入，支出，<strong>收支比</strong>（特别重要，在学习里）</p>
</blockquote>
<h4 id="投资"><a href="#投资" class="headerlink" title="投资"></a>投资</h4><ol>
<li>短期投资<ul>
<li>银行的各种存款，</li>
<li>银行短期理财产品，</li>
<li>货币基金，</li>
<li>网络现金类理财产品</li>
</ul>
</li>
<li>中长期<ul>
<li>基金定投（选择何是的定投周期，设定定投实现，选择合适基金）</li>
<li>年金保险（强制储蓄，缓解家庭压力）</li>
</ul>
</li>
</ol>
<h4 id="负债"><a href="#负债" class="headerlink" title="负债"></a>负债</h4><blockquote>
<p>简单来说就是拿明天的钱，圆今天的梦。家庭在合理范围负债对家庭有益。</p>
</blockquote>
<ul>
<li>负债率，（负债/资产）*100%。建议在30-50%（在有稳定收入的情况下），50%以上则过高</li>
<li>当家庭负债率低于50%，每月所需还金额在收入的1/3时，那么家庭财务就是安全的。</li>
<li>还债<ul>
<li>急事先办，急事急办</li>
<li>最大限度的提高还款金额</li>
<li>选出利息最高的债务</li>
<li>选取稳健的投资方式</li>
</ul>
</li>
</ul>
<h4 id="财产风险管理"><a href="#财产风险管理" class="headerlink" title="财产风险管理"></a>财产风险管理</h4><ol>
<li>事前预防<ul>
<li>深刻认识到财产风险的村子</li>
<li>做到分散财产风险</li>
</ul>
</li>
<li>事中控制（保护好自己，生命是第一位）</li>
<li>事后补救。（<strong>家庭成员要进行烦死，但不要互相责怪，而应该以健康积极的态度振作起来</strong>）</li>
<li><p>由于风险的存在，所以我们要做好风险预留。</p>
<ul>
<li><p>创造安全的家庭环境</p>
</li>
<li><p>提高家庭防范风险的意识</p>
</li>
<li><p>准备好一定的家庭应急备用金</p>
</li>
</ul>
</li>
<li><p>风险转移（主要是保险）</p>
<ul>
<li>人寿保险</li>
<li>人身意外伤害险</li>
<li>健康保险</li>
</ul>
</li>
<li><p>婚姻风险（财产转移，隐瞒财产，对外举债，擅自处置财产）</p>
<ul>
<li>明确财产归属权（婚姻法）</li>
<li>签订婚前协议</li>
<li>婚内分割家庭财产</li>
<li>父母出资和财产赠与的管理</li>
</ul>
</li>
</ol>
<h4 id="家庭财产投资管理"><a href="#家庭财产投资管理" class="headerlink" title="家庭财产投资管理"></a>家庭财产投资管理</h4><ol>
<li><p>股票（各种种类）</p>
<ul>
<li>选择一支好的股票（熟悉，热门，题材股，成长空间大，公司稳定的股票）</li>
<li>选择最佳购买时机（短线和中长线的时机不同）</li>
<li>选择最佳卖出时机（别贪心）</li>
</ul>
</li>
<li><p>基金（股票基金（较高风险，较高收益），债卷投资，货币基金，指数基金）</p>
<ul>
<li><p>选择合理的赎买时机。（设定理想目标&lt; 不要贪心&gt;，分析回报率）</p>
</li>
<li><p>选择合理的投资方式</p>
</li>
<li><p>| 532模式 | 敢于冒险的家庭     | 50%股票型，40%货币型，10%储蓄型 |<br>| ———- | :————————- | ———————————————- |<br>| 442模式 | 风险承受能力强的   | 40%股票型，40%货币型，20%储蓄型 |<br>| 334模式 | 不敢冒险的         | 30%股票型，30%货币型，40%储蓄型 |<br>| 244模式 | 不敢冒险的，开支大 | 20%股票型，40%货币型，40%储蓄型 |</p>
</li>
<li><p>基金定投也可</p>
</li>
</ul>
</li>
<li><p>债卷（几乎同上）</p>
</li>
<li><p>期货</p>
<ul>
<li>轻仓操作，衡量风险</li>
<li>组合投资，分散风险</li>
<li>及时止损！！！！！</li>
</ul>
</li>
<li><p>房地产和艺术品投资（不讲）</p>
</li>
<li>银行类理财产品（固定收益和浮动收益）</li>
<li><p>商业保险投资</p>
<ul>
<li>全面保险（家庭各个成员）</li>
<li>保险额度适当，双十原则</li>
<li>筛选性价比高的保险产品</li>
</ul>
</li>
<li><p>健康投资（！！！！！）</p>
<ul>
<li>多运动，便宜大量</li>
<li>定期接受检查</li>
<li>购买营养品和健康书看</li>
<li>购买健康保险</li>
</ul>
</li>
</ol>
<h3 id="理财"><a href="#理财" class="headerlink" title="理财"></a>理财</h3><blockquote>
<p>理财观有很多误区，要避免误区，在早期积累本金之后，就可以进行理财了。理财需要理性与耐性，只需要基础的理财知识就可以进行了。</p>
</blockquote>
<hr>
<h4 id="复利效应"><a href="#复利效应" class="headerlink" title="复利效应"></a>复利效应</h4><blockquote>
<p>复利模式，即有益的利滚利模式，而坚持一项投资的时间越长，回报就越高。</p>
</blockquote>
<ul>
<li>第一原则：不要间断。只要长期持有，时间越久，复利效果就越明显。这是金钱的时间价值。</li>
<li>第二原则：等待。一旦冲破冗长的积累阶段，复利就可以有效果</li>
<li>第三原则：人生也像是复利，同样的起点，每一步投资都是复利，在个人成就上，也就有了天差地别。所以，不管是投资和人生，从一开始我们都要持之以恒，只有这样才能得到最好的回报。</li>
<li>72法则：看上面有。可以预测资产翻倍，购买力减半，以及年薪上涨时间。</li>
</ul>
<h4 id="投资100法则"><a href="#投资100法则" class="headerlink" title="投资100法则"></a>投资100法则</h4><blockquote>
<p>理财要根据自己承受能力开选择合理的方案，这样自己才能在投资中放得开，而且最好的方式是跟着年龄走，学会分散投资，选择最佳的投资组合，实现最大化收益。</p>
</blockquote>
<ol>
<li>即（100-年龄）%的资产可以投资于股票及股票基金等投资方式。</li>
<li><p>投资新手要懂得分散投资：分散投资是一种经得起考验的投资策略。</p>
<ul>
<li>巴菲特把鸡蛋放到一个篮子里，那是人家强呀</li>
<li>年轻人不能在理财上过于相信自己的直觉，收益高，风险高。</li>
<li>投资不是赌博，不适合集中火力猛攻的投资方式</li>
</ul>
</li>
<li><p>选择最适合的投资组合方式</p>
<ul>
<li>投资工具组合（储蓄，基金等），用投资五分法</li>
<li>投资比例组合，上面有各种比例方式</li>
<li>投资时间组合，即短期、中期、长期投资方式的组合</li>
</ul>
</li>
</ol>
<h4 id="投资报酬率"><a href="#投资报酬率" class="headerlink" title="投资报酬率"></a>投资报酬率</h4><blockquote>
<p>看懂投资报酬率才能很好进行投资</p>
</blockquote>
<ol>
<li>投资报酬率 = 利润总额/投资总额<em>100% =（净利息+利息费用+所得税）/投资总额</em>100%</li>
<li>选择合理的理财方式（资产增值要克服通货膨胀的影响）<ul>
<li>分散投资可能会造成买一些不太熟悉的投资</li>
<li>集中投资前提是对公司规模前景充满信心。</li>
</ul>
</li>
</ol>
<h4 id="工薪阶层攒出第一桶金"><a href="#工薪阶层攒出第一桶金" class="headerlink" title="工薪阶层攒出第一桶金"></a>工薪阶层攒出第一桶金</h4><blockquote>
<p>理财的中心是<strong>管钱</strong>，包括攒钱、生钱和护钱。</p>
</blockquote>
<ul>
<li>强制储蓄</li>
<li>计划消费</li>
<li>尽量用现金付款</li>
<li>将信用卡与储蓄卡绑到一起（我不用信用卡）</li>
<li>延迟消费，不要追赶潮流，潮流较贵</li>
<li>工资分割（观察哪部分工资超出了）</li>
<li>记账与做预算，不做无谓的消费</li>
<li>只花1/3的薪水</li>
<li>节后理财，日后理财</li>
<li>意外储蓄（意外收入存起来）</li>
<li>小钱储蓄（获得的小金额利益进行储蓄）</li>
<li>先储蓄后消费（强制储蓄的一部分）</li>
</ul>
<h4 id="股票和国债"><a href="#股票和国债" class="headerlink" title="股票和国债"></a>股票和国债</h4><blockquote>
<p>略</p>
</blockquote>
<h4 id="基金"><a href="#基金" class="headerlink" title="基金"></a>基金</h4><blockquote>
<p>基金有风险</p>
</blockquote>
<ol>
<li>做一个明白的投资人（对于基金认识深刻，对自己更要深刻）</li>
<li>做一个独立思考的投资人</li>
<li>做一个耐心的投资人（相信基金，长期投资，学会换基 ）</li>
<li>选好基金（532，50%的积极行，30适度积极，20储蓄）</li>
<li>选3-5只基金</li>
</ol>
<h4 id="保险"><a href="#保险" class="headerlink" title="保险"></a>保险</h4><blockquote>
<p>略</p>
</blockquote>
<h4 id="投资的风险"><a href="#投资的风险" class="headerlink" title="投资的风险"></a>投资的风险</h4><blockquote>
<p>投资三要素：风险性，收益性和流动性</p>
</blockquote>
<ol>
<li>拿得起，放不下就不要进行投资</li>
</ol>
<p>结束</p>
]]></content>
  </entry>
  <entry>
    <title>AlexNet</title>
    <url>/2020/03/12/AlexNet/</url>
    <content><![CDATA[<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><blockquote>
<p>该篇介绍了python的基本语句</p>
</blockquote>
<a id="more"></a>
<h4 id="AlexNet小技巧"><a href="#AlexNet小技巧" class="headerlink" title="AlexNet小技巧"></a>AlexNet小技巧</h4><ul>
<li>使用Relu作为激活函数，验证其效果更佳，且解决了sigmoid在网络较深时的梯度弥散问题。</li>
<li>dropout随机忽略一些神经元，以避免模型过拟合。</li>
<li>采用最大池化，替代了平均池化，避免平均池化的模糊化效果。</li>
<li>lrn层，实际没用</li>
<li>数据增强</li>
</ul>
<h4 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h4><ul>
<li>加深网络结构来提升性能</li>
<li>多个核的使用是的对特征学习能力更强</li>
<li>先训练级别A的简单网络，再服用A的网络权重来初始化后面的复杂模型，收敛速度提高</li>
<li>用multi-scale来数据增强</li>
<li></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>csp准备</title>
    <url>/2020/03/02/csp%E5%87%86%E5%A4%87/</url>
    <content><![CDATA[<h1 id="csp准备"><a href="#csp准备" class="headerlink" title="csp准备"></a>csp准备</h1><blockquote>
<p>重拾C语言，准备csp的考试</p>
</blockquote>
<a id="more"></a>
<h4 id="奇技淫巧（小注意）"><a href="#奇技淫巧（小注意）" class="headerlink" title="奇技淫巧（小注意）"></a>奇技淫巧（小注意）</h4><ol>
<li>i++ 和++i 老生常谈</li>
<li>比较大的数组声明在main函数外，函数内申请内存有限制</li>
<li>fgets(buf,maxn,fin)将读取完整的数量不超过maxn的一行放入buf钟，而gets(s)没有数量限制</li>
<li>循环中不要对循环变量操作，千万不要！！！！！！！！！！！！！</li>
<li>不知道为啥我总忘掉typedef  typedef struct {double x,y;}Point;</li>
<li>对复杂表达式化简，减少运算并且要防止输出</li>
<li>建议把为此</li>
<li>把数组大小设置为 #define n 20，可以免去很多修改</li>
<li>建议把谓词（判断事物是否具有某种特性的函数）命名为is_xx 的形式，返回int值，1代表有</li>
<li>形参和实参，形实结合</li>
<li>数组名实际是指针，所以传参时应该传给指针，未避免指针悬挂，应给数组的长度</li>
<li>递归判断可用？：代替        return n==0? 1:n*f(n-1);</li>
<li>未避免浮点误差 建议+0.5，这样库用四舍五入。</li>
<li>从0~n-1到1~n的数学转化p=(p+n-1)%n+1;</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>xs</title>
    <url>/2020/02/24/xs/</url>
    <content><![CDATA[<h1 id="基本语句"><a href="#基本语句" class="headerlink" title="基本语句"></a>基本语句</h1><blockquote>
<p>该篇介绍了python的基本语句</p>
</blockquote>
<a id="more"></a>
<p><img src="/2020/02/24/xs/avatar.jpg" alt></p>
]]></content>
  </entry>
  <entry>
    <title>矩阵求导</title>
    <url>/2020/02/24/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/</url>
    <content><![CDATA[<h1 id="矩阵求导"><a href="#矩阵求导" class="headerlink" title="矩阵求导"></a>矩阵求导</h1><blockquote>
<p>该篇介绍了矩阵求导的基本方法</p>
</blockquote>
<a id="more"></a>
<blockquote>
<p>前言：对于机器学习来说，矩阵求导是很重要的一部分，同时对向量求导是其中的主要组成部分，这里 对于矩阵求导这里不展开。</p>
</blockquote>
<p>求导和梯度概念很相似，注意比较</p>
<script type="math/tex; mode=display">
前置公式\quad x^T=[x_1\quad x_2 \quad \cdots \quad x_n]</script><p>基础公式：</p>
<script type="math/tex; mode=display">
f(x)是关于x的单值函数:
\\所以 \quad
\frac {\partial f(x)} {\partial x^T} = 
\left [ 
\frac {\partial f(x)} {\partial x_1} \quad \frac {\partial f(x)} {\partial x_2} \quad \cdots \quad \frac {\partial f(x)} {\partial x_n}

\right]  
\\
\quad f(x)^T=\left [f_1(x)\quad f_2(x) \quad \cdots \quad f_n(x)\right ]
\\
所以 \quad   \frac{\partial f(x)}{\partial x^T}=
\left( \begin{matrix} 
   \frac{\partial f_1(x)}{\partial x_1} &\frac{\partial f_1(x)}{\partial x_2} &\cdots &\frac{\partial f_1(x)}{\partial x_n}

\\ \frac{\partial f_2(x)}{\partial x_1} &\frac{\partial f_2(x)}{\partial x_2} &\cdots &\frac{\partial f_2(x)}{\partial x_n}

\\ \vdots &\vdots &\ddots &\vdots

\\ \frac{\partial f_n(x)}{\partial x_1} &\frac{\partial f_n(x)}{\partial x_2} &\cdots &\frac{\partial f_n(x)}{\partial x_n}
\end{matrix} \right)
\\

\frac{\partial f(x)^T}{\partial x}是行向量向列向量求导，和上面列向量对行向量求导类似</script><p>常用公式：</p>
<script type="math/tex; mode=display">
\frac {\partial x^T}{\partial x} = I</script><p>规则：</p>
<script type="math/tex; mode=display">
当f(x)是单值函数时，线性法则，乘积法则和商法则同微积分\\
链式法则，若y(x)时关于x的向量值函数(列向量)时。则\\
 \frac {\partial f(y(x))}{\partial x} = \frac {\partial y^T(x)} {\partial x}
 \frac{\partial f(y)}{\partial y}</script><p>常用公式：</p>
<script type="math/tex; mode=display">
前提公式：x^Ty=<x,y>=y^Tx</script><script type="math/tex; mode=display">
\frac{\partial a^Tx}{\partial x} =
\frac{\partial x^Ta}{\partial x} =a
\\下面不展示转置后的项以及利用转置的步骤\\
\frac{\partial a^Ty(x)}{\partial x} =\frac{\partial y^T(x)}{\partial x} a
\\
\frac{\partial x^TAy}{\partial x} =Ay
\\
\frac{\partial x^TAx}{\partial x} =(A+A^T)x  \quad如果A对称，则为2Ax
\\
\frac{\partial y(x)^TAy(x)}{\partial x} =\frac {\partial y^T(x)}{\partial x}(A+A^T)y(x)
\\
\frac{\partial y(x)^TAz(x)}{\partial x} =\frac {\partial y^T(x)}{\partial x}Az(x)+ \frac {\partial z^T(x)}{\partial x}A^Ty(x)</script><p>单值函数的梯度矩阵(其余公式不证)</p>
<script type="math/tex; mode=display">
 \quad   \frac{\partial f(x)}{\partial A}=
\left( \begin{matrix} 
   \frac{\partial f(x)}{\partial A_{11}} &\frac{\partial f(x)}{\partial A_{12}} &\cdots &\frac{\partial f(x)}{\partial A_{1n}}

\\ \frac{\partial f(x)}{\partial A_{21}} &\frac{\partial f(x)}{\partial A_{22}} &\cdots &\frac{\partial f(x)}{\partial A_{2n}}

\\ \vdots &\vdots &\ddots &\vdots

\\ \frac{\partial f(x)}{\partial A_{n1}} &\frac{\partial f(x)}{\partial A_{n2}} &\cdots &\frac{\partial f(x)}{\partial A_{nn}}
\end{matrix} \right)
\\</script>]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>基础类</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数</title>
    <url>/2020/02/21/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</url>
    <content><![CDATA[<h1 id="基本语句"><a href="#基本语句" class="headerlink" title="基本语句"></a>基本语句</h1><blockquote>
<p>该篇介绍了python的基本语句</p>
</blockquote>
<a id="more"></a>
]]></content>
  </entry>
  <entry>
    <title>markdown数学公式</title>
    <url>/2020/02/21/markdown%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</url>
    <content><![CDATA[<h1 id="基本语句"><a href="#基本语句" class="headerlink" title="基本语句"></a>基本语句</h1><blockquote>
<p>该篇介绍了python的基本语句</p>
</blockquote>
<a id="more"></a>
<blockquote>
<p>如果是typora，记得在文件的偏好设置中的markdown语法的内联公式开启</p>
</blockquote>
<h4 id="行内与独行"><a href="#行内与独行" class="headerlink" title="行内与独行"></a>行内与独行</h4><ul>
<li><p>行内公式 ： 2个美元符包围  $ ss $</p>
</li>
<li><p>独行公式：4个美元符包围 </p>
<script type="math/tex; mode=display">ss</script></li>
</ul>
<h4 id="上标下标组合"><a href="#上标下标组合" class="headerlink" title="上标下标组合"></a>上标下标组合</h4><ul>
<li>上标  ^符 $ x^4$</li>
<li>下标 _ 符$x_1$</li>
<li>组合符号 {}符   ${16}_{8x}O{2+}_{2}$</li>
</ul>
<h4 id="汉字、字体格式"><a href="#汉字、字体格式" class="headerlink" title="汉字、字体格式"></a>汉字、字体格式</h4><ul>
<li>汉字形式，符号：<code>\mbox{}</code>，如：$V_{\mbox {初始}}$</li>
<li>字体控制，符号：<code>\displaystyle</code>，如：$\displaystyle\frac{x+y}{y+z}$</li>
<li>下划线符号，符号：<code>\underline</code>，如：$\underline{x+y}$</li>
<li>标签，符号<code>\tag{数字}</code>，如：$\tag{11}$</li>
<li>上大括号，符号：<code>\overbrace{算式}</code>，如：$\overbrace{a+b+c+d}^{2.0}$</li>
<li>下大括号，符号：<code>\underbrace{算式}</code>，如：$a+\underbrace{b+c}_{1.0}+d$</li>
<li>上位符号，符号：<code>\stackrel{上位符号}{基位符号}</code>，如：$\vec{x}\stackrel{def}{=}x_1,\dots,x_n$</li>
</ul>
<h4 id="占位符"><a href="#占位符" class="headerlink" title="占位符"></a>占位符</h4><ul>
<li>两个quad空格，符号：<code>\qquad</code>，如：$x \qquad y$</li>
<li>quad空格，符号：<code>\quad</code>，如：$x \quad y$</li>
<li>大空格，符号<code>\</code>，如：$x \: y$ </li>
<li>紧贴，符号<code>\!</code>，如：$x!y$</li>
</ul>
<h4 id="定界符与组合"><a href="#定界符与组合" class="headerlink" title="定界符与组合"></a>定界符与组合</h4><ul>
<li>括号，符号：<code>（）\big(\big) \Big(\Big) \bigg(\bigg) \Bigg(\Bigg)</code>，如：$（）\big(\big)\quad \Big(\Big) \bigg(\bigg) \Bigg(\Bigg)$</li>
<li>大括号，符号：<code>\{ \}</code>，如：$\{x+y\}$  因为</li>
<li>自适应括号，符号：<code>\left \right</code>，如：$\left(x\right)$，$\left(x{yz}\right)$</li>
<li>组合公式，符号：<code>{上位公式 \choose 下位公式}</code>，如：${n+1 \choose k}={n \choose k}+{n \choose k-1}$</li>
<li>组合公式，符号：<code>{上位公式 \atop 下位公式}</code>，如：$\sum_{k_0,k_1,\ldots&gt;0 \atop k_0+k_1+\cdots=n}A_{k_0}A_{k_1}\cdots$</li>
</ul>
<h4 id="四则运算"><a href="#四则运算" class="headerlink" title="四则运算"></a>四则运算</h4><ul>
<li>加法运算，符号：<code>+</code>，如：$x+y=z$</li>
<li>减法运算，符号：<code>-</code>，如：$x-y=z$</li>
<li>加减运算，符号：<code>\pm</code>，如：$x \pm y=z$</li>
<li>减甲运算，符号：<code>\mp</code>，如：$x \mp y=z$</li>
<li>乘法运算，符号：<code>\times</code>，如：$x \times y=z$</li>
<li>点乘运算，符号：<code>\cdot</code>，如：$x \cdot y=z$</li>
<li>星乘运算，符号：<code>\ast</code>，如：$x \ast y=z$</li>
<li>除法运算，符号：<code>\div</code>，如：$x \div y=z$</li>
<li>斜法运算，符号：<code>/</code>，如：$x/y=z$</li>
<li>分式表示，符号：<code>\frac{分子}{分母}</code>，如：$\displaystyle \frac{x+y}{y+z}$</li>
<li>分式表示，符号：<code>{分子} \voer {分母}</code>，如：${x+y} \over {y+z}$</li>
<li>绝对值表示，符号：<code>||</code>，如：$|x+y|$</li>
</ul>
<h4 id="高级运算"><a href="#高级运算" class="headerlink" title="高级运算"></a>高级运算</h4><ul>
<li><p>平均数运算，符号：<code>\overline{算式}</code>，如：$\overline{xyz}$</p>
</li>
<li><p>开二次方运算，符号：<code>\sqrt</code>，如：$\sqrt x$</p>
</li>
<li><p>开方运算，符号：<code>\sqrt[开方数]{被开方数}</code>，如：$\sqrt[3]{x+y}$</p>
</li>
<li><p>对数运算，符号：<code>\log</code>，如：$\log(x)$</p>
</li>
<li><p>极限运算，符号：<code>\lim</code>，如：$\lim^{x \to \infty}_{y \to 0}{\displaystyle\frac{x}{y}}$</p>
</li>
<li><p>极限运算，符号：<code>\displaystyle \lim</code>，如：$\displaystyle \lim^{x \to \infty}_{y \to 0}{\frac{x}{y}}$</p>
</li>
<li><p>求和运算，符号：<code>\sum</code>，如：$\sum^{x \to \infty}_{y \to 0}{\frac{x}{y}}$</p>
</li>
<li><p>求和运算，符号：<code>\displaystyle \sum</code>，如：$\displaystyle {x \to \infty}_{y \to 0}{\frac{x}{y}}$</p>
</li>
<li><p>积分运算，符号：<code>\int</code>，如：$\int^{\infty}_{0}{xdx}$</p>
</li>
<li><p>积分运算，符号：<code>\displaystyle \int</code>，如：$\displaystyle \int^{\infty}_{0}{xdx}$</p>
</li>
<li><p>微分运算，符号：<code>\partial</code>，如：$\frac{\partial x}{\partial y}$</p>
</li>
<li><p>大括号，\left\{\begin{aligned}x&amp;=1\\y&amp;=2+x\end{aligned}\right.<script type="math/tex">\left\{
\begin{aligned}
x&=1\\
y&=2+x
\end{aligned}
\right.</script></p>
</li>
<li><p>矩阵表示，符号：<code>\begin{matrix} \end{matrix}</code>，如：</p>
<script type="math/tex; mode=display">
\qquad \left( \begin{matrix} 
1 &2 &\cdots &4 

\\ 5 &6 &\cdots & 8 \\ 
\vdots &\vdots &\ddots &\vdots 

\\13 &14 &\cdots &16

\end{matrix} \right)</script></li>
</ul>
<h4 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a>逻辑运算</h4><ul>
<li><p>等于运算，符号：<code>=</code>，如：$x+y=z$</p>
</li>
<li><p>大于运算，符号：<code>&gt;</code>，如：$x+y&gt;z$</p>
</li>
<li><p>小于运算，符号：<code>&lt;</code>，如：$x+y&lt;z$</p>
</li>
<li><p>大于等于运算，符号：<code>\geq</code>，如：$x+y \geq z$</p>
</li>
<li><p>小于等于运算，符号：<code>\leq</code>，如：$x+y \leq z$</p>
</li>
<li><p>不等于运算，符号：<code>\neq</code>，如：$x+y \neq z$</p>
</li>
<li><p>不大于等于运算，符号：<code>\ngeq</code>，如：$x+y \ngeq z$</p>
</li>
<li><p>不大于等于运算，符号：<code>\not\geq</code>，如：$x+y \not\geq z$</p>
</li>
<li><p>不小于等于运算，符号：<code>\nleq</code>，如：$x+y \nleq z$</p>
</li>
<li><p>不小于等于运算，符号：<code>\not\leq</code>，如：$x+y \not\leq z$</p>
</li>
<li><p>约等于运算，符号：<code>\approx</code>，如：$x+y \approx z$</p>
</li>
<li><p>恒定等于运算，符号：<code>\equiv</code>，如：$x+y \equiv z$</p>
</li>
</ul>
<h4 id="集合运算"><a href="#集合运算" class="headerlink" title="集合运算"></a>集合运算</h4><ul>
<li><p>属于运算，符号：<code>\in</code>，如：$x \in y$</p>
<p>不属于运算，符号：<code>\notin</code>，如：$x \notin y$</p>
<p>不属于运算，符号：<code>\not\in</code>，如：$x \not\in y$</p>
<p>子集运算，符号：<code>\subset</code>，如：$x \subset y$</p>
<p>子集运算，符号：<code>\supset</code>，如：$x \supset y$</p>
<p>真子集运算，符号：<code>\subseteq</code>，如：$x \subseteq y$</p>
<p>非真子集运算，符号：<code>\subsetneq</code>，如：$x \subsetneq y$</p>
<p>真子集运算，符号：<code>\supseteq</code>，如：$x \supseteq y$</p>
<p>非真子集运算，符号：<code>\supsetneq</code>，如：$x \supsetneq y$</p>
<p>非子集运算，符号：<code>\not\subset</code>，如：$x \not\subset y$</p>
<p>非子集运算，符号：<code>\not\supset</code>，如：$x \not\supset y$</p>
<p>并集运算，符号：<code>\cup</code>，如：$x \cup y$</p>
<p>交集运算，符号：<code>\cap</code>，如：$x \cap y$</p>
<p>差集运算，符号：<code>\setminus</code>，如：$x \setminus y$</p>
<p>同或运算，符号：<code>\bigodot</code>，如：$x \bigodot y$</p>
<p>同与运算，符号：<code>\bigotimes</code>，如：$x \bigotimes y$</p>
<p>实数集合，符号：<code>\mathbb{R}</code>，如：$\mathbb{R}$</p>
<p>自然数集合，符号：<code>\mathbb{Z}</code>，如:$\mathbb{Z}$</p>
<p>空集，符号：<code>\emptyset</code>，如：$\emptyset$</p>
</li>
</ul>
<h4 id="数学符号"><a href="#数学符号" class="headerlink" title="数学符号"></a>数学符号</h4><p>无穷，符号：<code>\infty</code>，如：$\infty$</p>
<p>虚数，符号：<code>\imath</code>，如：$\imath$</p>
<p>虚数，符号：<code>\jmath</code>，如：$\jmath$</p>
<p>数学符号，符号<code>\hat{a}</code>，如：$\hat{a}$</p>
<p>数学符号，符号<code>\check{a}</code>，如：$\check{a}$</p>
<p>数学符号，符号<code>\breve{a}</code>，如：$\breve{a}$</p>
<p>数学符号，符号<code>\tilde{a}</code>，如：$\tilde{a}$</p>
<p>数学符号，符号<code>\bar{a}</code>，如：$\bar{a}$</p>
<p>矢量符号，符号<code>\vec{a}</code>，如：$\vec{a}$</p>
<p>数学符号，符号<code>\acute{a}</code>，如：$\acute{a}$</p>
<p>数学符号，符号<code>\grave{a}</code>，如：$\grave{a}$</p>
<p>数学符号，符号<code>\mathring{a}</code>，如：$\mathring{a}$</p>
<p>一阶导数符号，符号<code>\dot{a}</code>，如：$\dot{a}$</p>
<p>二阶导数符号，符号<code>\ddot{a}</code>，如：$\ddot{a}$</p>
<p>上箭头，符号：<code>\uparrow</code>，如：$\uparrow$</p>
<p>上箭头，符号：<code>\Uparrow</code>，如：$\Uparrow$</p>
<p>下箭头，符号：<code>\downarrow</code>，如：$\downarrow$</p>
<p>下箭头，符号：<code>\Downarrow</code>，如：$\Downarrow$</p>
<p>左箭头，符号：<code>\leftarrow</code>，如：$\leftarrow$</p>
<p>左箭头，符号：<code>\Leftarrow</code>，如：$\Leftarrow$</p>
<p>右箭头，符号：<code>\rightarrow</code>，如：$\rightarrow$</p>
<p>右箭头，符号：<code>\Rightarrow</code>，如：$\Rightarrow$</p>
<p>底端对齐的省略号，符号：<code>\ldots</code>，如：$1,2,\ldots,n$</p>
<p>中线对齐的省略号，符号：<code>\cdots</code>，如：$x_1^2 + x_2^2 + \cdots + x_n^2$</p>
<p>竖直对齐的省略号，符号：<code>\vdots</code>，如：$\vdots$</p>
<p>斜对齐的省略号，符号：<code>\ddots</code>，如：$\ddots$</p>
<h4 id="希腊字符"><a href="#希腊字符" class="headerlink" title="希腊字符"></a>希腊字符</h4><p><img src="/2020/02/21/markdown%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/Users\31311\Desktop\github_web\source\images\字符.png" alt></p>
<p><img src="/2020/02/21/markdown%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/Users\31311\Desktop\github_web\source\images\字符2.png" alt></p>
]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>-基础类</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习导论</title>
    <url>/2020/02/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/</url>
    <content><![CDATA[<h1 id="机器学习导论"><a href="#机器学习导论" class="headerlink" title="机器学习导论"></a>机器学习导论</h1><blockquote>
<p><strong><em>ML = Matrix （信息）+ Optimation（优化） + Algorithm（使优化更有效） +Statistics（建模）</em></strong></p>
</blockquote>
<a id="more"></a>
<p>one-hot表示方法，单纯表示方法</p>
<p>监督学习问题：分类问题和回归问题</p>
<p>分类属于特殊的回归问题</p>
<p>最小二乘估计‘</p>
<p>矩阵求导相关？？</p>
<p>岭回归（最小二乘估计加上惩罚）</p>
<p>数据分为三组：Training data， Validation data，Test data</p>
<p>极大似然估计（MLE</p>
<p>无监督问题：聚类</p>
<p>半监督学习</p>
<p>迁移学习：将测试数据也放进去</p>
<p>参数（模型参数固定，不需要训练，超参数）和非参数（需要训练，提前未给定）</p>
<p>判别模型和生成模型</p>
]]></content>
  </entry>
  <entry>
    <title>大数据机器学习</title>
    <url>/2020/02/17/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h1 id="大数据机器学习"><a href="#大数据机器学习" class="headerlink" title="大数据机器学习"></a>大数据机器学习</h1><blockquote>
<p>该篇介绍了python的基本语句</p>
</blockquote>
<a id="more"></a>
<blockquote>
<p> 机器学习是一门多领域交叉学课，涉及概率论，统计学，逼近论，图分析，算法复杂度理论，是自动“学习的算法”，与统计推断学密切相关，也被称为统计学习理论</p>
</blockquote>
<p><em>辉煌历史不提，比如深蓝，文字转换，3dvr，生物信息学</em></p>
<p>机器学习是人工智能的计算方法，表示学习（浅层自编码器），深度学习（多层感知机）</p>
<h3 id="共性和差异"><a href="#共性和差异" class="headerlink" title="共性和差异"></a>共性和差异</h3><ul>
<li><p>基于规则的系统：不可学习，专家系统</p>
</li>
<li><p>基于表示的系统</p>
<ul>
<li>学习获得，经过特征映射得到</li>
<li>深度学习，通过层的深入得到递进的特征，从而特征映射获得结果</li>
</ul>
</li>
</ul>
<p>计算机视觉是机器学习最重要的领域</p>
<p>机器学</p>
]]></content>
  </entry>
  <entry>
    <title>加性高斯模糊自编码器</title>
    <url>/2020/02/17/%E5%8A%A0%E6%80%A7%E9%AB%98%E6%96%AF%E6%A8%A1%E7%B3%8A%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</url>
    <content><![CDATA[<h1 id="加性高斯噪声自动编码器"><a href="#加性高斯噪声自动编码器" class="headerlink" title="加性高斯噪声自动编码器"></a>加性高斯噪声自动编码器</h1><blockquote>
<p>该篇介绍了加性高斯噪声自动编码器的构造方法，用类实现方便集成</p>
</blockquote>
<a id="more"></a>
<h3 id="加性高斯噪声自动编码器-1"><a href="#加性高斯噪声自动编码器-1" class="headerlink" title="加性高斯噪声自动编码器"></a>加性高斯噪声自动编码器</h3><p><em>目的：给数字加上模糊后再还原它</em>，以及用sklearn库的尝试</p>
<h4 id="xavier-init"><a href="#xavier-init" class="headerlink" title="xavier_init"></a>xavier_init</h4><blockquote>
<p>它为了保证前向传播和反向传播时每一层的方差一致，根据每层的输入个数和输出个数来决定参数随机初始化的分布范围，是一个通过该层的输入和输出参数个数得到的分布范围内的均匀分布。 </p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn.preprocessing <span class="keyword">as</span> prep</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment">### xaver_init方法保证初始化的方差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xavier_init</span><span class="params">(fan_in, fan_out, constant = <span class="number">1</span>)</span>:</span></span><br><span class="line">    low = -constant * np.sqrt(<span class="number">6.0</span> / (fan_in + fan_out))</span><br><span class="line">    high = constant * np.sqrt(<span class="number">6.0</span> / (fan_in + fan_out))</span><br><span class="line">    <span class="keyword">return</span> tf.random_uniform((fan_in, fan_out),</span><br><span class="line">                             minval = low, maxval = high,</span><br><span class="line">                             dtype = tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 建立自编码器类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdditiveGaussianNoiseAutoencoder</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_input, n_hidden, transfer_function = tf.nn.softplus, optimizer = tf.train.AdamOptimizer<span class="params">()</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 scale = <span class="number">0.1</span>)</span>:</span></span><br><span class="line">        self.n_input = n_input</span><br><span class="line">        self.n_hidden = n_hidden</span><br><span class="line">        self.transfer = transfer_function</span><br><span class="line">        self.training_scale = scale</span><br><span class="line">        </span><br><span class="line">        network_weights = self._initialize_weights()</span><br><span class="line">        self.weights = network_weights</span><br><span class="line"></span><br><span class="line">        <span class="comment"># model</span></span><br><span class="line">        <span class="comment">### 建立占位符</span></span><br><span class="line">        self.scale = tf.placeholder(tf.float32)  <span class="comment">##高斯模糊的大小</span></span><br><span class="line">        self.x = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_input])</span><br><span class="line">        <span class="comment">### 加高斯模糊的隐藏层</span></span><br><span class="line">        self.hidden = self.transfer(tf.add(tf.matmul(self.x + scale * tf.random_normal((n_input,)),</span><br><span class="line">                self.weights[<span class="string">'w1'</span>]),</span><br><span class="line">                self.weights[<span class="string">'b1'</span>]))</span><br><span class="line">        <span class="comment">###重建输出</span></span><br><span class="line">        self.reconstruction = tf.add(tf.matmul(self.hidden, self.weights[<span class="string">'w2'</span>]), self.weights[<span class="string">'b2'</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cost</span></span><br><span class="line">        self.cost = <span class="number">0.5</span> * tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), <span class="number">2.0</span>))</span><br><span class="line">        self.optimizer = optimizer.minimize(self.cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### 初始化</span></span><br><span class="line">        init = tf.global_variables_initializer()</span><br><span class="line">        self.sess = tf.Session()</span><br><span class="line">        self.sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### 建立字典参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_initialize_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        all_weights = dict()</span><br><span class="line">        all_weights[<span class="string">'w1'</span>] = tf.Variable(xavier_init(self.n_input, self.n_hidden))</span><br><span class="line">        all_weights[<span class="string">'b1'</span>] = tf.Variable(tf.zeros([self.n_hidden], dtype = tf.float32))</span><br><span class="line">        all_weights[<span class="string">'w2'</span>] = tf.Variable(tf.zeros([self.n_hidden, self.n_input], dtype = tf.float32))</span><br><span class="line">        all_weights[<span class="string">'b2'</span>] = tf.Variable(tf.zeros([self.n_input], dtype = tf.float32))</span><br><span class="line">        <span class="keyword">return</span> all_weights</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 训练</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">partial_fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict = &#123;self.x: X,</span><br><span class="line">                                                                            self.scale: self.training_scale</span><br><span class="line">                                                                            &#125;)</span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="comment">### 计算cost</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_total_cost</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.sess.run(self.cost, feed_dict = &#123;self.x: X,</span><br><span class="line">                                                     self.scale: self.training_scale</span><br><span class="line">                                                     &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.sess.run(self.hidden, feed_dict = &#123;self.x: X,</span><br><span class="line">                                                       self.scale: self.training_scale</span><br><span class="line">                                                       &#125;)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    无效代码</span></span><br><span class="line"><span class="string">    def generate(self, hidden = None):</span></span><br><span class="line"><span class="string">        if hidden is None:</span></span><br><span class="line"><span class="string">            hidden = np.random.normal(size = self.weights["b1"])</span></span><br><span class="line"><span class="string">        return self.sess.run(self.reconstruction, feed_dict = &#123;self.hidden: hidden&#125;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def reconstruct(self, X):</span></span><br><span class="line"><span class="string">        return self.sess.run(self.reconstruction, feed_dict = &#123;self.x: X,</span></span><br><span class="line"><span class="string">                                                               self.scale: self.training_scale</span></span><br><span class="line"><span class="string">                                                               &#125;)</span></span><br><span class="line"><span class="string">    def getWeights(self):</span></span><br><span class="line"><span class="string">        return self.sess.run(self.weights['w1'])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def getBiases(self):</span></span><br><span class="line"><span class="string">        return self.sess.run(self.weights['b1'])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    '''</span>    </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">preprocessing这个模块还提供了一个实用类StandarScaler，它可以在训练数据集上做了标准转换操作之后，把相同的转换应用到测试训练集中。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">这是相当好的一个功能。可以对训练数据，测试数据应用相同的转换，以后有新的数据进来也可以直接调用，不用再重新把数据放在一起再计算一次了。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standard_scale</span><span class="params">(X_train, X_test)</span>:</span></span><br><span class="line">    preprocessor = prep.StandardScaler().fit(X_train)</span><br><span class="line">    X_train = preprocessor.transform(X_train)</span><br><span class="line">    X_test = preprocessor.transform(X_test)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_test</span><br><span class="line"></span><br><span class="line"><span class="comment">### 随机返回batch_size的数据块</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_random_block_from_data</span><span class="params">(data, batch_size)</span>:</span></span><br><span class="line">    start_index = np.random.randint(<span class="number">0</span>, len(data) - batch_size)</span><br><span class="line">    <span class="keyword">return</span> data[start_index:(start_index + batch_size)]</span><br><span class="line"></span><br><span class="line">X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)</span><br><span class="line"></span><br><span class="line">n_samples = int(mnist.train.num_examples)</span><br><span class="line">training_epochs = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 生成对象</span></span><br><span class="line">autoencoder = AdditiveGaussianNoiseAutoencoder(n_input = <span class="number">784</span>,</span><br><span class="line">                                               n_hidden = <span class="number">200</span>,</span><br><span class="line">                                               transfer_function = tf.nn.softplus,</span><br><span class="line">                                               optimizer = tf.train.AdamOptimizer(learning_rate = <span class="number">0.001</span>),</span><br><span class="line">                                               scale = <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">    avg_cost = <span class="number">0.</span></span><br><span class="line">    total_batch = int(n_samples / batch_size)</span><br><span class="line">    <span class="comment"># Loop over all batches</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">        batch_xs = get_random_block_from_data(X_train, batch_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Fit training using batch data</span></span><br><span class="line">        cost = autoencoder.partial_fit(batch_xs)</span><br><span class="line">        <span class="comment"># Compute average loss</span></span><br><span class="line">        avg_cost += cost / n_samples * batch_size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display logs per epoch step</span></span><br><span class="line">    <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Total cost: "</span> + str(autoencoder.calc_total_cost(X_test)))</span><br></pre></td></tr></table></figure>
<pre><code>Epoch: 0001 cost= 18419.810043182
Epoch: 0002 cost= 12060.265021591
Epoch: 0003 cost= 10484.025294886
Epoch: 0004 cost= 10011.340872727
Epoch: 0005 cost= 10345.419741477
Epoch: 0006 cost= 8558.077478977
Epoch: 0007 cost= 9358.221301705
Epoch: 0008 cost= 8207.935371023
Epoch: 0009 cost= 9071.398990341
Epoch: 0010 cost= 7567.627819318
Epoch: 0011 cost= 8493.544392614
Epoch: 0012 cost= 9373.162595455
Epoch: 0013 cost= 8513.334756818
Epoch: 0014 cost= 8793.045787500
Epoch: 0015 cost= 8341.089327841
Epoch: 0016 cost= 7673.378882386
Epoch: 0017 cost= 8378.051731818
Epoch: 0018 cost= 7996.387832386
Epoch: 0019 cost= 8001.048122727
Epoch: 0020 cost= 8242.288252273
Total cost: 661986.6
</code></pre>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积神经网络</title>
    <url>/2020/02/17/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><blockquote>
<p>该篇介绍了卷积神经网络的基本构造方法以及相关函数</p>
</blockquote>
<a id="more"></a>
<h3 id="卷积神经网络-1"><a href="#卷积神经网络-1" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><h3 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h3><ol>
<li><p>正则化，惩罚参数，避免过拟合</p>
<pre><code>  + tf.nn.l1_loss ,l1范数，使得参数稀疏，很多0
  + tf.nn.l2_loss ,l2范数，使得参数变小
</code></pre></li>
<li><p>广播机制（broadcasting）</p>
<script type="math/tex; mode=display">
一个300*10的矩阵和一个1*10的向量相加。和直觉相悖。\\
其实是在tensorflow中允许矩阵和向量相加。\\
C=A+b\\
即C[ij]=A[ij]+b[j]。也就是给矩阵A的每一行都加上向量b。\\
那么这至少要求矩阵的列数和向量的元素个数对齐。\\
这种隐式的复制向量b到很多位置的办法，叫做broadcasting。\\</script></li>
<li><p>tf.constant(value,dtype=None,shape=None,name=’Const’,verify_shape=False)</p>
</li>
<li></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="comment">###tf.constant(value,dtype=None,shape=None,name='Const',verify_shape=False)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"> </span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tf.nn.conv2d(</span></span><br><span class="line"><span class="string">    input,  </span></span><br><span class="line"><span class="string">    ##需要做卷积的输入图像（tensor），具有[batch,in_height,in_width,in_channels]这样的4维</span></span><br><span class="line"><span class="string">    filter,</span></span><br><span class="line"><span class="string">    ##相当于CNN中的卷积核，它是一个tensor，</span></span><br><span class="line"><span class="string">    ##shape是[filter_height,filter_width,in_channels,out_channels]</span></span><br><span class="line"><span class="string">    strides,</span></span><br><span class="line"><span class="string">    ## 卷积在每一维的步长，一般为一个一维向量，长度为4，一般为[1,stride,stride,1]。</span></span><br><span class="line"><span class="string">    padding,</span></span><br><span class="line"><span class="string">    ## 定义元素边框和元素内容之间的空间，只能是‘SAME’（边缘填充）或者‘VALID’（边缘不填充）</span></span><br><span class="line"><span class="string">    use_cudnn_on_gpu=True,</span></span><br><span class="line"><span class="string">    data_format='NHWC',</span></span><br><span class="line"><span class="string">    dilations=[1, 1, 1, 1],</span></span><br><span class="line"><span class="string">    name=None</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span>    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tf.nn.max_pool(value, ksize, strides, padding, name=None)</span></span><br><span class="line"><span class="string">参数是四个，和卷积很类似：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">第一个参数value：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">第二个参数ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">第三个参数strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">第四个参数padding：和卷积类似，可以取'VALID' 或者'SAME'</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                        strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)  </span><br><span class="line">                        </span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br><span class="line">                        </span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line"></span><br><span class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br><span class="line"></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">        x:batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">        print(<span class="string">"step %d, training accuracy %g"</span>%(i, train_accuracy),end=<span class="string">' '</span>)</span><br><span class="line">    train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">"test accuracy %g"</span>%accuracy.eval(feed_dict=&#123;</span><br><span class="line">    x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<pre><code>test accuracy 0.9489
</code></pre><h3 id="cifar卷积神经网络-未成功"><a href="#cifar卷积神经网络-未成功" class="headerlink" title="cifar卷积神经网络(未成功)"></a>cifar卷积神经网络(未成功)</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cifar10,cifar10_input</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">max_steps = <span class="number">3000</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">data_dir = <span class="string">'/cifar'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_with_weight_loss</span><span class="params">(shape, stddev, wl)</span>:</span></span><br><span class="line">    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))</span><br><span class="line">    <span class="keyword">if</span> wl <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        weight_loss = tf.multiply(tf.nn.l2_loss(var), wl, name=<span class="string">'weight_loss'</span>)</span><br><span class="line">        <span class="comment">##将参数统一放入losses中</span></span><br><span class="line">        tf.add_to_collection(<span class="string">'losses'</span>, weight_loss)</span><br><span class="line">    <span class="keyword">return</span> var</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(logits, labels)</span>:</span></span><br><span class="line">    </span><br><span class="line">    labels = tf.cast(labels, tf.int64)</span><br><span class="line">    </span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,labels)</span></span><br><span class="line"><span class="string">    相当于戳logits进行softmax后再计算与labels的交叉熵，不同的是labels不是one-hot型</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        logits=logits, labels=labels, name=<span class="string">'cross_entropy_per_example'</span>)</span><br><span class="line">    cross_entropy_mean = tf.reduce_mean(cross_entropy, name=<span class="string">'cross_entropy'</span>)</span><br><span class="line">    </span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>, cross_entropy_mean)</span><br><span class="line">    </span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">     tf.get_collection(key,scope=None)该函数可以用来获取key集合中的所有元素，返回一个列表。列表的顺序依变量放入集合中的先后而定。</span></span><br><span class="line"><span class="string">      tf.add_n 将列表里东西相加</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> tf.add_n(tf.get_collection(<span class="string">'losses'</span>), name=<span class="string">'total_loss'</span>)</span><br><span class="line">  </span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#cifar10.maybe_download_and_extract()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">images_train, labels_train = cifar10_input.distorted_inputs(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">images_test, labels_test = cifar10_input.inputs(eval_data=<span class="literal">True</span>,</span><br><span class="line">                                                batch_size=batch_size)                                                  </span><br><span class="line"><span class="comment">#images_train, labels_train = cifar10.distorted_inputs()</span></span><br><span class="line"><span class="comment">#images_test, labels_test = cifar10.inputs(eval_data=True)</span></span><br><span class="line"></span><br><span class="line">image_holder = tf.placeholder(tf.float32, [batch_size, <span class="number">24</span>, <span class="number">24</span>, <span class="number">3</span>])</span><br><span class="line">label_holder = tf.placeholder(tf.int32, [batch_size])</span><br><span class="line"></span><br><span class="line"><span class="comment">#logits = inference(image_holder)</span></span><br><span class="line"></span><br><span class="line">weight1 = variable_with_weight_loss(shape=[<span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">64</span>], stddev=<span class="number">5e-2</span>, wl=<span class="number">0.0</span>)</span><br><span class="line">kernel1 = tf.nn.conv2d(image_holder, weight1, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">bias1 = tf.Variable(tf.constant(<span class="number">0.0</span>, shape=[<span class="number">64</span>]))</span><br><span class="line">conv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))</span><br><span class="line">pool1 = tf.nn.max_pool(conv1, ksize=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                       padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="comment">### 基本废弃了</span></span><br><span class="line">norm1 = tf.nn.lrn(pool1, <span class="number">4</span>, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">weight2 = variable_with_weight_loss(shape=[<span class="number">5</span>, <span class="number">5</span>, <span class="number">64</span>, <span class="number">64</span>], stddev=<span class="number">5e-2</span>, wl=<span class="number">0.0</span>)</span><br><span class="line">kernel2 = tf.nn.conv2d(norm1, weight2, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">bias2 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">64</span>]))</span><br><span class="line">conv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))</span><br><span class="line">norm2 = tf.nn.lrn(conv2, <span class="number">4</span>, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>)</span><br><span class="line">pool2 = tf.nn.max_pool(norm2, ksize=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                       padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line">reshape = tf.reshape(pool2, [batch_size, <span class="number">-1</span>])</span><br><span class="line">dim = reshape.get_shape()[<span class="number">1</span>].value</span><br><span class="line"></span><br><span class="line">weight3 = variable_with_weight_loss(shape=[dim, <span class="number">384</span>], stddev=<span class="number">0.04</span>, wl=<span class="number">0.004</span>)</span><br><span class="line">bias3 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">384</span>]))</span><br><span class="line">local3 = tf.nn.relu(tf.matmul(reshape, weight3) + bias3)</span><br><span class="line"></span><br><span class="line">weight4 = variable_with_weight_loss(shape=[<span class="number">384</span>, <span class="number">192</span>], stddev=<span class="number">0.04</span>, wl=<span class="number">0.004</span>)</span><br><span class="line">bias4 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">192</span>]))                                      </span><br><span class="line">local4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)</span><br><span class="line"></span><br><span class="line">weight5 = variable_with_weight_loss(shape=[<span class="number">192</span>, <span class="number">10</span>], stddev=<span class="number">1</span>/<span class="number">192.0</span>, wl=<span class="number">0.0</span>)</span><br><span class="line">bias5 = tf.Variable(tf.constant(<span class="number">0.0</span>, shape=[<span class="number">10</span>]))</span><br><span class="line">logits = tf.add(tf.matmul(local4, weight5), bias5)</span><br><span class="line"></span><br><span class="line">loss = loss(logits, label_holder)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_op = tf.train.AdamOptimizer(<span class="number">1e-3</span>).minimize(loss) <span class="comment">#0.72</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tf.nn.in_top_k(predictions, targets, k)</span></span><br><span class="line"><span class="string">返回predictions第1-k的值的索引，看是否包含targets</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">top_k_op = tf.nn.in_top_k(logits, label_holder, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"></span><br><span class="line">tf.train.start_queue_runners()</span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(max_steps):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    image_batch,label_batch = sess.run([images_train,labels_train])</span><br><span class="line">    _, loss_value = sess.run([train_op, loss],feed_dict=&#123;image_holder: image_batch, </span><br><span class="line">                                                         label_holder:label_batch&#125;)</span><br><span class="line">    duration = time.time() - start_time</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        examples_per_sec = batch_size / duration</span><br><span class="line">        sec_per_batch = float(duration)</span><br><span class="line">    </span><br><span class="line">        format_str = (<span class="string">'step %d, loss = %.2f (%.1f examples/sec; %.3f sec/batch)'</span>)</span><br><span class="line">        print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))</span><br><span class="line">    </span><br><span class="line"><span class="comment">###</span></span><br><span class="line">num_examples = <span class="number">10000</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">num_iter = int(math.ceil(num_examples / batch_size))</span><br><span class="line">true_count = <span class="number">0</span>  </span><br><span class="line">total_sample_count = num_iter * batch_size</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> step &lt; num_iter:</span><br><span class="line">    image_batch,label_batch = sess.run([images_test,labels_test])</span><br><span class="line">    predictions = sess.run([top_k_op],feed_dict=&#123;image_holder: image_batch,</span><br><span class="line">                                                 label_holder:label_batch&#125;)</span><br><span class="line">    true_count += np.sum(predictions)</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">precision = true_count / total_sample_count</span><br><span class="line">print(<span class="string">'precision @ 1 = %.3f'</span> % precision)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>全连接神经网络</title>
    <url>/2020/02/17/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h1 id="全连接神经网络"><a href="#全连接神经网络" class="headerlink" title="全连接神经网络"></a>全连接神经网络</h1><blockquote>
<p>该篇介绍了全连接神经网络，做了mnist的数据集</p>
</blockquote>
<a id="more"></a>
<h3 id="全连接神经网络-1"><a href="#全连接神经网络-1" class="headerlink" title="全连接神经网络"></a>全连接神经网络</h3><p><strong>简单介绍：就相当于线性函数，用随机梯度优化算法进行优化</strong></p>
<blockquote>
<ul>
<li>生成随机数：tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) </li>
<li>reduce_mean(input_tensor, axis=None) 返回均值，axis是多维数组每个维度的坐标。还拿2维来说，数字3的坐标是[0, 1]，那么第一个数字0的axis是0，第二个数字1的axis是1</li>
<li>tf.cast（var，type）投射到指定类型</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">##会话的展开可以使用with tf.Session() as sess:</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 占位符，None可以批量处理数据</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## 实际证明 zeros有用，比random好</span></span><br><span class="line"><span class="comment">#tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) </span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 注意这里是matmul(矩阵乘)，而不是multiply或是*【这两者等价，皆为元素乘】</span></span><br><span class="line"><span class="comment">###  softmax是激活函数，就这个好用</span></span><br><span class="line">y_ = tf.nn.softmax(tf.matmul(x,W)+b)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 香农熵 y * tf.log(y_)</span></span><br><span class="line"><span class="comment">### reduce_mean(input_tensor, axis=None) 返回均值，sum返回和</span></span><br><span class="line"><span class="comment">## axis是多维数组每个维度的坐标。还拿2维来说，数字3的坐标是[0, 1]，那么第一个数字0的axis是0，第二个数字1的axis是1</span></span><br><span class="line"><span class="comment">### 因此axis = 0 为按第一维度操作</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 先sum每个样本的误差，然后求均值</span></span><br><span class="line">loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_), axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    xs,ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    sess.run(train_step,feed_dict=&#123;x:xs,y:ys&#125;)</span><br><span class="line">    <span class="keyword">if</span> (i%<span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">        print(sess.run(loss,feed_dict=&#123;x:xs,y:ys&#125;))</span><br><span class="line"></span><br><span class="line">        <span class="comment">### tf.argmax(input,axis)根据axis取值的不同返回每行或者每列最大值的索引</span></span><br><span class="line"><span class="comment">###equal，相等的意思。顾名思义，就是判断，x, y 是不是相等，它的判断方法不是整体判断，</span></span><br><span class="line"><span class="comment">###而是逐个元素进行判断，如果相等就是True，不相等，就是False。</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">### tf.cast投射到浮点数</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line">print(sess.run(accuracy,feed_dict=&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>
<pre><code>结果：
1.7234213
0.28298083
0.34306362
0.22262634
0.18778156
0.32757837
0.22582108
0.2445744
0.33943504
0.23404454

准确率：0.9178
</code></pre><h3 id="多层神经网络（MLP）"><a href="#多层神经网络（MLP）" class="headerlink" title="多层神经网络（MLP）"></a>多层神经网络（MLP）</h3><p>  <em>背景介绍（只加了层数和dropout）</em></p>
<ol>
<li><strong>Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如图1所示。</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape,stddev=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(tf.random_normal(shape=shape,stddev=stddev))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(tf.zeros(shape))</span><br><span class="line"></span><br><span class="line">in_units = <span class="number">784</span></span><br><span class="line">h1_units = <span class="number">300</span></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, in_units])</span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">W1 = weight_variable([in_units,h1_units])</span><br><span class="line">b1 = bias_variable([h1_units])</span><br><span class="line"></span><br><span class="line">hidden1 = tf.nn.relu(tf.matmul(x, W1) + b1)</span><br><span class="line">hidden1_drop = tf.nn.dropout(hidden1, keep_prob)</span><br><span class="line"></span><br><span class="line">W2 = weight_variable([h1_units,<span class="number">10</span>])</span><br><span class="line">b2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">y = tf.nn.softmax(tf.matmul(hidden1_drop, W2) + b2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define loss and optimizer</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), axis=<span class="number">1</span>))</span><br><span class="line">train_step = tf.train.AdagradOptimizer(<span class="number">0.3</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3000</span>):</span><br><span class="line">  batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">  train_step.run(&#123;x: batch_xs, y_: batch_ys, keep_prob: <span class="number">0.75</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test trained model</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">print(accuracy.eval(&#123;x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<pre><code>0.9791
</code></pre>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>时间模块</title>
    <url>/2020/02/17/%E6%97%B6%E9%97%B4%E6%A8%A1%E5%9D%97/</url>
    <content><![CDATA[<h1 id="时间"><a href="#时间" class="headerlink" title="时间"></a>时间</h1><blockquote>
<p>time 和calendar 的简单应用</p>
</blockquote>
<a id="more"></a>
<h3 id="时间模块"><a href="#时间模块" class="headerlink" title="时间模块"></a>时间模块</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment">#time.time()返回到1970年的s数，为浮点数</span></span><br><span class="line"><span class="comment">#localtime 返回时间元组</span></span><br><span class="line">localTime = time.localtime(time.time()) </span><br><span class="line">print(localTime)</span><br><span class="line"><span class="comment">#asctime返回一个可读的时间</span></span><br><span class="line">print(time.asctime(localTime))</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    %y 两位数的年份表示（00-99）</span></span><br><span class="line"><span class="string">    %Y 四位数的年份表示（000-9999）</span></span><br><span class="line"><span class="string">    %m 月份（01-12）</span></span><br><span class="line"><span class="string">    %d 月内中的一天（0-31）</span></span><br><span class="line"><span class="string">    %H 24小时制小时数（0-23）</span></span><br><span class="line"><span class="string">    %I 12小时制小时数（01-12）</span></span><br><span class="line"><span class="string">    %M 分钟数（00=59）</span></span><br><span class="line"><span class="string">    %S 秒（00-59）</span></span><br><span class="line"><span class="string">    %a 本地简化星期名称</span></span><br><span class="line"><span class="string">    %A 本地完整星期名称</span></span><br><span class="line"><span class="string">    %b 本地简化的月份名称</span></span><br><span class="line"><span class="string">    %B 本地完整的月份名称</span></span><br><span class="line"><span class="string">    %c 本地相应的日期表示和时间表示</span></span><br><span class="line"><span class="string">    %j 年内的一天（001-366）</span></span><br><span class="line"><span class="string">    %p 本地A.M.或P.M.的等价符</span></span><br><span class="line"><span class="string">    %U 一年中的星期数（00-53）星期天为星期的开始</span></span><br><span class="line"><span class="string">    %w 星期（0-6），星期天为星期的开始</span></span><br><span class="line"><span class="string">    %W 一年中的星期数（00-53）星期一为星期的开始</span></span><br><span class="line"><span class="string">    %x 本地相应的日期表示</span></span><br><span class="line"><span class="string">    %X 本地相应的时间表示</span></span><br><span class="line"><span class="string">    %Z 当前时区的名称</span></span><br><span class="line"><span class="string">    %% %号本身</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (time.strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>, time.localtime()))</span><br></pre></td></tr></table></figure>
<pre><code>time.struct_time(tm_year=2020, tm_mon=2, tm_mday=15, tm_hour=19, tm_min=37, tm_sec=33, tm_wday=5, tm_yday=46, tm_isdst=0)
Sat Feb 15 19:37:33 2020
2020-02-15 19:37:33
</code></pre><h3 id="日历"><a href="#日历" class="headerlink" title="日历"></a>日历</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> calendar</span><br><span class="line"></span><br><span class="line">cal = calendar.month(<span class="number">2020</span>,<span class="number">2</span>)</span><br><span class="line">print(cal)</span><br></pre></td></tr></table></figure>
<pre><code>   February 2020
Mo Tu We Th Fr Sa Su
                1  2
 3  4  5  6  7  8  9
10 11 12 13 14 15 16
17 18 19 20 21 22 23
24 25 26 27 28 29
</code></pre><p>​    </p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>-基础类 -语言</tag>
      </tags>
  </entry>
  <entry>
    <title>函数模块</title>
    <url>/2020/02/15/%E5%87%BD%E6%95%B0%E6%A8%A1%E5%9D%97/</url>
    <content><![CDATA[<h1 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h1><blockquote>
<p>该篇介绍了python的自定义函数</p>
</blockquote>
<a id="more"></a>
<h3 id="类型"><a href="#类型" class="headerlink" title="类型"></a>类型</h3><ol>
<li>可变类型和不可变对象<ul>
<li>strings, tuples, 和 numbers 是不可更改的对象，而 list,dict 等则是可以修改的对象。</li>
<li>不可变类型 形参结合时，形参是深拷贝，  可变类型则是浅拷贝，因此会影响原来元素</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#不可变类型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">changeInt</span><span class="params">(a)</span>:</span></span><br><span class="line">    a = <span class="number">10</span></span><br><span class="line">b = <span class="number">2</span></span><br><span class="line">changeInt(b)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">changeList</span><span class="params">(a)</span>:</span></span><br><span class="line">    a.extend([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> b</span><br><span class="line">b =[<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">changeList(b)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<pre><code>2
[2, 3, 1, 2]
</code></pre><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><ul>
<li>必备参数</li>
<li>关键字参数 ,可以不必要符合顺序</li>
<li>默认参数 ，事先设置参数</li>
<li>不定长参数,示例</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可写函数说明</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printinfo</span><span class="params">( arg1, *vartuple )</span>:</span></span><br><span class="line">   <span class="string">"打印任何传入的参数"</span></span><br><span class="line">   print( <span class="string">"输出: "</span>)</span><br><span class="line">   <span class="keyword">print</span> (arg1)</span><br><span class="line">   <span class="keyword">for</span> var <span class="keyword">in</span> vartuple:</span><br><span class="line">      <span class="keyword">print</span> (var)</span><br><span class="line">   <span class="keyword">return</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 调用printinfo 函数</span></span><br><span class="line">printinfo( <span class="number">10</span> )</span><br><span class="line">printinfo( <span class="number">70</span>, <span class="number">60</span>, <span class="number">50</span> )</span><br></pre></td></tr></table></figure>
<pre><code>输出: 
10
输出: 
70
60
50
</code></pre><h3 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h3><blockquote>
<p>同C语言的define方法，看示例</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sum = <span class="keyword">lambda</span> arg1, arg2: arg1 + arg2</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 调用sum函数</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"相加后的值为 : "</span>, sum( <span class="number">10</span>, <span class="number">20</span> ))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"相加后的值为 : "</span>, sum( <span class="number">20</span>, <span class="number">20</span> ))</span><br></pre></td></tr></table></figure>
<pre><code>相加后的值为 :  30
相加后的值为 :  40
</code></pre><h3 id="变量作用域"><a href="#变量作用域" class="headerlink" title="变量作用域"></a>变量作用域</h3><blockquote>
<p>同C语言</p>
</blockquote>
<h3 id="import-相关"><a href="#import-相关" class="headerlink" title="import 相关"></a>import 相关</h3><blockquote>
<p>略</p>
</blockquote>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>基础类</tag>
        <tag>语言</tag>
      </tags>
  </entry>
  <entry>
    <title>结构详细</title>
    <url>/2020/02/15/%E7%BB%93%E6%9E%84%E8%AF%A6%E7%BB%86/</url>
    <content><![CDATA[<h1 id="结构详细"><a href="#结构详细" class="headerlink" title="结构详细"></a>结构详细</h1><blockquote>
<p>该篇介绍了python的详细结构</p>
</blockquote>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="comment">#dir(math)</span></span><br></pre></td></tr></table></figure>
<h3 id="random-随机函数"><a href="#random-随机函数" class="headerlink" title="random 随机函数"></a>random 随机函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> *</span><br><span class="line">print(choice(range(<span class="number">10</span>))) <span class="comment">#简单</span></span><br><span class="line">print(randrange(<span class="number">1</span>,<span class="number">10</span>,<span class="number">2</span>))  <span class="comment">#x,y,z,为start，end，step</span></span><br><span class="line">print(random())</span><br><span class="line">print(uniform(<span class="number">1</span>,<span class="number">2</span>))  <span class="comment">#x,y为start和end</span></span><br></pre></td></tr></table></figure>
<pre><code>3
1
0.17893877145507286
1.054488927162555
</code></pre><h3 id="字符串格式化"><a href="#字符串格式化" class="headerlink" title="字符串格式化"></a>字符串格式化</h3><blockquote>
<p>同C语言</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"my nameis %s,%d years"</span>%(<span class="string">'yb'</span>,<span class="number">20</span>))</span><br></pre></td></tr></table></figure>
<pre><code>my nameis yb,20 years
</code></pre><h3 id="列表详细函数"><a href="#列表详细函数" class="headerlink" title="列表详细函数"></a>列表详细函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list =[]</span><br><span class="line">list.append(<span class="string">"fool"</span>)   <span class="comment">#尾部添加单个元素</span></span><br><span class="line">list.extend([<span class="string">"genius"</span>]) <span class="comment">#尾部添加列表</span></span><br><span class="line"></span><br><span class="line">print(list)</span><br><span class="line"><span class="keyword">del</span> list[<span class="number">0</span>]</span><br><span class="line">print(list)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">list.reverse()</span></span><br><span class="line"><span class="string">反向列表中元素</span></span><br><span class="line"><span class="string">list.sort(cmp=None, key=None, reverse=False)</span></span><br><span class="line"><span class="string">对原列表进行排序</span></span><br><span class="line"><span class="string">list.remove(obj)</span></span><br><span class="line"><span class="string">移除列表中某个值的第一个匹配项</span></span><br><span class="line"><span class="string">list.insert(index, obj)</span></span><br><span class="line"><span class="string">将对象插入列表</span></span><br><span class="line"><span class="string">list.count(obj) </span></span><br><span class="line"><span class="string">统计某个元素在列表中出现的次数</span></span><br><span class="line"><span class="string">list.index(obj)</span></span><br><span class="line"><span class="string">从列表中找出某个值第一个匹配项的索引位置</span></span><br><span class="line"><span class="string">cmp(list1, list2)</span></span><br><span class="line"><span class="string">比较两个列表的元素</span></span><br><span class="line"><span class="string">len(list)</span></span><br><span class="line"><span class="string">列表元素个数</span></span><br><span class="line"><span class="string">max(list)</span></span><br><span class="line"><span class="string">返回列表元素最大值</span></span><br><span class="line"><span class="string">list(seq)</span></span><br><span class="line"><span class="string">将元组转换为列表</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<pre><code>[&#39;fool&#39;, &#39;genius&#39;]
[&#39;genius&#39;]
</code></pre><h3 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h3><ul>
<li>注意一个元素的元组为 tup1 = (50,)</li>
<li>注意不允许删改，必须新建</li>
<li>方法同列表</li>
</ul>
<h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><ul>
<li>键不可变，值可以</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict = &#123;<span class="string">'Name'</span>: <span class="string">'Zara'</span>, <span class="string">'Age'</span>: <span class="number">7</span>, <span class="string">'Class'</span>: <span class="string">'First'</span>&#125;</span><br><span class="line">dict[<span class="string">'Name'</span>]=<span class="string">'yb'</span></span><br><span class="line">dict[<span class="string">'x'</span>]=<span class="number">4</span></span><br><span class="line">print(dict)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dict.keys():   <span class="comment">#keys返回键值</span></span><br><span class="line">    print(dict[i],end=<span class="string">' '</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dict.values():  <span class="comment">#values返回值</span></span><br><span class="line">    print(i,end=<span class="string">' '</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dict.items():   <span class="comment"># items 返回键和值的二元组</span></span><br><span class="line">    print(i[<span class="number">0</span>],end=<span class="string">' '</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">dict.clear()</span></span><br><span class="line"><span class="string">删除字典内所有元素</span></span><br><span class="line"><span class="string">dict.copy()</span></span><br><span class="line"><span class="string">返回一个字典的浅复制</span></span><br><span class="line"><span class="string">dict.deepcopy()</span></span><br><span class="line"><span class="string">返回一个字典的深复制</span></span><br><span class="line"><span class="string">dict.fromkeys(seq[, val])</span></span><br><span class="line"><span class="string">创建一个新字典，以序列 seq 中元素做字典的键，val 为字典所有键对应的初始值</span></span><br><span class="line"><span class="string">dict.update(dict2)</span></span><br><span class="line"><span class="string">把字典dict2的键/值对更新到dict里</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<pre><code>{&#39;Name&#39;: &#39;yb&#39;, &#39;Age&#39;: 7, &#39;Class&#39;: &#39;First&#39;, &#39;x&#39;: 4}
yb 7 First 4 yb 7 First 4 Name
Age
Class
x
</code></pre>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>基础类</tag>
        <tag>语言</tag>
      </tags>
  </entry>
  <entry>
    <title>基本语句</title>
    <url>/2020/02/15/%E5%9F%BA%E6%9C%AC%E8%AF%AD%E5%8F%A5/</url>
    <content><![CDATA[<h1 id="基本语句"><a href="#基本语句" class="headerlink" title="基本语句"></a>基本语句</h1><blockquote>
<p>该篇介绍了python的基本语句</p>
</blockquote>
<a id="more"></a>
<h3 id="条件语句"><a href="#条件语句" class="headerlink" title="条件语句"></a>条件语句</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if 判断条件：</span><br><span class="line">    执行语句……</span><br><span class="line">elif 判断条件：</span><br><span class="line">    执行语句</span><br><span class="line">else：</span><br><span class="line">    执行语句……</span><br></pre></td></tr></table></figure>
<h3 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h3><ol>
<li>while循环</li>
<li>for循环</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> (count &lt; <span class="number">9</span>):</span><br><span class="line">   <span class="keyword">print</span> (<span class="string">'The count is:'</span>, count)</span><br><span class="line">   count = count + <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"循环结束"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Good bye!"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>The count is: 0
The count is: 1
The count is: 2
The count is: 3
The count is: 4
The count is: 5
The count is: 6
The count is: 7
The count is: 8
循环结束
Good bye!
</code></pre><h4 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> letter <span class="keyword">in</span> <span class="string">'Python'</span>:     <span class="comment"># 第一个实例</span></span><br><span class="line">   <span class="keyword">print</span> (<span class="string">'当前字母 :'</span>, letter)</span><br><span class="line"> </span><br><span class="line">fruits = [<span class="string">'banana'</span>, <span class="string">'apple'</span>,  <span class="string">'mango'</span>]</span><br><span class="line"><span class="keyword">for</span> fruit <span class="keyword">in</span> fruits:        <span class="comment"># 第二个实例</span></span><br><span class="line">   <span class="keyword">print</span> (<span class="string">'当前水果 :'</span>, fruit)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(fruits)):</span><br><span class="line">    print(fruits[i])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"x"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Good bye!"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>当前字母 : P
当前字母 : y
当前字母 : t
当前字母 : h
当前字母 : o
当前字母 : n
当前水果 : banana
当前水果 : apple
当前水果 : mango
banana
apple
mango
x
Good bye!
</code></pre><h4 id="实践-1"><a href="#实践-1" class="headerlink" title="实践"></a>实践</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">20</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>,i):</span><br><span class="line">        <span class="keyword">if</span>(i%j==<span class="number">0</span>):</span><br><span class="line">            print(i,end=<span class="string">' '</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br></pre></td></tr></table></figure>
<pre><code>4 6 8 9 10 12 14 15 16 18 
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">i = <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span>(i&lt;<span class="number">20</span>):</span><br><span class="line">    j = <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span>(j&lt;i):</span><br><span class="line">        <span class="keyword">if</span>(i%j == <span class="number">0</span>): <span class="keyword">break</span></span><br><span class="line">        j=j+<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span>(j&gt;=i): print(i,<span class="string">"是素数"</span>,end=<span class="string">' '</span>)</span><br><span class="line">    i=i+<span class="number">1</span></span><br></pre></td></tr></table></figure>
<pre><code>2 是素数 3 是素数 5 是素数 7 是素数 11 是素数 13 是素数 17 是素数 19 是素数 
</code></pre>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>基础类</tag>
        <tag>语言</tag>
      </tags>
  </entry>
  <entry>
    <title>变量类型及数据结构</title>
    <url>/2020/02/15/%E5%8F%98%E9%87%8F%E7%B1%BB%E5%9E%8B%E5%8F%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<h1 id="变量类型及数据结构"><a href="#变量类型及数据结构" class="headerlink" title="变量类型及数据结构"></a>变量类型及数据结构</h1><blockquote>
<p>该篇介绍了python的变量类型，字符串以及简单的列表，元组和字典</p>
</blockquote>
<a id="more"></a>
<h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><ol>
<li><p>s=”a1a2···an”(n&gt;=0) </p>
<ul>
<li>从左到右索引<strong>默认0开始</strong>的，最大范围是字符串长度少1</li>
<li>从右到左索引默认-1开始的，最大范围是字符串开头</li>
</ul>
</li>
<li><p>加号（+）是字符串连接运算符，星号（*）是重复操作</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">str = <span class="string">"hello,world"</span></span><br><span class="line">print(str)      <span class="comment"># 输出完成字符串</span></span><br><span class="line">print(str[<span class="number">2</span>])   <span class="comment"># 输出字符串的地三个字符</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## [x,y,z] 输出x到y间的字符，包括x，不包括y，步长为z</span></span><br><span class="line">print(str[<span class="number">2</span>:<span class="number">5</span>:<span class="number">2</span>]) <span class="comment"># 输出第三道第六个字符，不包括第六个</span></span><br><span class="line">print(str[<span class="number">2</span>:])  <span class="comment"># 输出第3个开头</span></span><br><span class="line">print(str*<span class="number">2</span>)</span><br><span class="line">print(str+<span class="string">"s"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>hello,world
l
lo
llo,world
hello,worldhello,world
hello,worlds
</code></pre><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><blockquote>
<p>操作同字符串</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list = [ <span class="string">'runoob'</span>, <span class="number">786</span> , <span class="number">2.23</span>, <span class="string">'john'</span>, <span class="number">70.2</span> ]</span><br><span class="line">tinylist = [<span class="number">123</span>, <span class="string">'john'</span>]</span><br><span class="line"> </span><br><span class="line"><span class="keyword">print</span> (list)               <span class="comment"># 输出完整列表</span></span><br><span class="line"><span class="keyword">print</span> (list[<span class="number">0</span>])            <span class="comment"># 输出列表的第一个元素</span></span><br><span class="line"><span class="keyword">print</span> (list[<span class="number">1</span>:<span class="number">3</span>] )         <span class="comment"># 输出第二个至第三个元素 </span></span><br><span class="line"><span class="keyword">print</span> (list[<span class="number">2</span>:] )          <span class="comment"># 输出从第三个开始至列表末尾的所有元素</span></span><br><span class="line"><span class="keyword">print</span> (tinylist * <span class="number">2</span>)       <span class="comment"># 输出列表两次</span></span><br><span class="line"><span class="keyword">print</span> (list + tinylist)    <span class="comment"># 打印组合的列表</span></span><br></pre></td></tr></table></figure>
<pre><code>[&#39;runoob&#39;, 786, 2.23, &#39;john&#39;, 70.2]
runoob
[786, 2.23]
[2.23, &#39;john&#39;, 70.2]
[123, &#39;john&#39;, 123, &#39;john&#39;]
[&#39;runoob&#39;, 786, 2.23, &#39;john&#39;, 70.2, 123, &#39;john&#39;]
</code></pre><h3 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h3><blockquote>
<p>相当于只读列表</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list = (<span class="string">'runoob'</span>, <span class="number">786</span> , <span class="number">2.23</span>, <span class="string">'john'</span>, <span class="number">70.2</span> )</span><br><span class="line">tinylist = (<span class="number">123</span>, <span class="string">'john'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">print</span> (list)               <span class="comment"># 输出完整列表</span></span><br><span class="line"><span class="keyword">print</span> (list[<span class="number">0</span>])            <span class="comment"># 输出列表的第一个元素</span></span><br><span class="line"><span class="keyword">print</span> (list[<span class="number">1</span>:<span class="number">3</span>] )         <span class="comment"># 输出第二个至第三个元素 </span></span><br><span class="line"><span class="keyword">print</span> (list[<span class="number">2</span>:] )          <span class="comment"># 输出从第三个开始至列表末尾的所有元素</span></span><br><span class="line"><span class="keyword">print</span> (tinylist * <span class="number">2</span>)       <span class="comment"># 输出列表两次</span></span><br><span class="line"><span class="keyword">print</span> (list + tinylist)    <span class="comment"># 打印组合的列表</span></span><br></pre></td></tr></table></figure>
<pre><code>(&#39;runoob&#39;, 786, 2.23, &#39;john&#39;, 70.2)
runoob
(786, 2.23)
(2.23, &#39;john&#39;, 70.2)
(123, &#39;john&#39;, 123, &#39;john&#39;)
(&#39;runoob&#39;, 786, 2.23, &#39;john&#39;, 70.2, 123, &#39;john&#39;)
</code></pre><h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><blockquote>
<p>通过键来存取，而非偏移</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict = &#123;&#125;</span><br><span class="line">dict[<span class="string">'one'</span>] = <span class="string">"This is one"</span></span><br><span class="line">dict[<span class="number">2</span>] = <span class="string">"This is two"</span></span><br><span class="line">tinydict = &#123;<span class="string">'name'</span>: <span class="string">'john'</span>,<span class="string">'code'</span>:<span class="number">6734</span>, <span class="string">'dept'</span>: <span class="string">'sales'</span>&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">print</span> (dict[<span class="string">'one'</span>])          <span class="comment"># 输出键为'one' 的值</span></span><br><span class="line"><span class="keyword">print</span> (dict[<span class="number">2</span>])              <span class="comment"># 输出键为 2 的值</span></span><br><span class="line"><span class="keyword">print</span> (tinydict)             <span class="comment"># 输出完整的字典</span></span><br><span class="line"><span class="keyword">print</span> (tinydict.keys())      <span class="comment"># 输出所有键,结果为列表</span></span><br><span class="line"><span class="keyword">print</span> (tinydict.values())    <span class="comment"># 输出所有值，结果为列表</span></span><br></pre></td></tr></table></figure>
<pre><code>This is one
This is two
{&#39;name&#39;: &#39;john&#39;, &#39;code&#39;: 6734, &#39;dept&#39;: &#39;sales&#39;}
dict_keys([&#39;name&#39;, &#39;code&#39;, &#39;dept&#39;])
dict_values([&#39;john&#39;, 6734, &#39;sales&#39;])
</code></pre><h3 id="数据转换类型"><a href="#数据转换类型" class="headerlink" title="数据转换类型"></a>数据转换类型</h3><p>列举较为重要的</p>
<ul>
<li>tuple(s),list(s)</li>
<li>set(s) 转换为可变集合</li>
<li>frozenset(s) 不可变集合</li>
<li>dict(s) 创建一个字典。d 必须是一个序列 (key,value)元组。</li>
</ul>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>基础类</tag>
        <tag>语言</tag>
      </tags>
  </entry>
  <entry>
    <title>markdown语法</title>
    <url>/2020/02/15/markdown%E8%AF%AD%E6%B3%95/</url>
    <content><![CDATA[<h1 id="markdown基本语法"><a href="#markdown基本语法" class="headerlink" title="markdown基本语法"></a>markdown基本语法</h1><blockquote>
<p>该篇介绍了markdown 的基本语法及typora的快捷键</p>
</blockquote>
<a id="more"></a>
<h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><ol>
<li>n个#代表了n级标题</li>
<li>typora 快捷键 ctrl+ n</li>
</ol>
<h3 id="粗体-斜体、删除线和下划线"><a href="#粗体-斜体、删除线和下划线" class="headerlink" title="粗体 斜体、删除线和下划线"></a>粗体 斜体、删除线和下划线</h3><ol>
<li><em>斜体</em>  （**包围）快捷键ctrl+i  （italic）</li>
<li><strong>粗体</strong>（四个*包围）快捷键 ctrl+b （bold）</li>
<li><strong><em>加粗斜体</em></strong>  （6分*包围） 无快捷键</li>
<li><del>删除线</del> （4个~包围）无快捷键</li>
<li>下划线 快捷键 ctrl+u （underline） <u>快乐</u></li>
</ol>
<h3 id="引用块"><a href="#引用块" class="headerlink" title="引用块"></a>引用块</h3><blockquote>
<p>文字 用&gt;开头</p>
</blockquote>
<h3 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h3><p><code>行内代码</code>   //用两个`包围，必须时英文字符</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">多行代码 &#x2F;&#x2F;用6个&#96;包围 ，必须英文</span><br></pre></td></tr></table></figure>
<h3 id="公式块"><a href="#公式块" class="headerlink" title="公式块"></a>公式块</h3><script type="math/tex; mode=display">
数学公式，用$$$$包围</script><h3 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h3><ol>
<li><hr>
<p>用3个-或+或*都可</p>
</li>
</ol>
<h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><ul>
<li>+,-,*代表无须列表</li>
<li>1.代表有序列表项</li>
</ul>
<h3 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h3><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>直接记忆快捷项 ctrl+t（table)</p>
<h3 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h3><p><a href="www.baidu.com">ssss</a></p>
<www.baidu.com>

<p>1.用【l链接文字】（链接地址）//用英文 快捷键ctrl+k </p>
<p>2.用《链接地址》 //用英文</p>
<h3 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![图片文字](图片地址 &quot;图片描述&quot;)</span><br></pre></td></tr></table></figure>
<p>快捷键ctrl+shift+i (image)</p>
</www.baidu.com>]]></content>
      <categories>
        <category>makedown</category>
      </categories>
      <tags>
        <tag>基础类</tag>
      </tags>
  </entry>
  <entry>
    <title>python_start</title>
    <url>/2020/02/15/python-start/</url>
    <content><![CDATA[<h1 id="python基础"><a href="#python基础" class="headerlink" title="python基础"></a>python基础</h1><blockquote>
<p>该篇介绍了python 的基础</p>
</blockquote>
<a id="more"></a>
<h3 id="标识符"><a href="#标识符" class="headerlink" title="标识符"></a>标识符</h3><ul>
<li>在 Python 里，标识符由字母、数字、下划线组成。</li>
<li>在 Python 中，所有标识符可以包括英文、数字以及下划线(_)，但不能以数字开头。</li>
<li>Python 中的标识符是区分大小写的。</li>
<li>以下划线开头的标识符是有特殊意义的。以单下划线开头 _foo 的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用 from xxx import * 而导入。</li>
<li>以双下划线开头的 <strong>foo 代表类的私有成员，以双下划线开头和结尾的 </strong>foo<strong> 代表 Python 里特殊方法专用的标识，如</strong> <strong>init__</strong>() 代表类的构造函数。</li>
</ul>
<hr>
<h3 id="保留字"><a href="#保留字" class="headerlink" title="保留字"></a>保留字</h3><blockquote>
<p>略</p>
</blockquote>
<h3 id="行和缩进"><a href="#行和缩进" class="headerlink" title="行和缩进"></a>行和缩进</h3><blockquote>
<p>4个空格</p>
</blockquote>
<hr>
<h3 id="多行语句"><a href="#多行语句" class="headerlink" title="多行语句"></a>多行语句</h3><blockquote>
<p>\</p>
</blockquote>
<hr>
<h3 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h3><ul>
<li><h1 id="单行"><a href="#单行" class="headerlink" title="单行"></a>单行</h1></li>
<li>“”” “””  多行注释</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注释</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">这是多行注释，使用双引号</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print(<span class="string">"按下enter键退出\n"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>按下enter键退出
</code></pre><h3 id="print"><a href="#print" class="headerlink" title="print"></a>print</h3><blockquote>
<p><del>不换行加逗号</del>,实测没用，用第一种方法吧</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = <span class="string">"a"</span></span><br><span class="line">y = <span class="string">"b"</span></span><br><span class="line">print(x,y)</span><br><span class="line">print(x)</span><br><span class="line">print(<span class="string">"-----"</span>)</span><br><span class="line">print(x,)</span><br><span class="line">print(y,)</span><br></pre></td></tr></table></figure>
<pre><code>a b
a
-----
a
b
</code></pre><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p><strong>多变量赋值(特别是内存的思考)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">a=b=c a,b,c分在同一个内存区</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">a=b=c=<span class="number">1</span> </span><br><span class="line">print(a)</span><br><span class="line">b=<span class="number">2</span></span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">q,w=1,1 在多个位置</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">q,w =<span class="number">1</span>,<span class="number">1</span></span><br><span class="line">print(q)</span><br><span class="line">w=<span class="number">2</span></span><br><span class="line">print(w)</span><br><span class="line"></span><br><span class="line"><span class="comment">## del 删除了引用，但不是删除空间，所以b可以用</span></span><br><span class="line"><span class="keyword">del</span> a  </span><br><span class="line"><span class="comment">#print(a) 报错</span></span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<pre><code>1
2
1
2
2
</code></pre><h3 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h3><p>挑有用的讲</p>
<ul>
<li>a**b ，返回a的b次幂</li>
<li>a//b ，返回a整除b，向下取整</li>
<li>逻辑符为 and or not</li>
<li>成员运算符 in 和not in  如果在指定的序列中找到值返回 True，否则返回 False。</li>
<li>身份运算符 is 和is not    is 是判断两个标识符是不是引用自一个对象</li>
</ul>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>基础类</tag>
        <tag>语言</tag>
      </tags>
  </entry>
</search>
